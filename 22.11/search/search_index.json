{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Harvest is the open-metrics endpoint for ONTAP and StorageGRID NetApp Harvest brings observability to ONTAP and StorageGRID clusters. Harvest collects performance, capacity and hardware metrics from ONTAP and StorageGRID, transforms them, and routes them to your choice of time-series database. The included Grafana dashboards deliver the datacenter insights you need, while new metrics can be collected with a few edits of the included template files. Harvest is open-source, released under an Apache2 license , and offers great flexibility in how you collect, augment, and export your datacenter metrics. Note Hop onto our Discord or GitHub discussions and say hi. \ud83d\udc4b\ud83c\udffd","title":"What is Harvest?"},{"location":"MigratePrometheusDocker/","text":"Migrate Prometheus Docker Volume \u00b6 If you want to keep your historical Prometheus data, and you generated your harvest-compose.yml file via bin/harvest generate before Harvest 22.11 , please follow the steps below to migrate your historical Prometheus data. This is not required if you generated your harvest-compose.yml file via bin/harvest generate at Harvest release 22.11 or after. Outline of steps: 1. Stop Prometheus container so data acquiesces 2. Find historical Prometheus volume and create new Prometheus data volume 3. Create a new Prometheus volume that Harvest 22.11 and after will use 4. Copy the historical Prometheus data from the old volume to the new one 5. Optionally remove the historical Prometheus volume Stop Prometheus container \u00b6 It's safe to run the stop and rm commands below regardless if Prometheus is running or not since removing the container does not touch the historical data stored in the volume. Stop all containers named Prometheus and remove them. docker stop ( docker ps -fname = prometheus -q ) && docker rm ( docker ps -a -fname = prometheus -q ) Docker may complain if the container is not running, like so. You can ignore this. Ignorable output when container is not running (click me) \"docker stop\" requires at least 1 argument. See 'docker stop --help' . Usage: docker stop [ OPTIONS ] CONTAINER [ CONTAINER... ] Stop one or more running containers Find the name of the Prometheus volume that has the historical data \u00b6 docker volume ls -f name = prometheus -q Output should look like this: harvest-22080-1_linux_amd64_prometheus_data # historical Prometheus data here harvest_prometheus_data # it is fine if this line is missing We want to copy the historical data from harvest-22080-1_linux_amd64_prometheus_data to harvest_prometheus_data If harvest_prometheus_data already exists, you need to decide if you want to move that volume's data to a different volume or remove it. If you want to remove the volume, run docker volume rm harvest_prometheus_data . If you want to move the data, adjust the command below to first copy harvest_prometheus_data to a different volume and then remove it. Create new Prometheus volume \u00b6 We're going to create a new mount named, harvest_prometheus_data by executing: docker volume create --name harvest_prometheus_data Copy the historical Prometheus data \u00b6 We will copy the historical Prometheus data from the old volume to the new one by mounting both volumes and copying data between them. NOTE : Prometheus only supports copying a single volume. It will not work if you attempt to copy multiple volumes into the same destination volume. # replace `HISTORICAL_VOLUME` with the name of the Prometheus volume that contains you historical data found in step 2. docker run --rm -it -v $HISTORICAL_VOLUME :/from -v harvest_prometheus_data:/to alpine ash -c \"cd /from ; cp -av . /to\" Output will look something like this: './wal' -> '/to/./wal' './wal/00000000' -> '/to/./wal/00000000' './chunks_head' -> '/to/./chunks_head' ... Optionally remove historical Prometheus data \u00b6 Before removing the historical data, start your compose stack and make sure everything works. Once you're satisfied that you can destroy the old data, remove it like so. # replace `HISTORICAL_VOLUME` with the name of the Prometheus volume that contains your historical data found in step 2. docker volume rm $HISTORICAL_VOLUME Reference \u00b6 Rename Docker Volume","title":"Migrate Prometheus Docker Volume"},{"location":"MigratePrometheusDocker/#migrate-prometheus-docker-volume","text":"If you want to keep your historical Prometheus data, and you generated your harvest-compose.yml file via bin/harvest generate before Harvest 22.11 , please follow the steps below to migrate your historical Prometheus data. This is not required if you generated your harvest-compose.yml file via bin/harvest generate at Harvest release 22.11 or after. Outline of steps: 1. Stop Prometheus container so data acquiesces 2. Find historical Prometheus volume and create new Prometheus data volume 3. Create a new Prometheus volume that Harvest 22.11 and after will use 4. Copy the historical Prometheus data from the old volume to the new one 5. Optionally remove the historical Prometheus volume","title":"Migrate Prometheus Docker Volume"},{"location":"MigratePrometheusDocker/#stop-prometheus-container","text":"It's safe to run the stop and rm commands below regardless if Prometheus is running or not since removing the container does not touch the historical data stored in the volume. Stop all containers named Prometheus and remove them. docker stop ( docker ps -fname = prometheus -q ) && docker rm ( docker ps -a -fname = prometheus -q ) Docker may complain if the container is not running, like so. You can ignore this. Ignorable output when container is not running (click me) \"docker stop\" requires at least 1 argument. See 'docker stop --help' . Usage: docker stop [ OPTIONS ] CONTAINER [ CONTAINER... ] Stop one or more running containers","title":"Stop Prometheus container"},{"location":"MigratePrometheusDocker/#find-the-name-of-the-prometheus-volume-that-has-the-historical-data","text":"docker volume ls -f name = prometheus -q Output should look like this: harvest-22080-1_linux_amd64_prometheus_data # historical Prometheus data here harvest_prometheus_data # it is fine if this line is missing We want to copy the historical data from harvest-22080-1_linux_amd64_prometheus_data to harvest_prometheus_data If harvest_prometheus_data already exists, you need to decide if you want to move that volume's data to a different volume or remove it. If you want to remove the volume, run docker volume rm harvest_prometheus_data . If you want to move the data, adjust the command below to first copy harvest_prometheus_data to a different volume and then remove it.","title":"Find the name of the Prometheus volume that has the historical data"},{"location":"MigratePrometheusDocker/#create-new-prometheus-volume","text":"We're going to create a new mount named, harvest_prometheus_data by executing: docker volume create --name harvest_prometheus_data","title":"Create new Prometheus volume"},{"location":"MigratePrometheusDocker/#copy-the-historical-prometheus-data","text":"We will copy the historical Prometheus data from the old volume to the new one by mounting both volumes and copying data between them. NOTE : Prometheus only supports copying a single volume. It will not work if you attempt to copy multiple volumes into the same destination volume. # replace `HISTORICAL_VOLUME` with the name of the Prometheus volume that contains you historical data found in step 2. docker run --rm -it -v $HISTORICAL_VOLUME :/from -v harvest_prometheus_data:/to alpine ash -c \"cd /from ; cp -av . /to\" Output will look something like this: './wal' -> '/to/./wal' './wal/00000000' -> '/to/./wal/00000000' './chunks_head' -> '/to/./chunks_head' ...","title":"Copy the historical Prometheus data"},{"location":"MigratePrometheusDocker/#optionally-remove-historical-prometheus-data","text":"Before removing the historical data, start your compose stack and make sure everything works. Once you're satisfied that you can destroy the old data, remove it like so. # replace `HISTORICAL_VOLUME` with the name of the Prometheus volume that contains your historical data found in step 2. docker volume rm $HISTORICAL_VOLUME","title":"Optionally remove historical Prometheus data"},{"location":"MigratePrometheusDocker/#reference","text":"Rename Docker Volume","title":"Reference"},{"location":"configure-ems/","text":"EMS collector \u00b6 The EMS collector collects ONTAP event management system ( EMS) events via the ONTAP REST API. This collector uses a YAML template file to define which events to collect, export, and what labels to attach to each metric. This means you can collect new EMS events or attach new labels by editing the default template file or by extending existing templates . The default template file contains 60+ EMS events. Supported ONTAP Systems \u00b6 Any cDOT ONTAP system using 9.6 or higher. Requirements \u00b6 It is recommended to create a read-only user on the ONTAP system. See prepare an ONTAP cDOT cluster for details. Metrics \u00b6 This collector collects EMS events from ONTAP and for each received EMS event, creates new metrics prefixed with ems_events . Harvest supports two types of ONTAP EMS events: Normal EMS events Single shot events. When ONTAP detects a problem, an event is raised. When the issue is addressed, ONTAP does not raise another event reflecting that the problem was resolved. Bookend EMS events ONTAP creates bookend events in matching pairs. ONTAP creates an event when an issue is detected and another paired event when the event is resolved. Typically, these events share a common set of properties. Collector Configuration \u00b6 The parameters of the collector are distributed across three files: Harvest configuration file (default: harvest.yml ) EMS collector configuration file (default: conf/ems/default.yaml ) EMS template file (located in conf/ems/9.6.0/ems.yaml ) Except for addr , datacenter , and auth_style , all other parameters of the EMS collector can be defined in either of these three files. Parameters defined in the lower-level files, override parameters in the higher-level file. This allows you to configure each EMS event individually, or use the same parameters for all events. EMS Collector Configuration File \u00b6 This configuration file contains the parameters that are used to configure the EMS collector. These parameters can be defined in your harvest.yml or conf/ems/default.yaml file. parameter type description default client_timeout Go duration how long to wait for server responses 1m schedule list, required the polling frequency of the collector/object. Should include exactly the following two elements in the order specified: - instance Go duration polling frequency for updating the instance cache (example value: 24h = 1440m ) - data Go duration polling frequency for updating the data cache (example value: 3m ) Note Harvest allows defining poll intervals on sub-second level (e.g. 1ms ), however keep in mind the following: API response of an ONTAP system can take several seconds, so the collector is likely to enter failed state if the poll interval is less than client_timeout . Small poll intervals will create significant workload on the ONTAP system. The EMS configuration file should contain the following section mapping the Ems object to the corresponding template file. objects : Ems : ems.yaml Even though the EMS mapping shown above references a single file named ems.yaml , there may be multiple versions of that file across subdirectories named after ONTAP releases. See cDOT for examples. At runtime, the EMS collector will select the appropriate object configuration file that most closely matches the targeted ONTAP system. EMS Template File \u00b6 The EMS template file should contain the following parameters: parameter type description default name string display name of the collector. this matches the named defined in your conf/ems/default.yaml file EMS object string short name of the object, used to prefix metrics ems query string REST API endpoint used to query EMS events api/support/ems/events exports list list of default labels attached to each exported metric events list list of EMS events to collect. See Event Parameters Event Parameters \u00b6 This section defines the list of EMS events you want to collect, which properties to export, what labels to attach, and how to handle bookend pairs. The EMS event template parameters are explained below along with an example for reference. name is the ONTAP EMS event name. (collect ONTAP EMS events with the name of LUN.offline ) matches list of name-value pairs used to further filter ONTAP events. Some EMS events include arguments and these name-value pairs provide a way to filter on those arguments. (Only collect ONTAP EMS events where volume_name has the value abc_vol ) exports list of EMS event parameters to export. These exported parameters are attached as labels to each matching EMS event. labels that are prefixed with ^^ use that parameter to define instance uniqueness . resolve_when_ems (applicable to bookend events only). Lists the resolving event that pairs with the issuing event name is the ONTAP EMS event name of the resolving EMS event ( LUN.online ). When the resolving event is received, the issuing EMS event will be resolved. In this example, Harvest will raise an event when it finds the ONTAP EMS event named LUN.offline and that event will be resolved when the EMS event named LUN.online is received. resolve_after (optional, Go duration, default = 28 days) resolve the issuing EMS after the specified duration has elapsed ( 672h = 28d ). If the bookend pair is not received within the resolve_after duration, the issuing EMS event expires. resolve_key (optional) bookend key used to match bookend EMS events. Defaults to prefixed ( ^^ ) labels in exports section. resolve_key allows you to override what is defined in the exports section. Labels are only exported if they are included in the exports section. Example template definition for the LUN.offline EMS event: - name : LUN.offline matches : - name : volume_name value : abc_vol exports : - ^^parameters.object_uuid => object_uuid - parameters.object_type => object_type - parameters.lun_path => lun_path - parameters.volume_name => volume - parameters.volume_dsid => volume_ds_id resolve_when_ems : - name : LUN.online resolve_after : 672h resolve_key : - ^^parameters.object_uuid => object_uuid How do I find the full list of supported EMS events? \u00b6 ONTAP documents the list of EMS events created in the ONTAP EMS Event Catalog . You can also query a live system and ask the cluster for its event catalog like so: curl --insecure --user \"user:password\" 'https://10.61.124.110/api/support/ems/messages?fields=*' Example Output { \"records\": [ { \"name\": \"AccessCache.NearLimits\", \"severity\": \"alert\", \"description\": \"This message occurs when the access cache module is near its limits for entries or export rules. Reaching these limits can prevent new clients from being able to mount and perform I/O on the storage system, and can also cause clients to be granted or denied access based on stale cached information.\", \"corrective_action\": \"Ensure that the number of clients accessing the storage system continues to be below the limits for access cache entries and export rules across those entries. If the set of clients accessing the storage system is constantly changing, consider using the \\\"vserver export-policy access-cache config modify\\\" command to reduce the harvest timeout parameter so that cache entries for clients that are no longer accessing the storage system can be evicted sooner.\", \"snmp_trap_type\": \"severity_based\", \"deprecated\": false }, ... { \"name\": \"ztl.smap.online.status\", \"severity\": \"notice\", \"description\": \"This message occurs when the specified partition on a Software Defined Flash drive could not be onlined due to internal S/W or device error.\", \"corrective_action\": \"NONE\", \"snmp_trap_type\": \"severity_based\", \"deprecated\": false } ], \"num_records\": 7273 } Ems Prometheus Alerts \u00b6 Refer Prometheus-Alerts","title":"EMS"},{"location":"configure-ems/#ems-collector","text":"The EMS collector collects ONTAP event management system ( EMS) events via the ONTAP REST API. This collector uses a YAML template file to define which events to collect, export, and what labels to attach to each metric. This means you can collect new EMS events or attach new labels by editing the default template file or by extending existing templates . The default template file contains 60+ EMS events.","title":"EMS collector"},{"location":"configure-ems/#supported-ontap-systems","text":"Any cDOT ONTAP system using 9.6 or higher.","title":"Supported ONTAP Systems"},{"location":"configure-ems/#requirements","text":"It is recommended to create a read-only user on the ONTAP system. See prepare an ONTAP cDOT cluster for details.","title":"Requirements"},{"location":"configure-ems/#metrics","text":"This collector collects EMS events from ONTAP and for each received EMS event, creates new metrics prefixed with ems_events . Harvest supports two types of ONTAP EMS events: Normal EMS events Single shot events. When ONTAP detects a problem, an event is raised. When the issue is addressed, ONTAP does not raise another event reflecting that the problem was resolved. Bookend EMS events ONTAP creates bookend events in matching pairs. ONTAP creates an event when an issue is detected and another paired event when the event is resolved. Typically, these events share a common set of properties.","title":"Metrics"},{"location":"configure-ems/#collector-configuration","text":"The parameters of the collector are distributed across three files: Harvest configuration file (default: harvest.yml ) EMS collector configuration file (default: conf/ems/default.yaml ) EMS template file (located in conf/ems/9.6.0/ems.yaml ) Except for addr , datacenter , and auth_style , all other parameters of the EMS collector can be defined in either of these three files. Parameters defined in the lower-level files, override parameters in the higher-level file. This allows you to configure each EMS event individually, or use the same parameters for all events.","title":"Collector Configuration"},{"location":"configure-ems/#ems-collector-configuration-file","text":"This configuration file contains the parameters that are used to configure the EMS collector. These parameters can be defined in your harvest.yml or conf/ems/default.yaml file. parameter type description default client_timeout Go duration how long to wait for server responses 1m schedule list, required the polling frequency of the collector/object. Should include exactly the following two elements in the order specified: - instance Go duration polling frequency for updating the instance cache (example value: 24h = 1440m ) - data Go duration polling frequency for updating the data cache (example value: 3m ) Note Harvest allows defining poll intervals on sub-second level (e.g. 1ms ), however keep in mind the following: API response of an ONTAP system can take several seconds, so the collector is likely to enter failed state if the poll interval is less than client_timeout . Small poll intervals will create significant workload on the ONTAP system. The EMS configuration file should contain the following section mapping the Ems object to the corresponding template file. objects : Ems : ems.yaml Even though the EMS mapping shown above references a single file named ems.yaml , there may be multiple versions of that file across subdirectories named after ONTAP releases. See cDOT for examples. At runtime, the EMS collector will select the appropriate object configuration file that most closely matches the targeted ONTAP system.","title":"EMS Collector Configuration File"},{"location":"configure-ems/#ems-template-file","text":"The EMS template file should contain the following parameters: parameter type description default name string display name of the collector. this matches the named defined in your conf/ems/default.yaml file EMS object string short name of the object, used to prefix metrics ems query string REST API endpoint used to query EMS events api/support/ems/events exports list list of default labels attached to each exported metric events list list of EMS events to collect. See Event Parameters","title":"EMS Template File"},{"location":"configure-ems/#event-parameters","text":"This section defines the list of EMS events you want to collect, which properties to export, what labels to attach, and how to handle bookend pairs. The EMS event template parameters are explained below along with an example for reference. name is the ONTAP EMS event name. (collect ONTAP EMS events with the name of LUN.offline ) matches list of name-value pairs used to further filter ONTAP events. Some EMS events include arguments and these name-value pairs provide a way to filter on those arguments. (Only collect ONTAP EMS events where volume_name has the value abc_vol ) exports list of EMS event parameters to export. These exported parameters are attached as labels to each matching EMS event. labels that are prefixed with ^^ use that parameter to define instance uniqueness . resolve_when_ems (applicable to bookend events only). Lists the resolving event that pairs with the issuing event name is the ONTAP EMS event name of the resolving EMS event ( LUN.online ). When the resolving event is received, the issuing EMS event will be resolved. In this example, Harvest will raise an event when it finds the ONTAP EMS event named LUN.offline and that event will be resolved when the EMS event named LUN.online is received. resolve_after (optional, Go duration, default = 28 days) resolve the issuing EMS after the specified duration has elapsed ( 672h = 28d ). If the bookend pair is not received within the resolve_after duration, the issuing EMS event expires. resolve_key (optional) bookend key used to match bookend EMS events. Defaults to prefixed ( ^^ ) labels in exports section. resolve_key allows you to override what is defined in the exports section. Labels are only exported if they are included in the exports section. Example template definition for the LUN.offline EMS event: - name : LUN.offline matches : - name : volume_name value : abc_vol exports : - ^^parameters.object_uuid => object_uuid - parameters.object_type => object_type - parameters.lun_path => lun_path - parameters.volume_name => volume - parameters.volume_dsid => volume_ds_id resolve_when_ems : - name : LUN.online resolve_after : 672h resolve_key : - ^^parameters.object_uuid => object_uuid","title":"Event Parameters"},{"location":"configure-ems/#how-do-i-find-the-full-list-of-supported-ems-events","text":"ONTAP documents the list of EMS events created in the ONTAP EMS Event Catalog . You can also query a live system and ask the cluster for its event catalog like so: curl --insecure --user \"user:password\" 'https://10.61.124.110/api/support/ems/messages?fields=*' Example Output { \"records\": [ { \"name\": \"AccessCache.NearLimits\", \"severity\": \"alert\", \"description\": \"This message occurs when the access cache module is near its limits for entries or export rules. Reaching these limits can prevent new clients from being able to mount and perform I/O on the storage system, and can also cause clients to be granted or denied access based on stale cached information.\", \"corrective_action\": \"Ensure that the number of clients accessing the storage system continues to be below the limits for access cache entries and export rules across those entries. If the set of clients accessing the storage system is constantly changing, consider using the \\\"vserver export-policy access-cache config modify\\\" command to reduce the harvest timeout parameter so that cache entries for clients that are no longer accessing the storage system can be evicted sooner.\", \"snmp_trap_type\": \"severity_based\", \"deprecated\": false }, ... { \"name\": \"ztl.smap.online.status\", \"severity\": \"notice\", \"description\": \"This message occurs when the specified partition on a Software Defined Flash drive could not be onlined due to internal S/W or device error.\", \"corrective_action\": \"NONE\", \"snmp_trap_type\": \"severity_based\", \"deprecated\": false } ], \"num_records\": 7273 }","title":"How do I find the full list of supported EMS events?"},{"location":"configure-ems/#ems-prometheus-alerts","text":"Refer Prometheus-Alerts","title":"Ems Prometheus Alerts"},{"location":"configure-grafana/","text":"Grafana \u00b6 Grafana hosts the Harvest dashboards and needs to be setup before importing your dashboards .","title":"Configure Grafana"},{"location":"configure-grafana/#grafana","text":"Grafana hosts the Harvest dashboards and needs to be setup before importing your dashboards .","title":"Grafana"},{"location":"configure-harvest-advanced/","text":"This chapter describes additional advanced configuration possibilities of NetApp Harvest. For a typical installation this level of detail is likely not needed.","title":"Configure Harvest (advanced)"},{"location":"configure-harvest-basic/","text":"The main configuration file, harvest.yml , consists of the following sections, described below: Pollers \u00b6 All pollers are defined in harvest.yml , the main configuration file of Harvest, under the section Pollers . parameter type description default Poller name (header) required Poller name, user-defined value datacenter required Datacenter name, user-defined value addr required by some collectors IPv4 or FQDN of the target system collectors required List of collectors to run for this poller exporters required List of exporter names from the Exporters section. Note: this should be the name of the exporter (e.g. prometheus1 ), not the value of the exporter key (e.g. Prometheus ) auth_style required by Zapi* collectors Either basic_auth or certificate_auth basic_auth username , password required if auth_style is basic_auth ssl_cert , ssl_key optional if auth_style is certificate_auth Absolute paths to SSL (client) certificate and key used to authenticate with the target system. If not provided, the poller will look for <hostname>.key and <hostname>.pem in $HARVEST_HOME/cert/ . To create certificates for ONTAP systems, see using certificate authentication use_insecure_tls optional, bool If true, disable TLS verification when connecting to ONTAP cluster false credentials_file optional, string Path to a yaml file that contains cluster credentials. The file should have the same shape as harvest.yml . See here for examples. Path can be relative to harvest.yml or absolute tls_min_version optional, string Minimum TLS version to use when connecting to ONTAP cluster: One of tls10, tls11, tls12 or tls13 Platform decides labels optional, list of key-value pairs Each of the key-value pairs will be added to a poller's metrics. Details below log_max_bytes Maximum size of the log file before it will be rotated 5_242_880 (5 MB) log_max_files Number of rotated log files to keep 5 log optional, list of collector names Matching collectors log their ZAPI request/response Defaults \u00b6 This section is optional. If there are parameters identical for all your pollers (e.g. datacenter, authentication method, login preferences), they can be grouped under this section. The poller section will be checked first and if the values aren't found there, the defaults will be consulted. Exporters \u00b6 All exporters need two types of parameters: exporter parameters - defined in harvest.yml under Exporters section export_options - these options are defined in the Matrix data structure that is emitted from collectors and plugins The following two parameters are required for all exporters: parameter type description default Exporter name (header) required Name of the exporter instance, this is a user-defined value exporter required Name of the exporter class (e.g. Prometheus, InfluxDB, Http) - these can be found under the cmd/exporters/ directory Note: when we talk about the Prometheus Exporter or InfluxDB Exporter , we mean the Harvest modules that send the data to a database, NOT the names used to refer to the actual databases. Prometheus Exporter \u00b6 InfluxDB Exporter \u00b6 Tools \u00b6 This section is optional. You can uncomment the grafana_api_token key and add your Grafana API token so harvest does not prompt you for the key when importing dashboards. Tools: #grafana_api_token: 'aaa-bbb-ccc-ddd' Configuring collectors \u00b6 Collectors are configured by their own configuration files ( templates ), which are stored in subdirectories in conf/ . Most collectors run concurrently and collect a subset of related metrics. For example, node related metrics are grouped together and run independently of the disk related metrics. Below is a snippet from conf/zapi/default.yaml In this example, the default.yaml template contains a list of objects (e.g. Node) that reference sub-templates (e.g. node.yaml). This decomposition groups related metrics together and at runtime, a Zapi collector per object will be created and each of these collectors will run concurrently. Using the snippet below, we expect there to be four Zapi collectors running, each with a different subtemplate and object. collector: Zapi objects: Node: node.yaml Aggregate: aggr.yaml Volume: volume.yaml SnapMirror: snapmirror.yaml At start-up, Harvest looks for two files ( default.yaml and custom.yaml ) in the conf directory of the collector (e.g. conf/zapi/default.yaml ). The default.yaml is installed by default, while the custom.yaml is an optional file you can create to add new templates . When present, the custom.yaml file will be merged with the default.yaml file. This behavior can be overridden in your harvest.yml , see here for an example. For a list of collector-specific parameters, refer to their individual documentation. Zapi and ZapiPerf \u00b6 Rest and RestPerf \u00b6 EMS \u00b6 StorageGRID \u00b6 Unix \u00b6 Labels \u00b6 Labels offer a way to add additional key-value pairs to a poller's metrics. These allow you to tag a cluster's metrics in a cross-cutting fashion. Here's an example: cluster-03: datacenter: DC-01 addr: 10.0.1.1 labels: - org: meg # add an org label with the value \"meg\" - ns: rtp # add a namespace label with the value \"rtp\" These settings add two key-value pairs to each metric collected from cluster-03 like this: node_vol_cifs_write_data{org=\"meg\",ns=\"rtp\",datacenter=\"DC-01\",cluster=\"cluster-03\",node=\"umeng-aff300-05\"} 10 Keep in mind that each unique combination of key-value pairs increases the amount of stored data. Use them sparingly. See PrometheusNaming for details. Credentials File \u00b6 If you would rather not list cluster credentials in your harvest.yml , you can use the credentials_file section in your harvest.yml to point to a file that contains the credentials. At runtime, the credentials_file will be read and the included credentials will be used to authenticate with the matching cluster(s). This is handy when integrating with 3rd party credential stores. See #884 for examples. The format of the credentials_file is similar to harvest.yml and can contain multiple cluster credentials. Example: Snippet from harvest.yml : Pollers : cluster1 : addr : 10.193.48.11 credentials_file : secrets/cluster1.yml exporters : - prom1 File secrets/cluster1.yml : Pollers : cluster1 : username : harvest password : foo","title":"Configure Harvest (basic)"},{"location":"configure-harvest-basic/#pollers","text":"All pollers are defined in harvest.yml , the main configuration file of Harvest, under the section Pollers . parameter type description default Poller name (header) required Poller name, user-defined value datacenter required Datacenter name, user-defined value addr required by some collectors IPv4 or FQDN of the target system collectors required List of collectors to run for this poller exporters required List of exporter names from the Exporters section. Note: this should be the name of the exporter (e.g. prometheus1 ), not the value of the exporter key (e.g. Prometheus ) auth_style required by Zapi* collectors Either basic_auth or certificate_auth basic_auth username , password required if auth_style is basic_auth ssl_cert , ssl_key optional if auth_style is certificate_auth Absolute paths to SSL (client) certificate and key used to authenticate with the target system. If not provided, the poller will look for <hostname>.key and <hostname>.pem in $HARVEST_HOME/cert/ . To create certificates for ONTAP systems, see using certificate authentication use_insecure_tls optional, bool If true, disable TLS verification when connecting to ONTAP cluster false credentials_file optional, string Path to a yaml file that contains cluster credentials. The file should have the same shape as harvest.yml . See here for examples. Path can be relative to harvest.yml or absolute tls_min_version optional, string Minimum TLS version to use when connecting to ONTAP cluster: One of tls10, tls11, tls12 or tls13 Platform decides labels optional, list of key-value pairs Each of the key-value pairs will be added to a poller's metrics. Details below log_max_bytes Maximum size of the log file before it will be rotated 5_242_880 (5 MB) log_max_files Number of rotated log files to keep 5 log optional, list of collector names Matching collectors log their ZAPI request/response","title":"Pollers"},{"location":"configure-harvest-basic/#defaults","text":"This section is optional. If there are parameters identical for all your pollers (e.g. datacenter, authentication method, login preferences), they can be grouped under this section. The poller section will be checked first and if the values aren't found there, the defaults will be consulted.","title":"Defaults"},{"location":"configure-harvest-basic/#exporters","text":"All exporters need two types of parameters: exporter parameters - defined in harvest.yml under Exporters section export_options - these options are defined in the Matrix data structure that is emitted from collectors and plugins The following two parameters are required for all exporters: parameter type description default Exporter name (header) required Name of the exporter instance, this is a user-defined value exporter required Name of the exporter class (e.g. Prometheus, InfluxDB, Http) - these can be found under the cmd/exporters/ directory Note: when we talk about the Prometheus Exporter or InfluxDB Exporter , we mean the Harvest modules that send the data to a database, NOT the names used to refer to the actual databases.","title":"Exporters"},{"location":"configure-harvest-basic/#prometheus-exporter","text":"","title":"Prometheus Exporter"},{"location":"configure-harvest-basic/#influxdb-exporter","text":"","title":"InfluxDB Exporter"},{"location":"configure-harvest-basic/#tools","text":"This section is optional. You can uncomment the grafana_api_token key and add your Grafana API token so harvest does not prompt you for the key when importing dashboards. Tools: #grafana_api_token: 'aaa-bbb-ccc-ddd'","title":"Tools"},{"location":"configure-harvest-basic/#configuring-collectors","text":"Collectors are configured by their own configuration files ( templates ), which are stored in subdirectories in conf/ . Most collectors run concurrently and collect a subset of related metrics. For example, node related metrics are grouped together and run independently of the disk related metrics. Below is a snippet from conf/zapi/default.yaml In this example, the default.yaml template contains a list of objects (e.g. Node) that reference sub-templates (e.g. node.yaml). This decomposition groups related metrics together and at runtime, a Zapi collector per object will be created and each of these collectors will run concurrently. Using the snippet below, we expect there to be four Zapi collectors running, each with a different subtemplate and object. collector: Zapi objects: Node: node.yaml Aggregate: aggr.yaml Volume: volume.yaml SnapMirror: snapmirror.yaml At start-up, Harvest looks for two files ( default.yaml and custom.yaml ) in the conf directory of the collector (e.g. conf/zapi/default.yaml ). The default.yaml is installed by default, while the custom.yaml is an optional file you can create to add new templates . When present, the custom.yaml file will be merged with the default.yaml file. This behavior can be overridden in your harvest.yml , see here for an example. For a list of collector-specific parameters, refer to their individual documentation.","title":"Configuring collectors"},{"location":"configure-harvest-basic/#zapi-and-zapiperf","text":"","title":"Zapi and ZapiPerf"},{"location":"configure-harvest-basic/#rest-and-restperf","text":"","title":"Rest and RestPerf"},{"location":"configure-harvest-basic/#ems","text":"","title":"EMS"},{"location":"configure-harvest-basic/#storagegrid","text":"","title":"StorageGRID"},{"location":"configure-harvest-basic/#unix","text":"","title":"Unix"},{"location":"configure-harvest-basic/#labels","text":"Labels offer a way to add additional key-value pairs to a poller's metrics. These allow you to tag a cluster's metrics in a cross-cutting fashion. Here's an example: cluster-03: datacenter: DC-01 addr: 10.0.1.1 labels: - org: meg # add an org label with the value \"meg\" - ns: rtp # add a namespace label with the value \"rtp\" These settings add two key-value pairs to each metric collected from cluster-03 like this: node_vol_cifs_write_data{org=\"meg\",ns=\"rtp\",datacenter=\"DC-01\",cluster=\"cluster-03\",node=\"umeng-aff300-05\"} 10 Keep in mind that each unique combination of key-value pairs increases the amount of stored data. Use them sparingly. See PrometheusNaming for details.","title":"Labels"},{"location":"configure-harvest-basic/#credentials-file","text":"If you would rather not list cluster credentials in your harvest.yml , you can use the credentials_file section in your harvest.yml to point to a file that contains the credentials. At runtime, the credentials_file will be read and the included credentials will be used to authenticate with the matching cluster(s). This is handy when integrating with 3rd party credential stores. See #884 for examples. The format of the credentials_file is similar to harvest.yml and can contain multiple cluster credentials. Example: Snippet from harvest.yml : Pollers : cluster1 : addr : 10.193.48.11 credentials_file : secrets/cluster1.yml exporters : - prom1 File secrets/cluster1.yml : Pollers : cluster1 : username : harvest password : foo","title":"Credentials File"},{"location":"configure-rest/","text":"Rest Collector \u00b6 The Rest collectors uses the REST protocol to collect data from ONTAP systems. The RestPerf collector is an extension of this collector, therefore they share many parameters and configuration settings. Target System \u00b6 Target system can be cDot ONTAP system. 9.12.1 and after are supported, however the default configuration files may not completely match with all versions. See REST Strategy for more details. Requirements \u00b6 No SDK or other requirements. It is recommended to create a read-only user for Harvest on the ONTAP system (see prepare monitored clusters for details) Metrics \u00b6 The collector collects a dynamic set of metrics. ONTAP returns JSON documents and Harvest allows you to define templates to extract values from the JSON document via a dot notation path. You can view ONTAP's full set of REST APIs by visiting https://docs.netapp.com/us-en/ontap-automation/reference/api_reference.html#access-a-copy-of-the-ontap-rest-api-reference-documentation As an example, the /api/storage/aggregates endpoint, lists all data aggregates in the cluster. Below is an example response from this endpoint: { \"records\" : [ { \"uuid\" : \"3e59547d-298a-4967-bd0f-8ae96cead08c\" , \"name\" : \"umeng_aff300_aggr2\" , \"space\" : { \"block_storage\" : { \"size\" : 8117898706944 , \"available\" : 4889853616128 } }, \"state\" : \"online\" , \"volume_count\" : 36 } ] } The Rest collector will take this document, extract the records section and convert the metrics above into: name , space.block_storage.size , space.block_storage.available , state and volume_count . Metric names will be taken, as is, unless you specify a short display name. See counters for more details. Parameters \u00b6 The parameters of the collector are distributed across three files: Harvest configuration file (default: harvest.yml ) Rest configuration file (default: conf/rest/default.yaml ) Each object has its own configuration file (located in conf/rest/$version/ ) Except for addr and datacenter , all other parameters of the Rest collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level ones. This allows you to configure each object individually, or use the same parameters for all objects. The full set of parameters are described below . Collector configuration file \u00b6 This configuration file contains a list of objects that should be collected and the filenames of their templates ( explained in the next section). Additionally, this file contains the parameters that are applied as defaults to all objects. As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well. parameter type description default client_timeout duration (Go-syntax) how long to wait for server responses 30s schedule list, required how frequently to retrieve metrics from ONTAP - data duration (Go-syntax) how frequently this collector/object should retrieve metrics from ONTAP 3 minutes The template should define objects in the objects section. Example: objects : Aggr : aggr.yaml For each object, we define the filename of the object configuration file. The object configuration files are located in subdirectories matching the ONTAP version that was used to create these files. It is possible to have multiple version-subdirectories for multiple ONTAP versions. At runtime, the collector will select the object configuration file that closest matches the version of the target ONTAP system. Object configuration file \u00b6 The Object configuration file (\"subtemplate\") should contain the following parameters: parameter type description default name string, required display name of the collector that will collect this object query string, required REST endpoint used to issue a REST request object string, required short name of the object counters string list of counters to collect (see notes below) plugins list plugins and their parameters to run on the collected data export_options list parameters to pass to exporters (see notes below) counters \u00b6 This section defines the list of counters that will be collected. These counters can be labels, numeric metrics or histograms. The exact property of each counter is fetched from ONTAP and updated periodically. The display name of a counter can be changed with => (e.g., space.block_storage.size => space_total ). Counters that are stored as labels will only be exported if they are included in the export_options section. export_options \u00b6 Parameters in this section tell the exporters how to handle the collected data. The set of parameters varies by exporter. For Prometheus and InfluxDB exporters, the following parameters can be defined: instances_keys (list): display names of labels to export with each data-point instance_labels (list): display names of labels to export as a separate data-point include_all_labels (bool): export all labels with each data-point (overrides previous two parameters) RestPerf Collector \u00b6 RestPerf collects performance metrics from ONTAP systems using the REST protocol. The collector is designed to be easily extendable to collect new objects or to collect additional counters from already configured objects. This collector is an extension of the Rest collector . The major difference between them is that RestPerf collects only the performance ( perf ) APIs. Additionally, RestPerf always calculates final values from the deltas of two subsequent polls. Metrics \u00b6 RestPerf metrics are calculated the same as ZapiPerf metrics. More details about how performance metrics are calculated can be found here . Parameters \u00b6 The parameters of the collector are distributed across three files: Harvest configuration file (default: harvest.yml ) RestPerf configuration file (default: conf/restperf/default.yaml ) Each object has its own configuration file (located in conf/restperf/$version/ ) Except for addr , datacenter and auth_style , all other parameters of the RestPerf collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level file. This allows the user to configure each objects individually, or use the same parameters for all objects. The full set of parameters are described below . RestPerf configuration file \u00b6 This configuration file (the \"template\") contains a list of objects that should be collected and the filenames of their configuration (explained in the next section). Additionally, this file contains the parameters that are applied as defaults to all objects. (As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well). parameter type description default use_insecure_tls bool, optional skip verifying TLS certificate of the target system false client_timeout duration (Go-syntax) how long to wait for server responses 30s latency_io_reqd int, optional threshold of IOPs for calculating latency metrics (latencies based on very few IOPs are unreliable) 100 schedule list, required the poll frequencies of the collector/object, should include exactly these three elements in the exact same other: - counter duration (Go-syntax) poll frequency of updating the counter metadata cache 20 minutes - instance duration (Go-syntax) poll frequency of updating the instance cache 10 minutes - data duration (Go-syntax) poll frequency of updating the data cache Note Harvest allows defining poll intervals on sub-second level (e.g. 1ms ), however keep in mind the following: API response of an ONTAP system can take several seconds, so the collector is likely to enter failed state if the poll interval is less than client_timeout . Small poll intervals will create significant workload on the ONTAP system, as many counters are aggregated on-demand. Some metric values become less significant if they are calculated for very short intervals (e.g. latencies) 1 minute The template should define objects in the objects section. Example: objects : SystemNode : system_node.yaml HostAdapter : hostadapter.yaml Note that for each object we only define the filename of the object configuration file. The object configuration files are located in subdirectories matching to the ONTAP version that was used to create these files. It is possible to have multiple version-subdirectories for multiple ONTAP versions. At runtime, the collector will select the object configuration file that closest matches to the version of the target ONTAP system. (A mismatch is tolerated since RestPerf will fetch and validate counter metadata from the system.) Object configuration file \u00b6 Refer Object configuration file counters \u00b6 Refer Counters Some counters require a \"base-counter\" for post-processing. If the base-counter is missing, RestPerf will still run, but the missing data won't be exported. export_options \u00b6 Refer Export Options","title":"REST"},{"location":"configure-rest/#rest-collector","text":"The Rest collectors uses the REST protocol to collect data from ONTAP systems. The RestPerf collector is an extension of this collector, therefore they share many parameters and configuration settings.","title":"Rest Collector"},{"location":"configure-rest/#target-system","text":"Target system can be cDot ONTAP system. 9.12.1 and after are supported, however the default configuration files may not completely match with all versions. See REST Strategy for more details.","title":"Target System"},{"location":"configure-rest/#requirements","text":"No SDK or other requirements. It is recommended to create a read-only user for Harvest on the ONTAP system (see prepare monitored clusters for details)","title":"Requirements"},{"location":"configure-rest/#metrics","text":"The collector collects a dynamic set of metrics. ONTAP returns JSON documents and Harvest allows you to define templates to extract values from the JSON document via a dot notation path. You can view ONTAP's full set of REST APIs by visiting https://docs.netapp.com/us-en/ontap-automation/reference/api_reference.html#access-a-copy-of-the-ontap-rest-api-reference-documentation As an example, the /api/storage/aggregates endpoint, lists all data aggregates in the cluster. Below is an example response from this endpoint: { \"records\" : [ { \"uuid\" : \"3e59547d-298a-4967-bd0f-8ae96cead08c\" , \"name\" : \"umeng_aff300_aggr2\" , \"space\" : { \"block_storage\" : { \"size\" : 8117898706944 , \"available\" : 4889853616128 } }, \"state\" : \"online\" , \"volume_count\" : 36 } ] } The Rest collector will take this document, extract the records section and convert the metrics above into: name , space.block_storage.size , space.block_storage.available , state and volume_count . Metric names will be taken, as is, unless you specify a short display name. See counters for more details.","title":"Metrics"},{"location":"configure-rest/#parameters","text":"The parameters of the collector are distributed across three files: Harvest configuration file (default: harvest.yml ) Rest configuration file (default: conf/rest/default.yaml ) Each object has its own configuration file (located in conf/rest/$version/ ) Except for addr and datacenter , all other parameters of the Rest collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level ones. This allows you to configure each object individually, or use the same parameters for all objects. The full set of parameters are described below .","title":"Parameters"},{"location":"configure-rest/#collector-configuration-file","text":"This configuration file contains a list of objects that should be collected and the filenames of their templates ( explained in the next section). Additionally, this file contains the parameters that are applied as defaults to all objects. As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well. parameter type description default client_timeout duration (Go-syntax) how long to wait for server responses 30s schedule list, required how frequently to retrieve metrics from ONTAP - data duration (Go-syntax) how frequently this collector/object should retrieve metrics from ONTAP 3 minutes The template should define objects in the objects section. Example: objects : Aggr : aggr.yaml For each object, we define the filename of the object configuration file. The object configuration files are located in subdirectories matching the ONTAP version that was used to create these files. It is possible to have multiple version-subdirectories for multiple ONTAP versions. At runtime, the collector will select the object configuration file that closest matches the version of the target ONTAP system.","title":"Collector configuration file"},{"location":"configure-rest/#object-configuration-file","text":"The Object configuration file (\"subtemplate\") should contain the following parameters: parameter type description default name string, required display name of the collector that will collect this object query string, required REST endpoint used to issue a REST request object string, required short name of the object counters string list of counters to collect (see notes below) plugins list plugins and their parameters to run on the collected data export_options list parameters to pass to exporters (see notes below)","title":"Object configuration file"},{"location":"configure-rest/#counters","text":"This section defines the list of counters that will be collected. These counters can be labels, numeric metrics or histograms. The exact property of each counter is fetched from ONTAP and updated periodically. The display name of a counter can be changed with => (e.g., space.block_storage.size => space_total ). Counters that are stored as labels will only be exported if they are included in the export_options section.","title":"counters"},{"location":"configure-rest/#export_options","text":"Parameters in this section tell the exporters how to handle the collected data. The set of parameters varies by exporter. For Prometheus and InfluxDB exporters, the following parameters can be defined: instances_keys (list): display names of labels to export with each data-point instance_labels (list): display names of labels to export as a separate data-point include_all_labels (bool): export all labels with each data-point (overrides previous two parameters)","title":"export_options"},{"location":"configure-rest/#restperf-collector","text":"RestPerf collects performance metrics from ONTAP systems using the REST protocol. The collector is designed to be easily extendable to collect new objects or to collect additional counters from already configured objects. This collector is an extension of the Rest collector . The major difference between them is that RestPerf collects only the performance ( perf ) APIs. Additionally, RestPerf always calculates final values from the deltas of two subsequent polls.","title":"RestPerf Collector"},{"location":"configure-rest/#metrics_1","text":"RestPerf metrics are calculated the same as ZapiPerf metrics. More details about how performance metrics are calculated can be found here .","title":"Metrics"},{"location":"configure-rest/#parameters_1","text":"The parameters of the collector are distributed across three files: Harvest configuration file (default: harvest.yml ) RestPerf configuration file (default: conf/restperf/default.yaml ) Each object has its own configuration file (located in conf/restperf/$version/ ) Except for addr , datacenter and auth_style , all other parameters of the RestPerf collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level file. This allows the user to configure each objects individually, or use the same parameters for all objects. The full set of parameters are described below .","title":"Parameters"},{"location":"configure-rest/#restperf-configuration-file","text":"This configuration file (the \"template\") contains a list of objects that should be collected and the filenames of their configuration (explained in the next section). Additionally, this file contains the parameters that are applied as defaults to all objects. (As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well). parameter type description default use_insecure_tls bool, optional skip verifying TLS certificate of the target system false client_timeout duration (Go-syntax) how long to wait for server responses 30s latency_io_reqd int, optional threshold of IOPs for calculating latency metrics (latencies based on very few IOPs are unreliable) 100 schedule list, required the poll frequencies of the collector/object, should include exactly these three elements in the exact same other: - counter duration (Go-syntax) poll frequency of updating the counter metadata cache 20 minutes - instance duration (Go-syntax) poll frequency of updating the instance cache 10 minutes - data duration (Go-syntax) poll frequency of updating the data cache Note Harvest allows defining poll intervals on sub-second level (e.g. 1ms ), however keep in mind the following: API response of an ONTAP system can take several seconds, so the collector is likely to enter failed state if the poll interval is less than client_timeout . Small poll intervals will create significant workload on the ONTAP system, as many counters are aggregated on-demand. Some metric values become less significant if they are calculated for very short intervals (e.g. latencies) 1 minute The template should define objects in the objects section. Example: objects : SystemNode : system_node.yaml HostAdapter : hostadapter.yaml Note that for each object we only define the filename of the object configuration file. The object configuration files are located in subdirectories matching to the ONTAP version that was used to create these files. It is possible to have multiple version-subdirectories for multiple ONTAP versions. At runtime, the collector will select the object configuration file that closest matches to the version of the target ONTAP system. (A mismatch is tolerated since RestPerf will fetch and validate counter metadata from the system.)","title":"RestPerf configuration file"},{"location":"configure-rest/#object-configuration-file_1","text":"Refer Object configuration file","title":"Object configuration file"},{"location":"configure-rest/#counters_1","text":"Refer Counters Some counters require a \"base-counter\" for post-processing. If the base-counter is missing, RestPerf will still run, but the missing data won't be exported.","title":"counters"},{"location":"configure-rest/#export_options_1","text":"Refer Export Options","title":"export_options"},{"location":"configure-storagegrid/","text":"StorageGRID Collector \u00b6 The StorageGRID collector uses REST calls to collect data from StorageGRID systems. Target System \u00b6 All StorageGRID versions are supported, however the default configuration files may not completely match with older systems. Requirements \u00b6 No SDK or other requirements. It is recommended to create a read-only user for Harvest on the StorageGRID system (see prepare monitored clusters for details) Metrics \u00b6 The collector collects a dynamic set of metrics via StorageGRID's REST API. StorageGRID returns JSON documents and Harvest allows you to define templates to extract values from the JSON document via a dot notation path. You can view StorageGRID's full set of REST APIs by visiting https://$STORAGE_GRID_HOSTNAME/grid/apidocs.html As an example, the /grid/accounts-cache endpoint, lists the tenant accounts in the cache and includes additional information, such as objectCount and dataBytes. Below is an example response from this endpoint: { \"data\" : [ { \"id\" : \"95245224059574669217\" , \"name\" : \"foople\" , \"policy\" : { \"quotaObjectBytes\" : 50000000000 }, \"objectCount\" : 6 , \"dataBytes\" : 10473454261 } ] } The StorageGRID collector will take this document, extract the data section and convert the metrics above into: name , policy.quotaObjectBytes , objectCount , and dataBytes . Metric names will be taken, as is, unless you specify a short display name. See counters for more details. Parameters \u00b6 The parameters of the collector are distributed across three files: Harvest configuration file (default: harvest.yml ) StorageGRID configuration file (default: conf/storagegrid/default.yaml ) Each object has its own configuration file (located in conf/storagegrid/$version/ ) Except for addr and datacenter , all other parameters of the StorageGRID collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level ones. This allows you to configure each object individually, or use the same parameters for all objects. The full set of parameters are described below . Harvest configuration file \u00b6 Parameters in the poller section should define the following required parameters. parameter type description default Poller name (header) string, required Poller name, user-defined value addr string, required address (IP or FQDN) of the ONTAP system datacenter string, required Datacenter name, user-defined value username , password string, required StorageGRID username and password with at least Tenant accounts permissions collectors list, required Name of collector to run for this poller, use StorageGrid for this collector StorageGRID configuration file \u00b6 This configuration file contains a list of objects that should be collected and the filenames of their templates ( explained in the next section). Additionally, this file contains the parameters that are applied as defaults to all objects. As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well. parameter type description default client_timeout duration (Go-syntax) how long to wait for server responses 30s schedule list, required how frequently to retrieve metrics from StorageGRID - data duration (Go-syntax) how frequently this collector/object should retrieve metrics from StorageGRID 5 minutes The template should define objects in the objects section. Example: objects : Tenant : tenant.yaml For each object, we define the filename of the object configuration file. The object configuration files are located in subdirectories matching the StorageGRID version that was used to create these files. It is possible to have multiple version-subdirectories for multiple StorageGRID versions. At runtime, the collector will select the object configuration file that closest matches the version of the target StorageGRID system. Object configuration file \u00b6 The Object configuration file (\"subtemplate\") should contain the following parameters: parameter type description default name string, required display name of the collector that will collect this object query string, required REST endpoint used to issue a REST request object string, required short name of the object api string StorageGRID REST endpoint version to use, overrides default management API version 3 counters list list of counters to collect (see notes below) plugins list plugins and their parameters to run on the collected data export_options list parameters to pass to exporters (see notes below) counters \u00b6 This section defines the list of counters that will be collected. These counters can be labels, numeric metrics or histograms. The exact property of each counter is fetched from StorageGRID and updated periodically. The display name of a counter can be changed with => (e.g., policy.quotaObjectBytes => logical_quota ). Counters that are stored as labels will only be exported if they are included in the export_options section. export_options \u00b6 Parameters in this section tell the exporters how to handle the collected data. The set of parameters varies by exporter. For Prometheus and InfluxDB exporters, the following parameters can be defined: instances_keys (list): display names of labels to export with each data-point instance_labels (list): display names of labels to export as a separate _label metric include_all_labels (bool): export all labels with each data-point (overrides previous two parameters)","title":"StorageGRID"},{"location":"configure-storagegrid/#storagegrid-collector","text":"The StorageGRID collector uses REST calls to collect data from StorageGRID systems.","title":"StorageGRID Collector"},{"location":"configure-storagegrid/#target-system","text":"All StorageGRID versions are supported, however the default configuration files may not completely match with older systems.","title":"Target System"},{"location":"configure-storagegrid/#requirements","text":"No SDK or other requirements. It is recommended to create a read-only user for Harvest on the StorageGRID system (see prepare monitored clusters for details)","title":"Requirements"},{"location":"configure-storagegrid/#metrics","text":"The collector collects a dynamic set of metrics via StorageGRID's REST API. StorageGRID returns JSON documents and Harvest allows you to define templates to extract values from the JSON document via a dot notation path. You can view StorageGRID's full set of REST APIs by visiting https://$STORAGE_GRID_HOSTNAME/grid/apidocs.html As an example, the /grid/accounts-cache endpoint, lists the tenant accounts in the cache and includes additional information, such as objectCount and dataBytes. Below is an example response from this endpoint: { \"data\" : [ { \"id\" : \"95245224059574669217\" , \"name\" : \"foople\" , \"policy\" : { \"quotaObjectBytes\" : 50000000000 }, \"objectCount\" : 6 , \"dataBytes\" : 10473454261 } ] } The StorageGRID collector will take this document, extract the data section and convert the metrics above into: name , policy.quotaObjectBytes , objectCount , and dataBytes . Metric names will be taken, as is, unless you specify a short display name. See counters for more details.","title":"Metrics"},{"location":"configure-storagegrid/#parameters","text":"The parameters of the collector are distributed across three files: Harvest configuration file (default: harvest.yml ) StorageGRID configuration file (default: conf/storagegrid/default.yaml ) Each object has its own configuration file (located in conf/storagegrid/$version/ ) Except for addr and datacenter , all other parameters of the StorageGRID collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level ones. This allows you to configure each object individually, or use the same parameters for all objects. The full set of parameters are described below .","title":"Parameters"},{"location":"configure-storagegrid/#harvest-configuration-file","text":"Parameters in the poller section should define the following required parameters. parameter type description default Poller name (header) string, required Poller name, user-defined value addr string, required address (IP or FQDN) of the ONTAP system datacenter string, required Datacenter name, user-defined value username , password string, required StorageGRID username and password with at least Tenant accounts permissions collectors list, required Name of collector to run for this poller, use StorageGrid for this collector","title":"Harvest configuration file"},{"location":"configure-storagegrid/#storagegrid-configuration-file","text":"This configuration file contains a list of objects that should be collected and the filenames of their templates ( explained in the next section). Additionally, this file contains the parameters that are applied as defaults to all objects. As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well. parameter type description default client_timeout duration (Go-syntax) how long to wait for server responses 30s schedule list, required how frequently to retrieve metrics from StorageGRID - data duration (Go-syntax) how frequently this collector/object should retrieve metrics from StorageGRID 5 minutes The template should define objects in the objects section. Example: objects : Tenant : tenant.yaml For each object, we define the filename of the object configuration file. The object configuration files are located in subdirectories matching the StorageGRID version that was used to create these files. It is possible to have multiple version-subdirectories for multiple StorageGRID versions. At runtime, the collector will select the object configuration file that closest matches the version of the target StorageGRID system.","title":"StorageGRID configuration file"},{"location":"configure-storagegrid/#object-configuration-file","text":"The Object configuration file (\"subtemplate\") should contain the following parameters: parameter type description default name string, required display name of the collector that will collect this object query string, required REST endpoint used to issue a REST request object string, required short name of the object api string StorageGRID REST endpoint version to use, overrides default management API version 3 counters list list of counters to collect (see notes below) plugins list plugins and their parameters to run on the collected data export_options list parameters to pass to exporters (see notes below)","title":"Object configuration file"},{"location":"configure-storagegrid/#counters","text":"This section defines the list of counters that will be collected. These counters can be labels, numeric metrics or histograms. The exact property of each counter is fetched from StorageGRID and updated periodically. The display name of a counter can be changed with => (e.g., policy.quotaObjectBytes => logical_quota ). Counters that are stored as labels will only be exported if they are included in the export_options section.","title":"counters"},{"location":"configure-storagegrid/#export_options","text":"Parameters in this section tell the exporters how to handle the collected data. The set of parameters varies by exporter. For Prometheus and InfluxDB exporters, the following parameters can be defined: instances_keys (list): display names of labels to export with each data-point instance_labels (list): display names of labels to export as a separate _label metric include_all_labels (bool): export all labels with each data-point (overrides previous two parameters)","title":"export_options"},{"location":"configure-templates/","text":"Creating/editing templates \u00b6 This document covers how to use Collector and Object templates to extend Harvest. How to add a new object template How to extend an existing object template There are a couple of ways to learn about ZAPIs and their attributes: ONTAP's documentation Using Harvest's zapi tool to explore available APIs and metrics on your cluster. Examples: $ harvest zapi --poller <poller> show apis # will print list of apis that are available # usually apis with the \"get-iter\" suffix can provide useful metrics $ harvest zapi --poller <poller> show attrs --api volume-get-iter # will print the attribute tree of the API $ harvest zapi --poller <poller> show data --api volume-get-iter # will print raw data of the API attribute tree (Replace <poller> with the name of a poller that can connect to an ONTAP system.) Collector templates \u00b6 Collector templates define which set of objects Harvest should collect from the system being monitored. In your harvest.yml configuration file, when you say that you want to use a Zapi collector, that collector will read the matching conf/zapi/default.yaml - same with ZapiPerf , it will read the conf/zapiperf/default.yaml file. Belows's a snippet from conf/zapi/default.yaml . Each object is mapped to a corresponding object template file. For example, the Node object searches for the most appropriate version of the node.yaml file in the conf/zapi/cdot/** directory. collector: Zapi objects: Node: node.yaml Aggregate: aggr.yaml Volume: volume.yaml Disk: disk.yaml Each collector will also check if a matching file named, custom.yaml exists, and if it does, it will read that file and merge it with default.yaml . The custom.yaml file should be located beside the matching default.yaml file. ( eg. conf/zapi/custom.yaml is beside conf/zapi/default.yaml ). Let's take a look at some examples. Define a poller that uses the default Zapi collector. Using the default template is the easiest and most used option. Pollers : jamaica : datacenter : munich addr : 10.10.10.10 collectors : - Zapi # will use conf/zapi/default.yaml and optionally merge with conf/zapi/custom.yaml Define a poller that uses the Zapi collector, but with a custom template file: Pollers : jamaica : datacenter : munich addr : 10.10.10.10 collectors : - ZapiPerf : - limited.yaml # will use conf/zapiperf/limited.yaml # more templates can be added, they will be merged Object Templates \u00b6 Object templates (example: conf/zapi/cdot/9.8.0/lun.yaml ) describe what to collect and export. These templates are used by collectors to gather metrics and send them to your time-series db. Object templates are made up of the following parts: the name of the object (or resource) to collect the ZAPI or REST query used to collect the object a list of object counters to collect and how to export them Instead of editing one of the existing templates, it's better to extend one of them. That way, your custom template will not be overwritten when upgrading Harvest. For example, if you want to extend conf/zapi/cdot/9.8.0/aggr.yaml , first create a copy (e.g., conf/zapi/cdot/9.8.0/custom_aggr.yaml ), and then tell Harvest to use your custom template by adding these lines to conf/zapi/custom.yaml : objects : Aggregate : custom_aggr.yaml After restarting your pollers, aggr.yaml and custom_aggr.yaml will be merged. Create a new object template \u00b6 In this example, imagine that Harvest doesn't already collect environment sensor data and you wanted to collect it. Sensor does comes from the environment-sensors-get-iter ZAPI. Here are the steps to add a new object template. Create the file conf/zapi/cdot/9.8.0/sensor.yaml (optionally replace 9.8.0 with the earliest version of ONTAP that supports sensor data. Refer to Harvest Versioned Templates for more information. Add the following content to your new sensor.yaml file. name : Sensor # this name must match the key in your custom.yaml file query : environment-sensors-get-iter object : sensor metric_type : int64 counters : environment-sensors-info : - critical-high-threshold => critical_high - critical-low-threshold => critical_low - ^discrete-sensor-state => discrete_state - ^discrete-sensor-value => discrete_value - ^^node-name => node - ^^sensor-name => sensor - ^sensor-type => type - ^threshold-sensor-state => threshold_state - threshold-sensor-value => threshold_value - ^value-units => unit - ^warning-high-threshold => warning_high - ^warning-low-threshold => warning_low export_options : include_all_labels : true Enable the new object template \u00b6 To enable the new sensor object template, create the conf/zapi/custom.yaml file with the lines shown below. objects : Sensor : sensor.yaml # this key must match the name in your sensor.yaml file The Sensor key used in the custom.yaml must match the name defined in the sensor.yaml file. That mapping is what connects this object with its template. In the future, if you add more object templates, you can add those in your existing custom.yaml file. Test your object template changes \u00b6 Test your new Sensor template with a single poller like this: ./bin/harvest start <poller> --foreground --verbose --collectors Zapi --objects Sensor Replace <poller> with the name of one of your ONTAP pollers. Once you have confirmed that the new template works, restart any already running pollers that you want to use the new template(s). Check the metrics \u00b6 If you are using the Prometheus exporter, you can scrape the poller's HTTP endpoint with curl or a web browser. E.g., my poller exports its data on port 15001. Adjust as needed for your exporter. curl -s 'http://localhost:15001/metrics' | grep ^sensor_ # sensor_ name matches the object: value in your sensor.yaml file. sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_high=\"3664\",node=\"shopfloor-02\",sensor=\"P3.3V STBY\",type=\"voltage\",warning_low=\"3040\",critical_low=\"2960\",threshold_state=\"normal\",unit=\"mV\",warning_high=\"3568\"} 3280 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"P1.2V STBY\",type=\"voltage\",threshold_state=\"normal\",warning_high=\"1299\",warning_low=\"1105\",critical_low=\"1086\",node=\"shopfloor-02\",critical_high=\"1319\",unit=\"mV\"} 1193 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",unit=\"mV\",critical_high=\"15810\",critical_low=\"0\",node=\"shopfloor-02\",sensor=\"P12V STBY\",type=\"voltage\",threshold_state=\"normal\"} 11842 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"P12V STBY Curr\",type=\"current\",threshold_state=\"normal\",unit=\"mA\",critical_high=\"3182\",critical_low=\"0\",node=\"shopfloor-02\"} 748 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_low=\"1470\",node=\"shopfloor-02\",sensor=\"Sysfan2 F2 Speed\",type=\"fan\",threshold_state=\"normal\",unit=\"RPM\",warning_low=\"1560\"} 2820 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"PSU2 Fan1 Speed\",type=\"fan\",threshold_state=\"normal\",unit=\"RPM\",warning_low=\"4600\",critical_low=\"4500\",node=\"shopfloor-01\"} 6900 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"PSU1 InPwr Monitor\",type=\"unknown\",threshold_state=\"normal\",unit=\"mW\",node=\"shopfloor-01\"} 132000 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_high=\"58\",type=\"thermal\",unit=\"C\",warning_high=\"53\",critical_low=\"0\",node=\"shopfloor-01\",sensor=\"Bat Temp\",threshold_state=\"normal\",warning_low=\"5\"} 24 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_high=\"9000\",node=\"shopfloor-01\",sensor=\"Bat Charge Volt\",type=\"voltage\",threshold_state=\"normal\",unit=\"mV\",warning_high=\"8900\"} 8200 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",node=\"shopfloor-02\",sensor=\"PSU1 InPwr Monitor\",type=\"unknown\",threshold_state=\"normal\",unit=\"mW\"} 132000 Extend an existing object template \u00b6 How to extend a Rest/RestPerf/Ems collector's existing object template \u00b6 Instead of editing one of the existing templates, it's better to copy one and edit the copy. That way, your custom template will not be overwritten when upgrading Harvest. For example, if you want to change conf/rest/cdot/9.12.0/aggr.yaml , first create a copy (e.g., conf/rest/cdot/9.12.0/custom_aggr.yaml ), then add these lines to conf/rest/custom.yaml : objects : Aggregate : custom_aggr.yaml After restarting pollers, aggr.yaml will be ignored and the new, custom_aggr.yaml subtemplate will be used instead. How to extend a Zapi/ZapiPerf collector's existing object template \u00b6 In this example, we want to extend one of the existing object templates that Harvest ships with, e.g. conf/zapi/cdot/9.8.0/lun.yaml and collect additional information as outlined below. Let's say you want to extend lun.yaml to: Increase client_timeout (You want to increase the default timeout of the lun ZAPI because it keeps timing out ) Add additional counters, e.g. multiprotocol-type , application Add a new counter to the already collected lun metrics using the value_to_num plugin Add a new application instance_keys and labels to the collected metrics Let's assume the existing template is located at conf/zapi/cdot/9.8.0/lun.yaml and contains the following. name : Lun query : lun-get-iter object : lun counters : lun-info : - ^node - ^path - ^qtree - size - size-used - ^state - ^^uuid - ^volume - ^vserver => svm plugins : - LabelAgent : # metric label zapi_value rest_value `default_value` value_to_num : - new_status state online online `0` split : - path `/` ,,,lun export_options : instance_keys : - node - qtree - lun - volume - svm instance_labels : - state ``` To extend the out-of-the-box `lun.yaml` template, create a `conf/zapi/custom.yaml` file if it doesn't already exist and add the lines shown below : ``` yaml objects : Lun : custom_lun.yaml Create a new object template conf/zapi/cdot/9.8.0/custom_lun.yaml with the lines shown below. client_timeout : 5m counters : lun-info : - ^multiprotocol-type - ^application plugins : - LabelAgent : value_to_num : - custom_status state online online `0` export_options : instance_keys : - application ``` When you restart your pollers, Harvest will take the out-of-the-box template (`lun.yaml`) and your new one (`custom_lun.yaml`) and merge them into the following : ``` yaml name : Lun query : lun-get-iter object : lun counters : lun-info : - ^node - ^path - ^qtree - size - size-used - ^state - ^^uuid - ^volume - ^vserver => svm - ^multiprotocol-type - ^application plugins : LabelAgent : value_to_num : - new_status state online online `0` - custom_status state online online `0` split : - path `/` ,,,lun export_options : instance_keys : - node - qtree - lun - volume - svm - application client_timeout : 5m To help understand the merging process and the resulting combined template, you can view the result with: bin/harvest doctor merge --template conf/zapi/cdot/9.8.0/lun.yaml --with conf/zapi/cdot/9.8.0/custom_lun.yaml Replace an existing object template for Zapi/ZapiPerf Collector \u00b6 You can only extend existing templates for Zapi/ZapiPerf Collector as explained above . If you need to replace one of the existing object templates, let us know on Discord or GitHub. Harvest Versioned Templates \u00b6 Harvest ships with a set of versioned templates tailored for specific versions of ONTAP. At runtime, Harvest uses a BestFit heuristic to pick the most appropriate template. The BestFit heuristic compares the list of Harvest templates with the ONTAP version and selects the best match. There are versioned templates for both the ZAPI and REST collectors. Below is an example of how the BestFit algorithm works - assume Harvest has these templated versions: 9.6.0 9.6.1 9.8.0 9.9.0 9.10.1 if you are monitoring a cluster at these versions, Harvest will select the indicated template: ONTAP version 9.4.1, Harvest will select the templates for 9.6.0 ONTAP version 9.6.0, Harvest will select the templates for 9.6.0 ONTAP version 9.7.X, Harvest will select the templates for 9.6.1 ONTAP version 9.12, Harvest will select the templates for 9.10.1 counters \u00b6 This section contains the complete or partial attribute tree of the queried API. Since the collector does not get counter metadata from the ONTAP system, two additional symbols are used for non-numeric attributes: ^ used as a prefix indicates that the attribute should be stored as a label ^^ indicates that the attribute is a label and an instance key (i.e., a label that uniquely identifies an instance, such as name , uuid ). If a single label does not uniquely identify an instance, then multiple instance keys should be indicated. Additionally, the symbol => can be used to set a custom display name for both instance labels and numeric counters. Example: name : Spare query : aggr-spare-get-iter object : spare collect_only_labels : true counters : aggr-spare-disk-info : - ^^disk # creates label aggr-disk - ^disk-type # creates label aggr-disk-type - ^is-disk-zeroed => is_disk_zeroed # creates label is_disk_zeroed - ^^original-owner => original_owner # creates label original_owner export_options : instance_keys : - disk - original_owner instance_labels : - disk_type - is_disk_zeroed Harvest does its best to determine a unique display name for each template's label and metric. Instead of relying on this heuristic, it is better to be explicit in your templates and define a display name using the caret ( ^ ) mapping. For example, instead of this: aggr-spare-disk-info: - ^^disk - ^disk-type do this: aggr-spare-disk-info: - ^^disk => disk - ^disk-type => disk_type See also #585","title":"Templates"},{"location":"configure-templates/#creatingediting-templates","text":"This document covers how to use Collector and Object templates to extend Harvest. How to add a new object template How to extend an existing object template There are a couple of ways to learn about ZAPIs and their attributes: ONTAP's documentation Using Harvest's zapi tool to explore available APIs and metrics on your cluster. Examples: $ harvest zapi --poller <poller> show apis # will print list of apis that are available # usually apis with the \"get-iter\" suffix can provide useful metrics $ harvest zapi --poller <poller> show attrs --api volume-get-iter # will print the attribute tree of the API $ harvest zapi --poller <poller> show data --api volume-get-iter # will print raw data of the API attribute tree (Replace <poller> with the name of a poller that can connect to an ONTAP system.)","title":"Creating/editing templates"},{"location":"configure-templates/#collector-templates","text":"Collector templates define which set of objects Harvest should collect from the system being monitored. In your harvest.yml configuration file, when you say that you want to use a Zapi collector, that collector will read the matching conf/zapi/default.yaml - same with ZapiPerf , it will read the conf/zapiperf/default.yaml file. Belows's a snippet from conf/zapi/default.yaml . Each object is mapped to a corresponding object template file. For example, the Node object searches for the most appropriate version of the node.yaml file in the conf/zapi/cdot/** directory. collector: Zapi objects: Node: node.yaml Aggregate: aggr.yaml Volume: volume.yaml Disk: disk.yaml Each collector will also check if a matching file named, custom.yaml exists, and if it does, it will read that file and merge it with default.yaml . The custom.yaml file should be located beside the matching default.yaml file. ( eg. conf/zapi/custom.yaml is beside conf/zapi/default.yaml ). Let's take a look at some examples. Define a poller that uses the default Zapi collector. Using the default template is the easiest and most used option. Pollers : jamaica : datacenter : munich addr : 10.10.10.10 collectors : - Zapi # will use conf/zapi/default.yaml and optionally merge with conf/zapi/custom.yaml Define a poller that uses the Zapi collector, but with a custom template file: Pollers : jamaica : datacenter : munich addr : 10.10.10.10 collectors : - ZapiPerf : - limited.yaml # will use conf/zapiperf/limited.yaml # more templates can be added, they will be merged","title":"Collector templates"},{"location":"configure-templates/#object-templates","text":"Object templates (example: conf/zapi/cdot/9.8.0/lun.yaml ) describe what to collect and export. These templates are used by collectors to gather metrics and send them to your time-series db. Object templates are made up of the following parts: the name of the object (or resource) to collect the ZAPI or REST query used to collect the object a list of object counters to collect and how to export them Instead of editing one of the existing templates, it's better to extend one of them. That way, your custom template will not be overwritten when upgrading Harvest. For example, if you want to extend conf/zapi/cdot/9.8.0/aggr.yaml , first create a copy (e.g., conf/zapi/cdot/9.8.0/custom_aggr.yaml ), and then tell Harvest to use your custom template by adding these lines to conf/zapi/custom.yaml : objects : Aggregate : custom_aggr.yaml After restarting your pollers, aggr.yaml and custom_aggr.yaml will be merged.","title":"Object Templates"},{"location":"configure-templates/#create-a-new-object-template","text":"In this example, imagine that Harvest doesn't already collect environment sensor data and you wanted to collect it. Sensor does comes from the environment-sensors-get-iter ZAPI. Here are the steps to add a new object template. Create the file conf/zapi/cdot/9.8.0/sensor.yaml (optionally replace 9.8.0 with the earliest version of ONTAP that supports sensor data. Refer to Harvest Versioned Templates for more information. Add the following content to your new sensor.yaml file. name : Sensor # this name must match the key in your custom.yaml file query : environment-sensors-get-iter object : sensor metric_type : int64 counters : environment-sensors-info : - critical-high-threshold => critical_high - critical-low-threshold => critical_low - ^discrete-sensor-state => discrete_state - ^discrete-sensor-value => discrete_value - ^^node-name => node - ^^sensor-name => sensor - ^sensor-type => type - ^threshold-sensor-state => threshold_state - threshold-sensor-value => threshold_value - ^value-units => unit - ^warning-high-threshold => warning_high - ^warning-low-threshold => warning_low export_options : include_all_labels : true","title":"Create a new object template"},{"location":"configure-templates/#enable-the-new-object-template","text":"To enable the new sensor object template, create the conf/zapi/custom.yaml file with the lines shown below. objects : Sensor : sensor.yaml # this key must match the name in your sensor.yaml file The Sensor key used in the custom.yaml must match the name defined in the sensor.yaml file. That mapping is what connects this object with its template. In the future, if you add more object templates, you can add those in your existing custom.yaml file.","title":"Enable the new object template"},{"location":"configure-templates/#test-your-object-template-changes","text":"Test your new Sensor template with a single poller like this: ./bin/harvest start <poller> --foreground --verbose --collectors Zapi --objects Sensor Replace <poller> with the name of one of your ONTAP pollers. Once you have confirmed that the new template works, restart any already running pollers that you want to use the new template(s).","title":"Test your object template changes"},{"location":"configure-templates/#check-the-metrics","text":"If you are using the Prometheus exporter, you can scrape the poller's HTTP endpoint with curl or a web browser. E.g., my poller exports its data on port 15001. Adjust as needed for your exporter. curl -s 'http://localhost:15001/metrics' | grep ^sensor_ # sensor_ name matches the object: value in your sensor.yaml file. sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_high=\"3664\",node=\"shopfloor-02\",sensor=\"P3.3V STBY\",type=\"voltage\",warning_low=\"3040\",critical_low=\"2960\",threshold_state=\"normal\",unit=\"mV\",warning_high=\"3568\"} 3280 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"P1.2V STBY\",type=\"voltage\",threshold_state=\"normal\",warning_high=\"1299\",warning_low=\"1105\",critical_low=\"1086\",node=\"shopfloor-02\",critical_high=\"1319\",unit=\"mV\"} 1193 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",unit=\"mV\",critical_high=\"15810\",critical_low=\"0\",node=\"shopfloor-02\",sensor=\"P12V STBY\",type=\"voltage\",threshold_state=\"normal\"} 11842 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"P12V STBY Curr\",type=\"current\",threshold_state=\"normal\",unit=\"mA\",critical_high=\"3182\",critical_low=\"0\",node=\"shopfloor-02\"} 748 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_low=\"1470\",node=\"shopfloor-02\",sensor=\"Sysfan2 F2 Speed\",type=\"fan\",threshold_state=\"normal\",unit=\"RPM\",warning_low=\"1560\"} 2820 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"PSU2 Fan1 Speed\",type=\"fan\",threshold_state=\"normal\",unit=\"RPM\",warning_low=\"4600\",critical_low=\"4500\",node=\"shopfloor-01\"} 6900 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",sensor=\"PSU1 InPwr Monitor\",type=\"unknown\",threshold_state=\"normal\",unit=\"mW\",node=\"shopfloor-01\"} 132000 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_high=\"58\",type=\"thermal\",unit=\"C\",warning_high=\"53\",critical_low=\"0\",node=\"shopfloor-01\",sensor=\"Bat Temp\",threshold_state=\"normal\",warning_low=\"5\"} 24 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",critical_high=\"9000\",node=\"shopfloor-01\",sensor=\"Bat Charge Volt\",type=\"voltage\",threshold_state=\"normal\",unit=\"mV\",warning_high=\"8900\"} 8200 sensor_value{datacenter=\"WDRF\",cluster=\"shopfloor\",node=\"shopfloor-02\",sensor=\"PSU1 InPwr Monitor\",type=\"unknown\",threshold_state=\"normal\",unit=\"mW\"} 132000","title":"Check the metrics"},{"location":"configure-templates/#extend-an-existing-object-template","text":"","title":"Extend an existing object template"},{"location":"configure-templates/#how-to-extend-a-restrestperfems-collectors-existing-object-template","text":"Instead of editing one of the existing templates, it's better to copy one and edit the copy. That way, your custom template will not be overwritten when upgrading Harvest. For example, if you want to change conf/rest/cdot/9.12.0/aggr.yaml , first create a copy (e.g., conf/rest/cdot/9.12.0/custom_aggr.yaml ), then add these lines to conf/rest/custom.yaml : objects : Aggregate : custom_aggr.yaml After restarting pollers, aggr.yaml will be ignored and the new, custom_aggr.yaml subtemplate will be used instead.","title":"How to extend a Rest/RestPerf/Ems collector's existing object template"},{"location":"configure-templates/#how-to-extend-a-zapizapiperf-collectors-existing-object-template","text":"In this example, we want to extend one of the existing object templates that Harvest ships with, e.g. conf/zapi/cdot/9.8.0/lun.yaml and collect additional information as outlined below. Let's say you want to extend lun.yaml to: Increase client_timeout (You want to increase the default timeout of the lun ZAPI because it keeps timing out ) Add additional counters, e.g. multiprotocol-type , application Add a new counter to the already collected lun metrics using the value_to_num plugin Add a new application instance_keys and labels to the collected metrics Let's assume the existing template is located at conf/zapi/cdot/9.8.0/lun.yaml and contains the following. name : Lun query : lun-get-iter object : lun counters : lun-info : - ^node - ^path - ^qtree - size - size-used - ^state - ^^uuid - ^volume - ^vserver => svm plugins : - LabelAgent : # metric label zapi_value rest_value `default_value` value_to_num : - new_status state online online `0` split : - path `/` ,,,lun export_options : instance_keys : - node - qtree - lun - volume - svm instance_labels : - state ``` To extend the out-of-the-box `lun.yaml` template, create a `conf/zapi/custom.yaml` file if it doesn't already exist and add the lines shown below : ``` yaml objects : Lun : custom_lun.yaml Create a new object template conf/zapi/cdot/9.8.0/custom_lun.yaml with the lines shown below. client_timeout : 5m counters : lun-info : - ^multiprotocol-type - ^application plugins : - LabelAgent : value_to_num : - custom_status state online online `0` export_options : instance_keys : - application ``` When you restart your pollers, Harvest will take the out-of-the-box template (`lun.yaml`) and your new one (`custom_lun.yaml`) and merge them into the following : ``` yaml name : Lun query : lun-get-iter object : lun counters : lun-info : - ^node - ^path - ^qtree - size - size-used - ^state - ^^uuid - ^volume - ^vserver => svm - ^multiprotocol-type - ^application plugins : LabelAgent : value_to_num : - new_status state online online `0` - custom_status state online online `0` split : - path `/` ,,,lun export_options : instance_keys : - node - qtree - lun - volume - svm - application client_timeout : 5m To help understand the merging process and the resulting combined template, you can view the result with: bin/harvest doctor merge --template conf/zapi/cdot/9.8.0/lun.yaml --with conf/zapi/cdot/9.8.0/custom_lun.yaml","title":"How to extend a Zapi/ZapiPerf collector's existing object template"},{"location":"configure-templates/#replace-an-existing-object-template-for-zapizapiperf-collector","text":"You can only extend existing templates for Zapi/ZapiPerf Collector as explained above . If you need to replace one of the existing object templates, let us know on Discord or GitHub.","title":"Replace an existing object template for Zapi/ZapiPerf Collector"},{"location":"configure-templates/#harvest-versioned-templates","text":"Harvest ships with a set of versioned templates tailored for specific versions of ONTAP. At runtime, Harvest uses a BestFit heuristic to pick the most appropriate template. The BestFit heuristic compares the list of Harvest templates with the ONTAP version and selects the best match. There are versioned templates for both the ZAPI and REST collectors. Below is an example of how the BestFit algorithm works - assume Harvest has these templated versions: 9.6.0 9.6.1 9.8.0 9.9.0 9.10.1 if you are monitoring a cluster at these versions, Harvest will select the indicated template: ONTAP version 9.4.1, Harvest will select the templates for 9.6.0 ONTAP version 9.6.0, Harvest will select the templates for 9.6.0 ONTAP version 9.7.X, Harvest will select the templates for 9.6.1 ONTAP version 9.12, Harvest will select the templates for 9.10.1","title":"Harvest Versioned Templates"},{"location":"configure-templates/#counters","text":"This section contains the complete or partial attribute tree of the queried API. Since the collector does not get counter metadata from the ONTAP system, two additional symbols are used for non-numeric attributes: ^ used as a prefix indicates that the attribute should be stored as a label ^^ indicates that the attribute is a label and an instance key (i.e., a label that uniquely identifies an instance, such as name , uuid ). If a single label does not uniquely identify an instance, then multiple instance keys should be indicated. Additionally, the symbol => can be used to set a custom display name for both instance labels and numeric counters. Example: name : Spare query : aggr-spare-get-iter object : spare collect_only_labels : true counters : aggr-spare-disk-info : - ^^disk # creates label aggr-disk - ^disk-type # creates label aggr-disk-type - ^is-disk-zeroed => is_disk_zeroed # creates label is_disk_zeroed - ^^original-owner => original_owner # creates label original_owner export_options : instance_keys : - disk - original_owner instance_labels : - disk_type - is_disk_zeroed Harvest does its best to determine a unique display name for each template's label and metric. Instead of relying on this heuristic, it is better to be explicit in your templates and define a display name using the caret ( ^ ) mapping. For example, instead of this: aggr-spare-disk-info: - ^^disk - ^disk-type do this: aggr-spare-disk-info: - ^^disk => disk - ^disk-type => disk_type See also #585","title":"counters"},{"location":"configure-unix/","text":"Unix \u00b6 This collector polls resource usage by Harvest pollers on the local system. Collector might be extended in the future to monitor any local or remote process. Target System \u00b6 The machine where Harvest is running (\"localhost\"). Requirements \u00b6 Collector requires any OS where the proc-filesystem is available. If you are a developer, you are welcome to add support for other platforms. Currently, supported platforms includes most Unix/Unix-like systems: Android / Termux DragonFly BSD FreeBSD IBM AIX Linux NetBSD Plan9 Solaris (On FreeBSD and NetBSD the proc-filesystem needs to be manually mounted). Parameters \u00b6 parameter type description default mount_point string, optional path to the proc filesystem `/proc Metrics \u00b6 The Collector follows the Linux proc(5) manual to parse a static set of metrics. Unless otherwise stated, the metric has a scalar value: metric type unit description start_time counter, float64 seconds process uptime cpu_percent gauge, float64 percent CPU used since last poll memory_percent gauge, float64 percent Memory used (RSS) since last poll cpu histogram, float64 seconds CPU used since last poll ( system , user , iowait ) memory histogram, uint64 kB Memory used since last poll ( rss , vms , swap , etc) io histogram, uint64 byte count IOs performed by process: rchar , wchar , read_bytes , write_bytes - read/write IOs syscr , syscw - syscalls for IO operations net histogram, uint64 count/byte Different IO operations over network devices ctx histogram, uint64 count Number of context switched ( voluntary , involuntary ) threads counter, uint64 count Number of threads fds counter, uint64 count Number of file descriptors Additionally, the collector provides the following instance labels: label description poller name of the poller pid PID of the poller Issues \u00b6 Collector will fail on WSL because some non-critical files, in the proc-filesystem, are not present.","title":"Unix"},{"location":"configure-unix/#unix","text":"This collector polls resource usage by Harvest pollers on the local system. Collector might be extended in the future to monitor any local or remote process.","title":"Unix"},{"location":"configure-unix/#target-system","text":"The machine where Harvest is running (\"localhost\").","title":"Target System"},{"location":"configure-unix/#requirements","text":"Collector requires any OS where the proc-filesystem is available. If you are a developer, you are welcome to add support for other platforms. Currently, supported platforms includes most Unix/Unix-like systems: Android / Termux DragonFly BSD FreeBSD IBM AIX Linux NetBSD Plan9 Solaris (On FreeBSD and NetBSD the proc-filesystem needs to be manually mounted).","title":"Requirements"},{"location":"configure-unix/#parameters","text":"parameter type description default mount_point string, optional path to the proc filesystem `/proc","title":"Parameters"},{"location":"configure-unix/#metrics","text":"The Collector follows the Linux proc(5) manual to parse a static set of metrics. Unless otherwise stated, the metric has a scalar value: metric type unit description start_time counter, float64 seconds process uptime cpu_percent gauge, float64 percent CPU used since last poll memory_percent gauge, float64 percent Memory used (RSS) since last poll cpu histogram, float64 seconds CPU used since last poll ( system , user , iowait ) memory histogram, uint64 kB Memory used since last poll ( rss , vms , swap , etc) io histogram, uint64 byte count IOs performed by process: rchar , wchar , read_bytes , write_bytes - read/write IOs syscr , syscw - syscalls for IO operations net histogram, uint64 count/byte Different IO operations over network devices ctx histogram, uint64 count Number of context switched ( voluntary , involuntary ) threads counter, uint64 count Number of threads fds counter, uint64 count Number of file descriptors Additionally, the collector provides the following instance labels: label description poller name of the poller pid PID of the poller","title":"Metrics"},{"location":"configure-unix/#issues","text":"Collector will fail on WSL because some non-critical files, in the proc-filesystem, are not present.","title":"Issues"},{"location":"configure-zapi/","text":"What about REST? ZAPI will reach end of availablity in ONTAP 9.13.1 released Q2 2023. Don't worry, Harvest has you covered. Switch to Harvest's REST collectors and collect idential metrics. See REST Strategy for more details. Zapi Collector \u00b6 The Zapi collectors uses the ZAPI protocol to collect data from ONTAP systems. The collector submits data as received from the target system, and does not perform any calculations or post-processing. Since the attributes of most APIs have an irregular tree structure, sometimes a plugin will be required to collect all metrics from an API. The ZapiPerf collector is an extension of this collector, therefore they share many parameters and configuration settings. Target System \u00b6 Target system can be any cDot or 7Mode ONTAP system. Any version is supported, however the default configuration files may not completely match with older systems. Requirements \u00b6 No SDK or other requirements. It is recommended to create a read-only user for Harvest on the ONTAP system (see prepare monitored clusters for details) Metrics \u00b6 The collector collects a dynamic set of metrics. Since most ZAPIs have a tree structure, the collector converts that structure into a flat metric representation. No post-processing or calculation is performed on the collected data itself. As an example, the aggr-get-iter ZAPI provides the following partial attribute tree: aggr-attributes : - aggr-raid-attributes : - disk-count - aggr-snapshot-attributes : - files-total The Zapi collector will convert this tree into two \"flat\" metrics: aggr_raid_disk_count and aggr_snapshot_files_total . (The algorithm to generate a name for the metrics will attempt to keep it as simple as possible, but sometimes it's useful to manually set a short display name. See counters for more details. Parameters \u00b6 The parameters and configuration are similar to those of the ZapiPerf collector . Only the differences will be discussed below. Collector configuration file \u00b6 Parameters different from ZapiPerf: parameter type description default schedule required same as for ZapiPerf, but only two elements: instance and data (collector does not run a counter poll) no_max_records bool, optional don't add max-records to the ZAPI request collect_only_labels bool, optional don't look for numeric metrics, only submit labels (suppresses the ErrNoMetrics error) only_cluster_instance bool, optional don't look for instance keys and assume only instance is the cluster itself Object configuration file \u00b6 The Zapi collector does not have the parameters instance_key and override parameters. The optional parameter metric_type allows you to override the default metric type ( uint64 ). The value of this parameter should be one of the metric types supported by the matrix data-structure . ZapiPerf Collector \u00b6 ZapiPerf \u00b6 ZapiPerf collects performance metrics from ONTAP systems using the ZAPI protocol. The collector is designed to be easily extendable to collect new objects or to collect additional counters from already configured objects. This collector is an extension of the Zapi collector . The major difference between them is that ZapiPerf collects only the performance ( perf ) APIs. Additionally, ZapiPerf always calculates final values from the deltas of two subsequent polls. Metrics \u00b6 The collector collects a dynamic set of metrics. The metric values are calculated from two consecutive polls (therefore, no metrics are emitted after the first poll). The calculation algorithm depends on the property and base-counter attributes of each metric, the following properties are supported: property formula description raw x = x i no post-processing, value x is submitted as it is delta x = x i - x i-1 delta of two poll values, x i and x i-1 rate x = (x i - x i-1 ) / (t i - t i-1 ) delta divided by the interval of the two polls in seconds average x = (x i - x i-1 ) / (y i - y i-1 ) delta divided by the delta of the base counter y percent x = 100 * (x i - x i-1 ) / (y i - y i-1 ) average multiplied by 100 Parameters \u00b6 The parameters of the collector are distributed across three files: Harvest configuration file (default: harvest.yml ) ZapiPerf configuration file (default: conf/zapiperf/default.yaml ) Each object has its own configuration file (located in conf/zapiperf/cdot/ and conf/zapiperf/7mode/ for cDot and 7Mode systems respectively) Except for addr , datacenter and auth_style , all other parameters of the ZapiPerf collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level file. This allows the user to configure each objects individually, or use the same parameters for all objects. The full set of parameters are described below . Harvest configuration file \u00b6 Parameters in poller section should define (at least) the address and authentication method of the target system: parameter type description default addr string, required address (IP or FQDN) of the ONTAP system datacenter string, required name of the datacenter where the target system is located auth_style string, optional authentication method: either basic_auth or certificate_auth basic_auth ssl_cert , ssl_key string, optional full path of the SSL certificate and key pairs (when using certificate_auth ) username , password string, optional full path of the SSL certificate and key pairs (when using basic_auth ) ZapiPerf configuration file \u00b6 This configuration file (the \"template\") contains a list of objects that should be collected and the filenames of their configuration (explained in the next section). Additionally, this file contains the parameters that are applied as defaults to all objects. (As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well). parameter type description default use_insecure_tls bool, optional skip verifying TLS certificate of the target system false client_timeout duration (Go-syntax) how long to wait for server responses 30s batch_size int, optional max instances per API request 500 latency_io_reqd int, optional threshold of IOPs for calculating latency metrics (latencies based on very few IOPs are unreliable) 100 schedule list, required the poll frequencies of the collector/object, should include exactly these three elements in the exact same other: - counter duration (Go-syntax) poll frequency of updating the counter metadata cache (example value: 1200s = 20m ) - instance duration (Go-syntax) poll frequency of updating the instance cache (example value: 600s = 10m ) - data duration (Go-syntax) poll frequency of updating the data cache (example value: 60s = 1m ) Note Harvest allows defining poll intervals on sub-second level (e.g. 1ms ), however keep in mind the following: API response of an ONTAP system can take several seconds, so the collector is likely to enter failed state if the poll interval is less than client_timeout . Small poll intervals will create significant workload on the ONTAP system, as many counters are aggregated on-demand. Some metric values become less significant if they are calculated for very short intervals (e.g. latencies) The template should define objects in the objects section. Example: objects : SystemNode : system_node.yaml HostAdapter : hostadapter.yaml Note that for each object we only define the filename of the object configuration file. The object configuration files are located in subdirectories matching to the ONTAP version that was used to create these files. It is possible to have multiple version-subdirectories for multiple ONTAP versions. At runtime, the collector will select the object configuration file that closest matches to the version of the target ONTAP system. (A mismatch is tolerated since ZapiPerf will fetch and validate counter metadata from the system.) Object configuration file \u00b6 The Object configuration file (\"subtemplate\") should contain the following parameters: parameter type description default name string display name of the collector that will collect this object object string short name of the object query string raw object name used to issue a ZAPI request counters list list of counters to collect (see notes below) instance_key string label to use as instance key (either name or uuid ) override list of key-value pairs override counter properties that we get from ONTAP (allows circumventing ZAPI bugs) plugins list plugins and their parameters to run on the collected data export_options list parameters to pass to exporters (see notes below) counters \u00b6 This section defines the list of counters that will be collected. These counters can be labels, numeric metrics or histograms. The exact property of each counter is fetched from ONTAP and updated periodically. Some counters require a \"base-counter\" for post-processing. If the base-counter is missing, ZapiPerf will still run, but the missing data won't be exported. The display name of a counter can be changed with => (e.g., nfsv3_ops => ops ). There's one conversion Harvest does for you by default, the instance_name counter will be renamed to the value of object . Counters that are stored as labels will only be exported if they are included in the export_options section. export_options \u00b6 Parameters in this section tell the exporters how to handle the collected data. The set of parameters varies by exporter. For Prometheus and InfluxDB exporters, the following parameters can be defined: instances_keys (list): display names of labels to export with each data-point instance_labels (list): display names of labels to export as a separate data-point include_all_labels (bool): export all labels with each data-point (overrides previous two parameters)","title":"ZAPI"},{"location":"configure-zapi/#zapi-collector","text":"The Zapi collectors uses the ZAPI protocol to collect data from ONTAP systems. The collector submits data as received from the target system, and does not perform any calculations or post-processing. Since the attributes of most APIs have an irregular tree structure, sometimes a plugin will be required to collect all metrics from an API. The ZapiPerf collector is an extension of this collector, therefore they share many parameters and configuration settings.","title":"Zapi Collector"},{"location":"configure-zapi/#target-system","text":"Target system can be any cDot or 7Mode ONTAP system. Any version is supported, however the default configuration files may not completely match with older systems.","title":"Target System"},{"location":"configure-zapi/#requirements","text":"No SDK or other requirements. It is recommended to create a read-only user for Harvest on the ONTAP system (see prepare monitored clusters for details)","title":"Requirements"},{"location":"configure-zapi/#metrics","text":"The collector collects a dynamic set of metrics. Since most ZAPIs have a tree structure, the collector converts that structure into a flat metric representation. No post-processing or calculation is performed on the collected data itself. As an example, the aggr-get-iter ZAPI provides the following partial attribute tree: aggr-attributes : - aggr-raid-attributes : - disk-count - aggr-snapshot-attributes : - files-total The Zapi collector will convert this tree into two \"flat\" metrics: aggr_raid_disk_count and aggr_snapshot_files_total . (The algorithm to generate a name for the metrics will attempt to keep it as simple as possible, but sometimes it's useful to manually set a short display name. See counters for more details.","title":"Metrics"},{"location":"configure-zapi/#parameters","text":"The parameters and configuration are similar to those of the ZapiPerf collector . Only the differences will be discussed below.","title":"Parameters"},{"location":"configure-zapi/#collector-configuration-file","text":"Parameters different from ZapiPerf: parameter type description default schedule required same as for ZapiPerf, but only two elements: instance and data (collector does not run a counter poll) no_max_records bool, optional don't add max-records to the ZAPI request collect_only_labels bool, optional don't look for numeric metrics, only submit labels (suppresses the ErrNoMetrics error) only_cluster_instance bool, optional don't look for instance keys and assume only instance is the cluster itself","title":"Collector configuration file"},{"location":"configure-zapi/#object-configuration-file","text":"The Zapi collector does not have the parameters instance_key and override parameters. The optional parameter metric_type allows you to override the default metric type ( uint64 ). The value of this parameter should be one of the metric types supported by the matrix data-structure .","title":"Object configuration file"},{"location":"configure-zapi/#zapiperf-collector","text":"","title":"ZapiPerf Collector"},{"location":"configure-zapi/#zapiperf","text":"ZapiPerf collects performance metrics from ONTAP systems using the ZAPI protocol. The collector is designed to be easily extendable to collect new objects or to collect additional counters from already configured objects. This collector is an extension of the Zapi collector . The major difference between them is that ZapiPerf collects only the performance ( perf ) APIs. Additionally, ZapiPerf always calculates final values from the deltas of two subsequent polls.","title":"ZapiPerf"},{"location":"configure-zapi/#metrics_1","text":"The collector collects a dynamic set of metrics. The metric values are calculated from two consecutive polls (therefore, no metrics are emitted after the first poll). The calculation algorithm depends on the property and base-counter attributes of each metric, the following properties are supported: property formula description raw x = x i no post-processing, value x is submitted as it is delta x = x i - x i-1 delta of two poll values, x i and x i-1 rate x = (x i - x i-1 ) / (t i - t i-1 ) delta divided by the interval of the two polls in seconds average x = (x i - x i-1 ) / (y i - y i-1 ) delta divided by the delta of the base counter y percent x = 100 * (x i - x i-1 ) / (y i - y i-1 ) average multiplied by 100","title":"Metrics"},{"location":"configure-zapi/#parameters_1","text":"The parameters of the collector are distributed across three files: Harvest configuration file (default: harvest.yml ) ZapiPerf configuration file (default: conf/zapiperf/default.yaml ) Each object has its own configuration file (located in conf/zapiperf/cdot/ and conf/zapiperf/7mode/ for cDot and 7Mode systems respectively) Except for addr , datacenter and auth_style , all other parameters of the ZapiPerf collector can be defined in either of these three files. Parameters defined in the lower-level file, override parameters in the higher-level file. This allows the user to configure each objects individually, or use the same parameters for all objects. The full set of parameters are described below .","title":"Parameters"},{"location":"configure-zapi/#harvest-configuration-file","text":"Parameters in poller section should define (at least) the address and authentication method of the target system: parameter type description default addr string, required address (IP or FQDN) of the ONTAP system datacenter string, required name of the datacenter where the target system is located auth_style string, optional authentication method: either basic_auth or certificate_auth basic_auth ssl_cert , ssl_key string, optional full path of the SSL certificate and key pairs (when using certificate_auth ) username , password string, optional full path of the SSL certificate and key pairs (when using basic_auth )","title":"Harvest configuration file"},{"location":"configure-zapi/#zapiperf-configuration-file","text":"This configuration file (the \"template\") contains a list of objects that should be collected and the filenames of their configuration (explained in the next section). Additionally, this file contains the parameters that are applied as defaults to all objects. (As mentioned before, any of these parameters can be defined in the Harvest or object configuration files as well). parameter type description default use_insecure_tls bool, optional skip verifying TLS certificate of the target system false client_timeout duration (Go-syntax) how long to wait for server responses 30s batch_size int, optional max instances per API request 500 latency_io_reqd int, optional threshold of IOPs for calculating latency metrics (latencies based on very few IOPs are unreliable) 100 schedule list, required the poll frequencies of the collector/object, should include exactly these three elements in the exact same other: - counter duration (Go-syntax) poll frequency of updating the counter metadata cache (example value: 1200s = 20m ) - instance duration (Go-syntax) poll frequency of updating the instance cache (example value: 600s = 10m ) - data duration (Go-syntax) poll frequency of updating the data cache (example value: 60s = 1m ) Note Harvest allows defining poll intervals on sub-second level (e.g. 1ms ), however keep in mind the following: API response of an ONTAP system can take several seconds, so the collector is likely to enter failed state if the poll interval is less than client_timeout . Small poll intervals will create significant workload on the ONTAP system, as many counters are aggregated on-demand. Some metric values become less significant if they are calculated for very short intervals (e.g. latencies) The template should define objects in the objects section. Example: objects : SystemNode : system_node.yaml HostAdapter : hostadapter.yaml Note that for each object we only define the filename of the object configuration file. The object configuration files are located in subdirectories matching to the ONTAP version that was used to create these files. It is possible to have multiple version-subdirectories for multiple ONTAP versions. At runtime, the collector will select the object configuration file that closest matches to the version of the target ONTAP system. (A mismatch is tolerated since ZapiPerf will fetch and validate counter metadata from the system.)","title":"ZapiPerf configuration file"},{"location":"configure-zapi/#object-configuration-file_1","text":"The Object configuration file (\"subtemplate\") should contain the following parameters: parameter type description default name string display name of the collector that will collect this object object string short name of the object query string raw object name used to issue a ZAPI request counters list list of counters to collect (see notes below) instance_key string label to use as instance key (either name or uuid ) override list of key-value pairs override counter properties that we get from ONTAP (allows circumventing ZAPI bugs) plugins list plugins and their parameters to run on the collected data export_options list parameters to pass to exporters (see notes below)","title":"Object configuration file"},{"location":"configure-zapi/#counters","text":"This section defines the list of counters that will be collected. These counters can be labels, numeric metrics or histograms. The exact property of each counter is fetched from ONTAP and updated periodically. Some counters require a \"base-counter\" for post-processing. If the base-counter is missing, ZapiPerf will still run, but the missing data won't be exported. The display name of a counter can be changed with => (e.g., nfsv3_ops => ops ). There's one conversion Harvest does for you by default, the instance_name counter will be renamed to the value of object . Counters that are stored as labels will only be exported if they are included in the export_options section.","title":"counters"},{"location":"configure-zapi/#export_options","text":"Parameters in this section tell the exporters how to handle the collected data. The set of parameters varies by exporter. For Prometheus and InfluxDB exporters, the following parameters can be defined: instances_keys (list): display names of labels to export with each data-point instance_labels (list): display names of labels to export as a separate data-point include_all_labels (bool): export all labels with each data-point (overrides previous two parameters)","title":"export_options"},{"location":"dashboards/","text":"Harvest can be used to import dashboards to Grafana. The bin/harvest garfana utility requires the address (hostname or IP), port of the Grafana server, and a Grafana API token. The port can be omitted if Grafana is configured to redirect the URL. Use the -d flag to point to the directory that contains the dashboards. Grafana API token \u00b6 The utility tool asks for an API token which can be generated from the Grafana web-gui. Click on Configuration in the left menu bar (1), click on API Keys (2) and click on the New API Key button. Choose a Key name (3), choose Editor for role (4) and click on add (5). Copy the generated key and paste it in your terminal or add the token to the Tools section of your configuration file. (see below) For example, let's say your Grafana server is on http://my.grafana.server:3000 and you want to import the Prometheus-based dashboards from the grafana directory. You would run this: $ bin/grafana import --addr my.grafana.server:3000 Similarly, to export: $ bin/grafana export --addr my.grafana.server:3000 --directory /path/to/export/directory --serverfolder grafanaFolderName By default, the dashboards are connected to the Prometheus datasource defined in Grafana. If your datasource has a different name, use the --datasource flag during import/export.","title":"Dashboards"},{"location":"dashboards/#grafana-api-token","text":"The utility tool asks for an API token which can be generated from the Grafana web-gui. Click on Configuration in the left menu bar (1), click on API Keys (2) and click on the New API Key button. Choose a Key name (3), choose Editor for role (4) and click on add (5). Copy the generated key and paste it in your terminal or add the token to the Tools section of your configuration file. (see below) For example, let's say your Grafana server is on http://my.grafana.server:3000 and you want to import the Prometheus-based dashboards from the grafana directory. You would run this: $ bin/grafana import --addr my.grafana.server:3000 Similarly, to export: $ bin/grafana export --addr my.grafana.server:3000 --directory /path/to/export/directory --serverfolder grafanaFolderName By default, the dashboards are connected to the Prometheus datasource defined in Grafana. If your datasource has a different name, use the --datasource flag during import/export.","title":"Grafana API token"},{"location":"influxdb-exporter/","text":"InfluxDB Exporter \u00b6 InfluxDB Install The information below describes how to setup Harvest's InfluxDB exporter. If you need help installing or setting up InfluxDB, check out their documention . Overview \u00b6 The InfluxDB Exporter will format metrics into the InfluxDB's line protocol and write it into a bucket. The Exporter is compatible with InfluxDB v2.0. For explanation about bucket , org and precision , see InfluxDB API documentation . If you are monitoring both CDOT and 7mode clusters, it is strongly recommended to use two different buckets. Parameters \u00b6 Overview of all parameters is provided below. Only one of url or addr should be provided and at least one of them is required. If addr is specified, it should be a valid TCP address or hostname of the InfluxDB server and should not include the scheme. When using addr , the bucket , org , and token key/values are required. addr only works with HTTP. If you need to use HTTPS, you should use url instead. If url is specified, you must add all arguments to the url. Harvest will do no additional processing and use exactly what you specify. ( e.g. url: https://influxdb.example.com:8086/write?db=netapp&u=user&p=pass&precision=2 . When using url , the bucket , org , port , and precision fields will be ignored. parameter type description default url string URL of the database, format: SCHEME://HOST[:PORT] addr string address of the database, format: HOST (HTTP only) port int, optional port of the database 8086 bucket string, required with addr InfluxDB bucket to write org string, required with addr InfluxDB organization name precision string, required with addr Preferred timestamp precision in seconds 2 client_timeout int, optional client timeout in seconds 5 token string token for authentication Example \u00b6 snippet from harvest.yml using addr : (supports HTTP only)) Exporters : my_influx : exporter : InfluxDB addr : localhost bucket : harvest org : harvest token : ZTTrt%24@#WNFM2VZTTNNT25wZWUdtUmhBZEdVUmd3dl@# snippet from harvest.yml using url : (supports both HTTP/HTTPS)) Exporters : influx2 : exporter : InfluxDB url : https://localhost:8086/api/v2/write?org=harvest&bucket=harvest&precision=s token : my-token== Notice: InfluxDB stores a token in ~/.influxdbv2/configs , but you can also retrieve it from the UI (usually serving on localhost:8086 ): click on \"Data\" on the left task bar, then on \"Tokens\".","title":"InfluxDB"},{"location":"influxdb-exporter/#influxdb-exporter","text":"InfluxDB Install The information below describes how to setup Harvest's InfluxDB exporter. If you need help installing or setting up InfluxDB, check out their documention .","title":"InfluxDB Exporter"},{"location":"influxdb-exporter/#overview","text":"The InfluxDB Exporter will format metrics into the InfluxDB's line protocol and write it into a bucket. The Exporter is compatible with InfluxDB v2.0. For explanation about bucket , org and precision , see InfluxDB API documentation . If you are monitoring both CDOT and 7mode clusters, it is strongly recommended to use two different buckets.","title":"Overview"},{"location":"influxdb-exporter/#parameters","text":"Overview of all parameters is provided below. Only one of url or addr should be provided and at least one of them is required. If addr is specified, it should be a valid TCP address or hostname of the InfluxDB server and should not include the scheme. When using addr , the bucket , org , and token key/values are required. addr only works with HTTP. If you need to use HTTPS, you should use url instead. If url is specified, you must add all arguments to the url. Harvest will do no additional processing and use exactly what you specify. ( e.g. url: https://influxdb.example.com:8086/write?db=netapp&u=user&p=pass&precision=2 . When using url , the bucket , org , port , and precision fields will be ignored. parameter type description default url string URL of the database, format: SCHEME://HOST[:PORT] addr string address of the database, format: HOST (HTTP only) port int, optional port of the database 8086 bucket string, required with addr InfluxDB bucket to write org string, required with addr InfluxDB organization name precision string, required with addr Preferred timestamp precision in seconds 2 client_timeout int, optional client timeout in seconds 5 token string token for authentication","title":"Parameters"},{"location":"influxdb-exporter/#example","text":"snippet from harvest.yml using addr : (supports HTTP only)) Exporters : my_influx : exporter : InfluxDB addr : localhost bucket : harvest org : harvest token : ZTTrt%24@#WNFM2VZTTNNT25wZWUdtUmhBZEdVUmd3dl@# snippet from harvest.yml using url : (supports both HTTP/HTTPS)) Exporters : influx2 : exporter : InfluxDB url : https://localhost:8086/api/v2/write?org=harvest&bucket=harvest&precision=s token : my-token== Notice: InfluxDB stores a token in ~/.influxdbv2/configs , but you can also retrieve it from the UI (usually serving on localhost:8086 ): click on \"Data\" on the left task bar, then on \"Tokens\".","title":"Example"},{"location":"license/","text":"Harvest's License","title":"License"},{"location":"manage-harvest/","text":"Coming Soon","title":"Manage Harvest Pollers"},{"location":"ontap-metrics/","text":"This document contains the details about Harvest Metrics and their relevant ONTAP ZAPI and REST API mapping. Creation Date : 2023-Jan-12 ONTAP Version: 9.12.1 Understanding the structure \u00b6 Click on Details below to help understand the structure of Metrics . Details - Harvest Metric : Name of metric exported by Harvest Description : Description of the metric ZAPI : endpoint : Name of ZAPI invoked to generate this metric metric : Metric name in ZAPI template : ZAPI template path unit : Unit of the counter. Possible values= per_sec, b_per_sec (bytes/s), kb_per_sec (Kbytes/s),mb_per_sec (Mbytes/s), percent, millisec, microsec, sec, or none type : Comma separated list of properties of the counter. The counter properties determine how raw counter values should be interpreted.Possible values= raw, rate, delta, percent, string,no-display and no-zero-values. base : Name of the counter used as the denominator to calculate values of counters involving averages and percentages. REST : endpoint : Name of REST API invoked to generate this metric metric : Metric name in REST API template : REST template path unit : Unit of the counter. Possible values : per_sec, b_per_sec (bytes/s), kb_per_sec (Kbytes/s),mb_per_sec (Mbytes/s), percent, millisec, microsec, sec, or none type : Comma separated list of properties of the counter. The counter properties determine how raw counter values should be interpreted.Possible values : raw, rate, delta, percent, string,no-display and no-zero-values. base : Name of the counter used as the denominator to calculate values of counters involving averages and percentages. Metrics \u00b6 - Harvest Metric : aggr_efficiency_savings Description : Space saved by storage efficiencies (logical_used - used) REST : endpoint : api/storage/aggregates metric : space.efficiency.savings template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_efficiency_savings_wo_snapshots Description : Space saved by storage efficiencies (logical_used - used) REST : endpoint : api/storage/aggregates metric : space.efficiency_without_snapshots.savings template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_efficiency_savings_wo_snapshots_flexclones Description : Space saved by storage efficiencies (logical_used - used) REST : endpoint : api/storage/aggregates metric : space.efficiency_without_snapshots_flexclones.savings template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_hybrid_cache_size_total Description : Total usable space in bytes of SSD cache. Only provided when hybrid_cache.enabled is 'true'. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.hybrid-cache-size-total template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : block_storage.hybrid_cache.size template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_hybrid_disk_count Description : Number of disks used in the cache tier of the aggregate. Only provided when hybrid_cache.enabled is 'true'. REST : endpoint : api/storage/aggregates metric : block_storage.hybrid_cache.disk_count template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_files_private_used Description : Number of system metadata files used. If the referenced file system is restricted or offline, a value of 0 is returned.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either footprint or **. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.files-private-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.files_private_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_files_total Description : Maximum number of user-visible files that this referenced file system can currently hold. If the referenced file system is restricted or offline, a value of 0 is returned. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.files-total template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.files_total template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_files_used Description : Number of user-visible files used in the referenced file system. If the referenced file system is restricted or offline, a value of 0 is returned. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.files-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.files_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_inodefile_private_capacity Description : Number of files that can currently be stored on disk for system metadata files. This number will dynamically increase as more system files are created.This is an advanced property; there is an added computationl cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either footprint or **. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.inodefile-private-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.file_private_capacity template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_inodefile_public_capacity Description : Number of files that can currently be stored on disk for user-visible files. This number will dynamically increase as more user-visible files are created.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either footprint or **. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.inodefile-public-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.file_public_capacity template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_maxfiles_available Description : The count of the maximum number of user-visible files currently allowable on the referenced file system. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.maxfiles-available template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.max_files_available template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_maxfiles_possible Description : The largest value to which the maxfiles-available parameter can be increased by reconfiguration, on the referenced file system. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.maxfiles-possible template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.max_files_possible template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_maxfiles_used Description : The number of user-visible files currently in use on the referenced file system. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.maxfiles-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.max_files_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_used_percent Description : The percentage of disk space currently in use based on user-visible file count on the referenced file system. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.percent-inode-used-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.used_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_logical_used_wo_snapshots Description : Logical used ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-logical-used-wo-snapshots template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : api/storage/aggregates metric : space.efficiency_without_snapshots.logical_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_logical_used_wo_snapshots_flexclones Description : Logical used ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-logical-used-wo-snapshots-flexclones template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : api/storage/aggregates metric : space.efficiency_without_snapshots_flexclones.logical_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_physical_used_wo_snapshots Description : Total Data Reduction Physical Used Without Snapshots ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-physical-used-wo-snapshots template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_physical_used_wo_snapshots_flexclones Description : Total Data Reduction Physical Used without snapshots and flexclones ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-physical-used-wo-snapshots-flexclones template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_primary_disk_count Description : Number of disks used in the aggregate. This includes parity disks, but excludes disks in the hybrid cache. REST : endpoint : api/storage/aggregates metric : block_storage.primary.disk_count template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_raid_disk_count Description : Number of disks in the aggregate. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-raid-attributes.disk-count template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_raid_plex_count Description : Number of plexes in the aggregate ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-raid-attributes.plex-count template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : block_storage.plexes.# template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_raid_size Description : Option to specify the maximum number of disks that can be included in a RAID group. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-raid-attributes.raid-size template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : block_storage.primary.raid_size template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_files_total Description : Total files allowed in Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.files-total template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : snapshot.files_total template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_files_used Description : Total files created in Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.files-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : snapshot.files_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_inode_used_percent ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.percent-inode-used-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml - Harvest Metric : aggr_snapshot_maxfiles_available Description : Maximum files available for Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.maxfiles-available template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : snapshot.max_files_available template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_maxfiles_possible Description : The largest value to which the maxfiles-available parameter can be increased by reconfiguration, on the referenced file system. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.maxfiles-possible template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_maxfiles_used Description : Files in use by Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.maxfiles-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : snapshot.max_files_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_reserve_percent Description : Percentage of space reserved for Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.snapshot-reserve-percent template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.snapshot.reserve_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_size_available Description : Available space for Snapshot copies in bytes ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.size-available template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.snapshot.available template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_size_total Description : Total space for Snapshot copies in bytes ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.size-total template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.snapshot.total template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_size_used Description : Space used by Snapshot copies in bytes ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.size-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.snapshot.used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_used_percent Description : Percentage of disk space used by Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.percent-used-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.snapshot.used_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_available Description : Space available in bytes. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.size-available template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.available template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_capacity_tier_used Description : Used space in bytes in the cloud store. Only applicable for aggregates with a cloud store tier. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.capacity-tier-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.cloud_storage.used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_data_compacted_count Description : Amount of compacted data in bytes. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.data-compacted-count template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.data_compacted_count template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_data_compaction_saved Description : Space saved in bytes by compacting the data. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.data-compaction-space-saved template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.data_compaction_space_saved template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_data_compaction_saved_percent Description : Percentage saved by compacting the data. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.data-compaction-space-saved-percent template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.data_compaction_space_saved_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_performance_tier_inactive_user_data Description : The size that is physically used in the block storage and has a cold temperature, in bytes. This property is only supported if the aggregate is either attached to a cloud store or can be attached to a cloud store.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either block_storage.inactive_user_data or **. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.performance-tier-inactive-user-data template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.inactive_user_data template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_performance_tier_inactive_user_data_percent Description : The percentage of inactive user data in the block storage. This property is only supported if the aggregate is either attached to a cloud store or can be attached to a cloud store.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either block_storage.inactive_user_data_percent or **. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.performance-tier-inactive-user-data-percent template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.inactive_user_data_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_physical_used Description : Total physical used size of an aggregate in bytes. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.physical-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.physical_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_physical_used_percent Description : Physical used percentage. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.physical-used-percent template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.physical_used_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_reserved ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.total-reserved-space template : conf/zapi/cdot/9.8.0/aggr.yaml - Harvest Metric : aggr_space_sis_saved Description : Amount of space saved in bytes by storage efficiency. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.sis-space-saved template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.volume_deduplication_space_saved template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_sis_saved_percent Description : Percentage of space saved by storage efficiency. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.sis-space-saved-percent template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.volume_deduplication_space_saved_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_sis_shared_count Description : Amount of shared bytes counted by storage efficiency. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.sis-shared-count template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.volume_deduplication_shared_count template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_total Description : Total usable space in bytes, not including WAFL reserve and aggregate Snapshot copy reserve. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.size-total template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.size template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_used Description : Space used or reserved in bytes. Includes volume guarantees and aggregate metadata. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.size-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_used_percent Description : The percentage of disk space currently in use on the referenced file system ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.percent-used-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_total_logical_used Description : Logical used ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-logical-used template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : api/storage/aggregates metric : space.efficiency.logical_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_total_physical_used Description : Total Physical Used ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-physical-used template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_volume_count_flexvol ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-volume-count-attributes.flexvol-count template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : volume_count template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : cluster_subsystem_outstanding_alerts Description : Number of outstanding alerts ZAPI : endpoint : diagnosis-subsystem-config-get-iter metric : diagnosis-subsystem-config-info.outstanding-alert-count template : conf/zapi/cdot/9.8.0/subsystem.yaml REST : endpoint : api/private/cli/system/health/subsystem metric : outstanding_alert_count template : conf/rest/9.12.0/subsystem.yaml - Harvest Metric : cluster_subsystem_suppressed_alerts Description : Number of suppressed alerts ZAPI : endpoint : diagnosis-subsystem-config-get-iter metric : diagnosis-subsystem-config-info.suppressed-alert-count template : conf/zapi/cdot/9.8.0/subsystem.yaml REST : endpoint : api/private/cli/system/health/subsystem metric : suppressed_alert_count template : conf/rest/9.12.0/subsystem.yaml - Harvest Metric : copy_manager_bce_copy_count_curr Description : Current number of copy requests being processed by the Block Copy Engine. ZAPI : endpoint : perf-object-get-instances copy_manager metric : bce_copy_count_curr template : conf/zapiperf/cdot/9.8.0/copy_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/copy_manager metric : block_copy_engine_current_copy_count template : conf/restperf/9.12.0/copy_manager.yaml unit : none type : delta - Harvest Metric : copy_manager_kb_copied Description : Sum of kilo-bytes copied. ZAPI : endpoint : perf-object-get-instances copy_manager metric : KB_copied template : conf/zapiperf/cdot/9.8.0/copy_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/copy_manager metric : KB_copied template : conf/restperf/9.12.0/copy_manager.yaml unit : none type : delta - Harvest Metric : copy_manager_ocs_copy_count_curr Description : Current number of copy requests being processed by the ONTAP copy subsystem. ZAPI : endpoint : perf-object-get-instances copy_manager metric : ocs_copy_count_curr template : conf/zapiperf/cdot/9.8.0/copy_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/copy_manager metric : ontap_copy_subsystem_current_copy_count template : conf/restperf/9.12.0/copy_manager.yaml unit : none type : delta - Harvest Metric : copy_manager_sce_copy_count_curr Description : Current number of copy requests being processed by the System Continuous Engineering. ZAPI : endpoint : perf-object-get-instances copy_manager metric : sce_copy_count_curr template : conf/zapiperf/cdot/9.8.0/copy_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/copy_manager metric : system_continuous_engineering_current_copy_count template : conf/restperf/9.12.0/copy_manager.yaml unit : none type : delta - Harvest Metric : copy_manager_spince_copy_count_curr Description : Current number of copy requests being processed by the SpinCE. ZAPI : endpoint : perf-object-get-instances copy_manager metric : spince_copy_count_curr template : conf/zapiperf/cdot/9.8.0/copy_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/copy_manager metric : spince_current_copy_count template : conf/restperf/9.12.0/copy_manager.yaml unit : none type : delta - Harvest Metric : disk_busy Description : The utilization percent of the disk ZAPI : endpoint : perf-object-get-instances disk:constituent metric : disk_busy template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : percent type : percent base : base_for_disk_busy REST : endpoint : api/cluster/counter/tables/disk:constituent metric : disk_busy_percent template : conf/restperf/9.12.0/disk.yaml unit : percent type : percent base : base_for_disk_busy - Harvest Metric : disk_bytes_per_sector Description : Bytes per sector. ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-inventory-info.bytes-per-sector template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/storage/disks metric : bytes_per_sector template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_capacity Description : Disk capacity in MB ZAPI : endpoint : perf-object-get-instances disk:constituent metric : disk_capacity template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : mb type : raw REST : endpoint : api/cluster/counter/tables/disk:constituent metric : capacity template : conf/restperf/9.12.0/disk.yaml unit : mb type : raw - Harvest Metric : disk_cp_read_chain Description : Average number of blocks transferred in each consistency point read operation during a CP ZAPI : endpoint : perf-object-get-instances disk:constituent metric : cp_read_chain template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : none type : average base : cp_reads REST : endpoint : api/cluster/counter/tables/disk:constituent metric : cp_read_chain template : conf/restperf/9.12.0/disk.yaml unit : none type : average base : cp_read_count - Harvest Metric : disk_cp_read_latency Description : Average latency per block in microseconds for consistency point read operations ZAPI : endpoint : perf-object-get-instances disk:constituent metric : cp_read_latency template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : microsec type : average base : cp_read_blocks REST : endpoint : api/cluster/counter/tables/disk:constituent metric : cp_read_latency template : conf/restperf/9.12.0/disk.yaml unit : microsec type : average base : cp_read_blocks - Harvest Metric : disk_cp_reads Description : Number of disk read operations initiated each second for consistency point processing ZAPI : endpoint : perf-object-get-instances disk:constituent metric : cp_reads template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : cp_read_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : disk_io_pending Description : Average number of I/Os issued to the disk for which we have not yet received the response ZAPI : endpoint : perf-object-get-instances disk:constituent metric : io_pending template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : none type : average base : base_for_disk_busy REST : endpoint : api/cluster/counter/tables/disk:constituent metric : io_pending template : conf/restperf/9.12.0/disk.yaml unit : none type : average base : base_for_disk_busy - Harvest Metric : disk_io_queued Description : Number of I/Os queued to the disk but not yet issued ZAPI : endpoint : perf-object-get-instances disk:constituent metric : io_queued template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : none type : average base : base_for_disk_busy REST : endpoint : api/cluster/counter/tables/disk:constituent metric : io_queued template : conf/restperf/9.12.0/disk.yaml unit : none type : average base : base_for_disk_busy - Harvest Metric : disk_power_on_hours Description : Hours powered on. REST : endpoint : api/storage/disks metric : stats.power_on_hours template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_sectors Description : Number of sectors on the disk. ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-inventory-info.capacity-sectors template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/storage/disks metric : sector_count template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_stats_average_latency Description : Average I/O latency across all active paths, in milliseconds. ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-stats-info.average-latency template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/storage/disks metric : stats.average_latency template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_stats_io_kbps Description : Total Disk Throughput in KBPS Across All Active Paths ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-stats-info.disk-io-kbps template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/private/cli/disk metric : disk_io_kbps_total template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_stats_sectors_read Description : Number of Sectors Read ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-stats-info.sectors-read template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/private/cli/disk metric : sectors_read template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_stats_sectors_written Description : Number of Sectors Written ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-stats-info.sectors-written template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/private/cli/disk metric : sectors_written template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_total_transfers Description : Total number of disk operations involving data transfer initiated per second ZAPI : endpoint : perf-object-get-instances disk:constituent metric : total_transfers template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : total_transfer_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : disk_uptime Description : Number of seconds the drive has been powered on ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-stats-info.power-on-time-interval template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_usable_size REST : endpoint : api/storage/disks metric : usable_size template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_user_read_blocks Description : Number of blocks transferred for user read operations per second ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_read_blocks template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_read_block_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : disk_user_read_chain Description : Average number of blocks transferred in each user read operation ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_read_chain template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : none type : average base : user_reads REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_read_chain template : conf/restperf/9.12.0/disk.yaml unit : none type : average base : user_read_count - Harvest Metric : disk_user_read_latency Description : Average latency per block in microseconds for user read operations ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_read_latency template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : microsec type : average base : user_read_blocks REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_read_latency template : conf/restperf/9.12.0/disk.yaml unit : microsec type : average base : user_read_block_count - Harvest Metric : disk_user_reads Description : Number of disk read operations initiated each second for retrieving data or metadata associated with user requests ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_reads template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_read_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : disk_user_write_blocks Description : Number of blocks transferred for user write operations per second ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_write_blocks template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_write_block_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : disk_user_write_chain Description : Average number of blocks transferred in each user write operation ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_write_chain template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : none type : average base : user_writes REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_write_chain template : conf/restperf/9.12.0/disk.yaml unit : none type : average base : user_write_count - Harvest Metric : disk_user_write_latency Description : Average latency per block in microseconds for user write operations ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_write_latency template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : microsec type : average base : user_write_blocks REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_write_latency template : conf/restperf/9.12.0/disk.yaml unit : microsec type : average base : user_write_block_count - Harvest Metric : disk_user_writes Description : Number of disk write operations initiated each second for storing data or metadata associated with user requests ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_writes template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_write_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : environment_sensor_threshold_value Description : Provides the sensor reading. ZAPI : endpoint : environment-sensors-get-iter metric : environment-sensors-info.threshold-sensor-value template : conf/zapi/cdot/9.8.0/sensor.yaml REST : endpoint : api/cluster/sensors metric : value template : conf/rest/9.12.0/sensor.yaml - Harvest Metric : fabricpool_average_latency Description : Note This counter is deprecated and will be removed in a future release. Average latencies executed during various phases of command execution. The execution-start latency represents the average time taken to start executing a operation. The request-prepare latency represent the average time taken to prepare the commplete request that needs to be sent to the server. The send latency represents the average time taken to send requests to the server. The execution-start-to-send-complete represents the average time taken to send a operation out since its execution started. The execution-start-to-first-byte-received represent the average time taken to to receive the first byte of a response since the command&apos;s request execution started. These counters can be used to identify performance bottlenecks within the object store client module. ZAPI : endpoint : perf-object-get-instances object_store_client_op metric : average_latency template : conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml unit : microsec type : average,no-zero-values base : ops - Harvest Metric : fabricpool_cloud_bin_op_latency_average Description : Cloud bin operation latency average in milliseconds. ZAPI : endpoint : perf-object-get-instances wafl_comp_aggr_vol_bin metric : cloud_bin_op_latency_average template : conf/zapiperf/cdot/9.8.0/wafl_comp_aggr_vol_bin.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/wafl_comp_aggr_vol_bin metric : cloud_bin_op_latency_average template : conf/restperf/9.12.0/wafl_comp_aggr_vol_bin.yaml unit : none type : raw - Harvest Metric : fabricpool_cloud_bin_operation Description : Cloud bin operation counters. ZAPI : endpoint : perf-object-get-instances wafl_comp_aggr_vol_bin metric : cloud_bin_operation template : conf/zapiperf/cdot/9.8.0/wafl_comp_aggr_vol_bin.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl_comp_aggr_vol_bin metric : cloud_bin_op template : conf/restperf/9.12.0/wafl_comp_aggr_vol_bin.yaml unit : none type : delta - Harvest Metric : fabricpool_get_throughput_bytes Description : Note This counter is deprecated and will be removed in a future release. Counter that indicates the throughput for GET command in bytes per second. ZAPI : endpoint : perf-object-get-instances object_store_client_op metric : get_throughput_bytes template : conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml unit : b_per_sec type : rate,no-zero-values - Harvest Metric : fabricpool_put_throughput_bytes Description : Note This counter is deprecated and will be removed in a future release. Counter that indicates the throughput for PUT command in bytes per second. ZAPI : endpoint : perf-object-get-instances object_store_client_op metric : put_throughput_bytes template : conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml unit : b_per_sec type : rate,no-zero-values - Harvest Metric : fabricpool_stats Description : Note This counter is deprecated and will be removed in a future release. Counter that indicates the number of object store operations sent, and their success and failure counts. The objstore_client_op_name array indicate the operation name such as PUT, GET, etc. The objstore_client_op_stats_name array contain the total number of operations, their success and failure counter for each operation. ZAPI : endpoint : perf-object-get-instances object_store_client_op metric : stats template : conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml unit : none type : delta,no-zero-values - Harvest Metric : fabricpool_throughput_ops Description : Counter that indicates the throughput for commands in ops per second. ZAPI : endpoint : perf-object-get-instances object_store_client_op metric : throughput_ops template : conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : fcp_avg_other_latency Description : Average latency for operations other than read and write ZAPI : endpoint : perf-object-get-instances fcp_port metric : avg_other_latency template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/fcp metric : average_other_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : other_ops - Harvest Metric : fcp_avg_read_latency Description : Average latency for read operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : avg_read_latency template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/fcp metric : average_read_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : read_ops - Harvest Metric : fcp_avg_write_latency Description : Average latency for write operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : avg_write_latency template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/fcp metric : average_write_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : write_ops - Harvest Metric : fcp_discarded_frames_count Description : Number of discarded frames. ZAPI : endpoint : perf-object-get-instances fcp_port metric : discarded_frames_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : discarded_frames_count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_int_count Description : Number of interrupts ZAPI : endpoint : perf-object-get-instances fcp_port metric : int_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : interrupt_count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_invalid_crc Description : Number of invalid cyclic redundancy checks (CRC count) ZAPI : endpoint : perf-object-get-instances fcp_port metric : invalid_crc template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : invalid.crc template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_invalid_transmission_word Description : Number of invalid transmission words ZAPI : endpoint : perf-object-get-instances fcp_port metric : invalid_transmission_word template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : invalid.transmission_word template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_isr_count Description : Number of interrupt responses ZAPI : endpoint : perf-object-get-instances fcp_port metric : isr_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : isr.count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_lif_avg_latency Description : Average latency for FCP operations ZAPI : endpoint : perf-object-get-instances fcp_lif metric : avg_latency template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : microsec type : average base : total_ops REST : endpoint : api/cluster/counter/tables/fcp_lif metric : average_latency template : conf/restperf/9.12.0/fcp_lif.yaml unit : microsec type : average base : total_ops - Harvest Metric : fcp_lif_avg_other_latency Description : Average latency for operations other than read and write ZAPI : endpoint : perf-object-get-instances fcp_lif metric : avg_other_latency template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/fcp_lif metric : average_other_latency template : conf/restperf/9.12.0/fcp_lif.yaml unit : microsec type : average base : other_ops - Harvest Metric : fcp_lif_avg_read_latency Description : Average latency for read operations ZAPI : endpoint : perf-object-get-instances fcp_lif metric : avg_read_latency template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/fcp_lif metric : average_read_latency template : conf/restperf/9.12.0/fcp_lif.yaml unit : microsec type : average base : read_ops - Harvest Metric : fcp_lif_avg_write_latency Description : Average latency for write operations ZAPI : endpoint : perf-object-get-instances fcp_lif metric : avg_write_latency template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/fcp_lif metric : average_write_latency template : conf/restperf/9.12.0/fcp_lif.yaml unit : microsec type : average base : write_ops - Harvest Metric : fcp_lif_other_ops Description : Number of operations that are not read or write. ZAPI : endpoint : perf-object-get-instances fcp_lif metric : other_ops template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : other_ops template : conf/restperf/9.12.0/fcp_lif.yaml unit : per_sec type : rate - Harvest Metric : fcp_lif_read_data Description : Amount of data read from the storage system ZAPI : endpoint : perf-object-get-instances fcp_lif metric : read_data template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : read_data template : conf/restperf/9.12.0/fcp_lif.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_lif_read_ops Description : Number of read operations ZAPI : endpoint : perf-object-get-instances fcp_lif metric : read_ops template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : read_ops template : conf/restperf/9.12.0/fcp_lif.yaml unit : per_sec type : rate - Harvest Metric : fcp_lif_total_ops Description : Total number of operations. ZAPI : endpoint : perf-object-get-instances fcp_lif metric : total_ops template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : total_ops template : conf/restperf/9.12.0/fcp_lif.yaml unit : per_sec type : rate - Harvest Metric : fcp_lif_write_data Description : Amount of data written to the storage system ZAPI : endpoint : perf-object-get-instances fcp_lif metric : write_data template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : write_data template : conf/restperf/9.12.0/fcp_lif.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_lif_write_ops Description : Number of write operations ZAPI : endpoint : perf-object-get-instances fcp_lif metric : write_ops template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : write_ops template : conf/restperf/9.12.0/fcp_lif.yaml unit : per_sec type : rate - Harvest Metric : fcp_link_down Description : Number of times the Fibre Channel link was lost ZAPI : endpoint : perf-object-get-instances fcp_port metric : link_down template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : link.down template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_link_failure Description : Number of link failures ZAPI : endpoint : perf-object-get-instances fcp_port metric : link_failure template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : link_failure template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_loss_of_signal Description : Number of times this port lost signal ZAPI : endpoint : perf-object-get-instances fcp_port metric : loss_of_signal template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : loss_of_signal template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_loss_of_sync Description : Number of times this port lost sync ZAPI : endpoint : perf-object-get-instances fcp_port metric : loss_of_sync template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : loss_of_sync template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_nvmf_avg_other_latency Description : Average latency for operations other than read and write (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_other_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_other_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_other_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf.other_ops - Harvest Metric : fcp_nvmf_avg_read_latency Description : Average latency for read operations (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_read_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_read_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_read_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf.read_ops - Harvest Metric : fcp_nvmf_avg_remote_other_latency Description : Average latency for remote operations other than read and write (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_remote_other_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_remote_other_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_remote_other_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf_remote.other_ops - Harvest Metric : fcp_nvmf_avg_remote_read_latency Description : Average latency for remote read operations (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_remote_read_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_remote_read_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_remote_read_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf_remote.read_ops - Harvest Metric : fcp_nvmf_avg_remote_write_latency Description : Average latency for remote write operations (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_remote_write_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_remote_write_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_remote_write_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf_remote.write_ops - Harvest Metric : fcp_nvmf_avg_write_latency Description : Average latency for write operations (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_write_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_write_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_write_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf.write_ops - Harvest Metric : fcp_nvmf_caw_data Description : Amount of CAW data sent to the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_caw_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.caw_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_caw_ops Description : Number of FC-NVMe CAW operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_caw_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.caw_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_command_slots Description : Number of command slots that have been used by initiators logging into this port. This shows the command fan-in on the port. ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_command_slots template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.command_slots template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_other_ops Description : Number of NVMF operations that are not read or write. ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_other_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.other_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_read_data Description : Amount of data read from the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_read_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.read_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_read_ops Description : Number of FC-NVMe read operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_read_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.read_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_remote_caw_data Description : Amount of remote CAW data sent to the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_caw_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.caw_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_remote_caw_ops Description : Number of FC-NVMe remote CAW operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_caw_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.caw_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_remote_other_ops Description : Number of NVMF remote operations that are not read or write. ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_other_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.other_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_remote_read_data Description : Amount of remote data read from the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_read_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.read_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_remote_read_ops Description : Number of FC-NVMe remote read operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_read_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.read_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_remote_total_data Description : Amount of remote FC-NVMe traffic to and from the storage system ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_total_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.total_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_remote_total_ops Description : Total number of remote FC-NVMe operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_total_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.total_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_remote_write_data Description : Amount of remote data written to the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_write_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.write_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_remote_write_ops Description : Number of FC-NVMe remote write operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_write_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.write_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_total_data Description : Amount of FC-NVMe traffic to and from the storage system ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_total_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.total_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_total_ops Description : Total number of FC-NVMe operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_total_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.total_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_write_data Description : Amount of data written to the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_write_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.write_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_write_ops Description : Number of FC-NVMe write operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_write_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.write_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_other_ops Description : Number of operations that are not read or write. ZAPI : endpoint : perf-object-get-instances fcp_port metric : other_ops template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : other_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_prim_seq_err Description : Number of primitive sequence errors ZAPI : endpoint : perf-object-get-instances fcp_port metric : prim_seq_err template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : primitive_seq_err template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_queue_full Description : Number of times a queue full condition occurred. ZAPI : endpoint : perf-object-get-instances fcp_port metric : queue_full template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta,no-zero-values REST : endpoint : api/cluster/counter/tables/fcp metric : queue_full template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_read_data Description : Amount of data read from the storage system ZAPI : endpoint : perf-object-get-instances fcp_port metric : read_data template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : read_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_read_ops Description : Number of read operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : read_ops template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : read_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_reset_count Description : Number of physical port resets ZAPI : endpoint : perf-object-get-instances fcp_port metric : reset_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : reset_count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_shared_int_count Description : Number of shared interrupts ZAPI : endpoint : perf-object-get-instances fcp_port metric : shared_int_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : shared_interrupt_count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_spurious_int_count Description : Number of spurious interrupts ZAPI : endpoint : perf-object-get-instances fcp_port metric : spurious_int_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : spurious_interrupt_count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_threshold_full Description : Number of times the total number of outstanding commands on the port exceeds the threshold supported by this port. ZAPI : endpoint : perf-object-get-instances fcp_port metric : threshold_full template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta,no-zero-values REST : endpoint : api/cluster/counter/tables/fcp metric : threshold_full template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_total_data Description : Amount of FCP traffic to and from the storage system ZAPI : endpoint : perf-object-get-instances fcp_port metric : total_data template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : total_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_total_ops Description : Total number of FCP operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : total_ops template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : total_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_write_data Description : Amount of data written to the storage system ZAPI : endpoint : perf-object-get-instances fcp_port metric : write_data template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : write_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_write_ops Description : Number of write operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : write_ops template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : write_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcvi_rdma_write_avg_latency Description : Average RDMA write I/O latency. ZAPI : endpoint : perf-object-get-instances fcvi metric : rdma_write_avg_latency template : conf/zapiperf/cdot/9.8.0/fcvi.yaml unit : microsec type : average base : rdma_write_ops REST : endpoint : api/cluster/counter/tables/fcvi metric : rdma.write_average_latency template : conf/restperf/9.12.0/fcvi.yaml unit : microsec type : average base : rdma.write_ops - Harvest Metric : fcvi_rdma_write_ops Description : Number of RDMA write I/Os issued per second. ZAPI : endpoint : perf-object-get-instances fcvi metric : rdma_write_ops template : conf/zapiperf/cdot/9.8.0/fcvi.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/fcvi metric : rdma.write_ops template : conf/restperf/9.12.0/fcvi.yaml unit : none type : rate - Harvest Metric : fcvi_rdma_write_throughput Description : RDMA write throughput in bytes per second. ZAPI : endpoint : perf-object-get-instances fcvi metric : rdma_write_throughput template : conf/zapiperf/cdot/9.8.0/fcvi.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcvi metric : rdma.write_throughput template : conf/restperf/9.12.0/fcvi.yaml unit : b_per_sec type : rate - Harvest Metric : flashcache_accesses Description : External cache accesses per second ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : accesses template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : accesses template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_disk_reads_replaced Description : Estimated number of disk reads per second replaced by cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : disk_reads_replaced template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : disk_reads_replaced template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_evicts Description : Number of blocks evicted from the external cache to make room for new blocks ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : evicts template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : evicts template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit Description : Number of WAFL buffers served off the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.total template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit_directory Description : Number of directory buffers served off the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit_directory template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.directory template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit_indirect Description : Number of indirect file buffers served off the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit_indirect template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.indirect template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit_metadata_file Description : Number of metadata file buffers served off the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit_metadata_file template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.metadata_file template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit_normal_lev0 Description : Number of normal level 0 WAFL buffers served off the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit_normal_lev0 template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.normal_level_zero template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit_percent Description : External cache hit rate ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit_percent template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : percent type : percent base : accesses REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.percent template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : percent type : average base : accesses - Harvest Metric : flashcache_inserts Description : Number of WAFL buffers inserted into the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : inserts template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : inserts template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_invalidates Description : Number of blocks invalidated in the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : invalidates template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : invalidates template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_miss Description : External cache misses ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : miss template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : miss.total template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_miss_directory Description : External cache misses accessing directory buffers ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : miss_directory template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : miss.directory template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_miss_indirect Description : External cache misses accessing indirect file buffers ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : miss_indirect template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : miss.indirect template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_miss_metadata_file Description : External cache misses accessing metadata file buffers ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : miss_metadata_file template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : miss.metadata_file template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_miss_normal_lev0 Description : External cache misses accessing normal level 0 buffers ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : miss_normal_lev0 template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : miss.normal_level_zero template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_usage Description : Percentage of blocks in external cache currently containing valid data ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : usage template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : percent type : raw REST : endpoint : api/cluster/counter/tables/external_cache metric : usage template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : percent type : raw - Harvest Metric : flashpool_cache_stats Description : Automated Working-set Analyzer (AWA) per-interval pseudo cache statistics for the most recent intervals. The number of intervals defined as recent is CM_WAFL_HYAS_INT_DIS_CNT. This array is a table with fields corresponding to the enum type of hyas_cache_stat_type_t. ZAPI : endpoint : perf-object-get-instances wafl_hya_sizer metric : cache_stats template : conf/zapiperf/cdot/9.8.0/wafl_hya_sizer.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/wafl_hya_sizer metric : cache_stats template : conf/restperf/9.12.0/wafl_hya_sizer.yaml unit : none type : raw - Harvest Metric : flashpool_evict_destage_rate Description : Number of block destage per second. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : evict_destage_rate template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : evict_destage_rate template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_evict_remove_rate Description : Number of block free per second. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : evict_remove_rate template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : evict_remove_rate template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_hya_read_hit_latency_average Description : Average of RAID I/O latency on read hit. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : hya_read_hit_latency_average template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_read_hit_latency_count REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : hya_read_hit_latency_average template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_read_hit_latency_count - Harvest Metric : flashpool_hya_read_miss_latency_average Description : Average read miss latency. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : hya_read_miss_latency_average template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_read_miss_latency_count REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : hya_read_miss_latency_average template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_read_miss_latency_count - Harvest Metric : flashpool_hya_write_hdd_latency_average Description : Average write latency to HDD. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : hya_write_hdd_latency_average template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_write_hdd_latency_count REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : hya_write_hdd_latency_average template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_write_hdd_latency_count - Harvest Metric : flashpool_hya_write_ssd_latency_average Description : Average of RAID I/O latency on write to SSD. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : hya_write_ssd_latency_average template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_write_ssd_latency_count REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : hya_write_ssd_latency_average template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_write_ssd_latency_count - Harvest Metric : flashpool_read_cache_ins_rate Description : Cache insert rate blocks/sec. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : read_cache_ins_rate template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : read_cache_insert_rate template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_read_ops_replaced Description : Number of HDD read operations replaced by SSD reads per second. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : read_ops_replaced template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : read_ops_replaced template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_read_ops_replaced_percent Description : Percentage of HDD read operations replace by SSD. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : read_ops_replaced_percent template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : percent type : average base : read_ops_total REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : read_ops_replaced_percent template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : percent type : average base : read_ops_total - Harvest Metric : flashpool_ssd_available Description : Total SSD blocks available. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : ssd_available template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : ssd_available template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : raw - Harvest Metric : flashpool_ssd_read_cached Description : Total read cached SSD blocks. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : ssd_read_cached template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : ssd_read_cached template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : raw - Harvest Metric : flashpool_ssd_total Description : Total SSD blocks. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : ssd_total template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : ssd_total template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : raw - Harvest Metric : flashpool_ssd_total_used Description : Total SSD blocks used. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : ssd_total_used template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : ssd_total_used template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : raw - Harvest Metric : flashpool_ssd_write_cached Description : Total write cached SSD blocks. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : ssd_write_cached template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : ssd_write_cached template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : raw - Harvest Metric : flashpool_wc_write_blks_total Description : Number of write-cache blocks written per second. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : wc_write_blks_total template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : wc_write_blocks_total template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_write_blks_replaced Description : Number of HDD write blocks replaced by SSD writes per second. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : write_blks_replaced template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : write_blocks_replaced template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_write_blks_replaced_percent Description : Percentage of blocks overwritten to write-cache among all disk writes. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : write_blks_replaced_percent template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : percent type : average base : est_write_blks_total REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : write_blocks_replaced_percent template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : percent type : average base : estimated_write_blocks_total - Harvest Metric : headroom_aggr_current_latency Description : This is the storage aggregate average latency per message at the disk level. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : current_latency template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : microsec type : average base : current_ops REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : current_latency template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : microsec type : average base : current_ops - Harvest Metric : headroom_aggr_current_ops Description : Total number of I/Os processed by the aggregate per second. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : current_ops template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : per_sec type : rate REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : current_ops template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : per_sec type : rate - Harvest Metric : headroom_aggr_current_utilization Description : This is the storage aggregate average utilization of all the data disks in the aggregate. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : current_utilization template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : percent type : percent base : current_utilization_total REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : current_utilization template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : percent type : percent base : current_utilization_denominator - Harvest Metric : headroom_aggr_ewma_daily Description : Daily exponential weighted moving average. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : ewma_daily template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : ewma.daily template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : raw - Harvest Metric : headroom_aggr_ewma_hourly Description : Hourly exponential weighted moving average. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : ewma_hourly template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : ewma.hourly template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : raw - Harvest Metric : headroom_aggr_ewma_monthly Description : Monthly exponential weighted moving average. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : ewma_monthly template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : ewma.monthly template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : raw - Harvest Metric : headroom_aggr_ewma_weekly Description : Weekly exponential weighted moving average. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : ewma_weekly template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : ewma.weekly template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : raw - Harvest Metric : headroom_aggr_optimal_point_confidence_factor Description : The confidence factor for the optimal point value based on the observed resource latency and utilization. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : optimal_point_confidence_factor template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : optimal_point.confidence_factor template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : average base : optimal_point.samples - Harvest Metric : headroom_aggr_optimal_point_latency Description : The latency component of the optimal point of the latency/utilization curve. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : optimal_point_latency template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : microsec type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : optimal_point.latency template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : microsec type : average base : optimal_point.samples - Harvest Metric : headroom_aggr_optimal_point_ops Description : The ops component of the optimal point derived from the latency/utilzation curve. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : optimal_point_ops template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : per_sec type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : optimal_point.ops template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : per_sec type : average base : optimal_point.samples - Harvest Metric : headroom_aggr_optimal_point_utilization Description : The utilization component of the optimal point of the latency/utilization curve. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : optimal_point_utilization template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : optimal_point.utilization template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : average base : optimal_point.samples - Harvest Metric : headroom_cpu_current_latency Description : Current operation latency of the resource. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : current_latency template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : microsec type : average base : current_ops REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : current_latency template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : microsec type : average base : current_ops - Harvest Metric : headroom_cpu_current_ops Description : Total number of operations per second (also referred to as dblade ops). ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : current_ops template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : per_sec type : rate REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : current_ops template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : per_sec type : rate - Harvest Metric : headroom_cpu_current_utilization Description : Average processor utilization across all processors in the system. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : current_utilization template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : percent type : percent base : current_utilization_total REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : current_utilization template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : percent type : percent base : elapsed_time - Harvest Metric : headroom_cpu_ewma_daily Description : Daily exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : ewma_daily template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : ewma.daily template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : raw - Harvest Metric : headroom_cpu_ewma_hourly Description : Hourly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : ewma_hourly template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : ewma.hourly template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : raw - Harvest Metric : headroom_cpu_ewma_monthly Description : Monthly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : ewma_monthly template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : ewma.monthly template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : raw - Harvest Metric : headroom_cpu_ewma_weekly Description : Weekly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : ewma_weekly template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : ewma.weekly template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : raw - Harvest Metric : headroom_cpu_optimal_point_confidence_factor Description : Confidence factor for the optimal point value based on the observed resource latency and utilization. The possible values are : 0 - unknown, 1 - low, 2 - medium, 3 - high. This counter can provide an average confidence factor over a range of time. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : optimal_point_confidence_factor template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : optimal_point.confidence_factor template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : average base : optimal_point.samples - Harvest Metric : headroom_cpu_optimal_point_latency Description : Latency component of the optimal point of the latency/utilization curve. This counter can provide an average latency over a range of time. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : optimal_point_latency template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : microsec type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : optimal_point.latency template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : microsec type : average base : optimal_point.samples - Harvest Metric : headroom_cpu_optimal_point_ops Description : Ops component of the optimal point derived from the latency/utilization curve. This counter can provide an average ops over a range of time. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : optimal_point_ops template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : per_sec type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : optimal_point.ops template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : per_sec type : average base : optimal_point.samples - Harvest Metric : headroom_cpu_optimal_point_utilization Description : Utilization component of the optimal point of the latency/utilization curve. This counter can provide an average utilization over a range of time. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : optimal_point_utilization template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : optimal_point.utilization template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : average base : optimal_point.samples - Harvest Metric : hostadapter_bytes_read Description : Bytes read through a host adapter ZAPI : endpoint : perf-object-get-instances hostadapter metric : bytes_read template : conf/zapiperf/cdot/9.8.0/hostadapter.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/host_adapter metric : bytes_read template : conf/restperf/9.12.0/hostadapter.yaml unit : per_sec type : rate - Harvest Metric : hostadapter_bytes_written Description : Bytes written through a host adapter ZAPI : endpoint : perf-object-get-instances hostadapter metric : bytes_written template : conf/zapiperf/cdot/9.8.0/hostadapter.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/host_adapter metric : bytes_written template : conf/restperf/9.12.0/hostadapter.yaml unit : per_sec type : rate - Harvest Metric : iscsi_lif_avg_latency Description : Average latency for iSCSI operations ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : avg_latency template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : microsec type : average base : cmd_transfered REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : average_latency template : conf/restperf/9.12.0/iscsi_lif.yaml unit : microsec type : average base : cmd_transferred - Harvest Metric : iscsi_lif_avg_other_latency Description : Average latency for operations other than read and write (for example, Inquiry, Report LUNs, SCSI Task Management Functions) ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : avg_other_latency template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_other_ops REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : average_other_latency template : conf/restperf/9.12.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_other_ops - Harvest Metric : iscsi_lif_avg_read_latency Description : Average latency for read operations ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : avg_read_latency template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_read_ops REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : average_read_latency template : conf/restperf/9.12.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_read_ops - Harvest Metric : iscsi_lif_avg_write_latency Description : Average latency for write operations ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : avg_write_latency template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_write_ops REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : average_write_latency template : conf/restperf/9.12.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_write_ops - Harvest Metric : iscsi_lif_cmd_transfered Description : Command transfered by this iSCSI conn ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : cmd_transfered template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : none type : rate - Harvest Metric : iscsi_lif_cmd_transferred Description : Command transferred by this iSCSI connection REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : cmd_transferred template : conf/restperf/9.12.0/iscsi_lif.yaml unit : none type : rate - Harvest Metric : iscsi_lif_iscsi_other_ops Description : iSCSI other operations per second on this logical interface (LIF) ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : iscsi_other_ops template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : iscsi_other_ops template : conf/restperf/9.12.0/iscsi_lif.yaml unit : per_sec type : rate - Harvest Metric : iscsi_lif_iscsi_read_ops Description : iSCSI read operations per second on this logical interface (LIF) ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : iscsi_read_ops template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : iscsi_read_ops template : conf/restperf/9.12.0/iscsi_lif.yaml unit : per_sec type : rate - Harvest Metric : iscsi_lif_iscsi_write_ops Description : iSCSI write operations per second on this logical interface (LIF) ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : iscsi_write_ops template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : iscsi_write_ops template : conf/restperf/9.12.0/iscsi_lif.yaml unit : per_sec type : rate - Harvest Metric : iscsi_lif_protocol_errors Description : Number of protocol errors from iSCSI sessions on this logical interface (LIF) ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : protocol_errors template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : protocol_errors template : conf/restperf/9.12.0/iscsi_lif.yaml unit : none type : delta - Harvest Metric : iscsi_lif_read_data Description : Amount of data read from the storage system in bytes ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : read_data template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : read_data template : conf/restperf/9.12.0/iscsi_lif.yaml unit : b_per_sec type : rate - Harvest Metric : iscsi_lif_write_data Description : Amount of data written to the storage system in bytes ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : write_data template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : write_data template : conf/restperf/9.12.0/iscsi_lif.yaml unit : b_per_sec type : rate - Harvest Metric : lif_recv_data Description : Number of bytes received per second ZAPI : endpoint : perf-object-get-instances lif metric : recv_data template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : received_data template : conf/restperf/9.12.0/lif.yaml unit : b_per_sec type : rate - Harvest Metric : lif_recv_errors Description : Number of received Errors per second ZAPI : endpoint : perf-object-get-instances lif metric : recv_errors template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : received_errors template : conf/restperf/9.12.0/lif.yaml unit : per_sec type : rate - Harvest Metric : lif_recv_packet Description : Number of packets received per second ZAPI : endpoint : perf-object-get-instances lif metric : recv_packet template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : received_packets template : conf/restperf/9.12.0/lif.yaml unit : per_sec type : rate - Harvest Metric : lif_sent_data Description : Number of bytes sent per second ZAPI : endpoint : perf-object-get-instances lif metric : sent_data template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : sent_data template : conf/restperf/9.12.0/lif.yaml unit : b_per_sec type : rate - Harvest Metric : lif_sent_errors Description : Number of sent errors per second ZAPI : endpoint : perf-object-get-instances lif metric : sent_errors template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : sent_errors template : conf/restperf/9.12.0/lif.yaml unit : per_sec type : rate - Harvest Metric : lif_sent_packet Description : Number of packets sent per second ZAPI : endpoint : perf-object-get-instances lif metric : sent_packet template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : sent_packets template : conf/restperf/9.12.0/lif.yaml unit : per_sec type : rate - Harvest Metric : lun_avg_read_latency Description : Average read latency in microseconds for all operations on the LUN ZAPI : endpoint : perf-object-get-instances lun metric : avg_read_latency template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/lun metric : average_read_latency template : conf/restperf/9.12.0/lun.yaml unit : microsec type : average base : read_ops - Harvest Metric : lun_avg_write_latency Description : Average write latency in microseconds for all operations on the LUN ZAPI : endpoint : perf-object-get-instances lun metric : avg_write_latency template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/lun metric : average_write_latency template : conf/restperf/9.12.0/lun.yaml unit : microsec type : average base : write_ops - Harvest Metric : lun_avg_xcopy_latency Description : Average latency in microseconds for xcopy requests ZAPI : endpoint : perf-object-get-instances lun metric : avg_xcopy_latency template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : microsec type : average base : xcopy_reqs REST : endpoint : api/cluster/counter/tables/lun metric : average_xcopy_latency template : conf/restperf/9.12.0/lun.yaml unit : microsec type : average base : xcopy_requests - Harvest Metric : lun_caw_reqs Description : Number of compare and write requests ZAPI : endpoint : perf-object-get-instances lun metric : caw_reqs template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/lun metric : caw_requests template : conf/restperf/9.12.0/lun.yaml unit : none type : rate - Harvest Metric : lun_enospc Description : Number of operations receiving ENOSPC errors ZAPI : endpoint : perf-object-get-instances lun metric : enospc template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/lun metric : enospc template : conf/restperf/9.12.0/lun.yaml unit : none type : delta - Harvest Metric : lun_queue_full Description : Queue full responses ZAPI : endpoint : perf-object-get-instances lun metric : queue_full template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : queue_full template : conf/restperf/9.12.0/lun.yaml unit : per_sec type : rate - Harvest Metric : lun_read_align_histo Description : Histogram of WAFL read alignment (number sectors off WAFL block start) ZAPI : endpoint : perf-object-get-instances lun metric : read_align_histo template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : percent type : percent base : read_ops_sent REST : endpoint : api/cluster/counter/tables/lun metric : read_align_histogram template : conf/restperf/9.12.0/lun.yaml unit : percent type : percent base : read_ops_sent - Harvest Metric : lun_read_data Description : Read bytes ZAPI : endpoint : perf-object-get-instances lun metric : read_data template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : read_data template : conf/restperf/9.12.0/lun.yaml unit : b_per_sec type : rate - Harvest Metric : lun_read_ops Description : Number of read operations ZAPI : endpoint : perf-object-get-instances lun metric : read_ops template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : read_ops template : conf/restperf/9.12.0/lun.yaml unit : per_sec type : rate - Harvest Metric : lun_read_partial_blocks Description : Percentage of reads whose size is not a multiple of WAFL block size ZAPI : endpoint : perf-object-get-instances lun metric : read_partial_blocks template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : percent type : percent base : read_ops REST : endpoint : api/cluster/counter/tables/lun metric : read_partial_blocks template : conf/restperf/9.12.0/lun.yaml unit : percent type : percent base : read_ops - Harvest Metric : lun_remote_bytes Description : I/O to or from a LUN which is not owned by the storage system handling the I/O. ZAPI : endpoint : perf-object-get-instances lun metric : remote_bytes template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : remote_bytes template : conf/restperf/9.12.0/lun.yaml unit : b_per_sec type : rate - Harvest Metric : lun_remote_ops Description : Number of operations received by a storage system that does not own the LUN targeted by the operations. ZAPI : endpoint : perf-object-get-instances lun metric : remote_ops template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : remote_ops template : conf/restperf/9.12.0/lun.yaml unit : per_sec type : rate - Harvest Metric : lun_size Description : The total provisioned size of the LUN. The LUN size can be increased but not be made smaller using the REST interface.<br/>The maximum and minimum sizes listed here are the absolute maximum and absolute minimum sizes in bytes. The actual minimum and maxiumum sizes vary depending on the ONTAP version, ONTAP platform and the available space in the containing volume and aggregate.<br/>For more information, see _Size properties_ in the _docs_ section of the ONTAP REST API documentation. ZAPI : endpoint : lun-get-iter metric : lun-info.size template : conf/zapi/cdot/9.8.0/lun.yaml REST : endpoint : api/storage/luns metric : space.size template : conf/rest/9.12.0/lun.yaml - Harvest Metric : lun_size_used Description : The amount of space consumed by the main data stream of the LUN.<br/>This value is the total space consumed in the volume by the LUN, including filesystem overhead, but excluding prefix and suffix streams. Due to internal filesystem overhead and the many ways SAN filesystems and applications utilize blocks within a LUN, this value does not necessarily reflect actual consumption/availability from the perspective of the filesystem or application. Without specific knowledge of how the LUN blocks are utilized outside of ONTAP, this property should not be used as an indicator for an out-of-space condition.<br/>For more information, see _Size properties_ in the _docs_ section of the ONTAP REST API documentation. ZAPI : endpoint : lun-get-iter metric : lun-info.size-used template : conf/zapi/cdot/9.8.0/lun.yaml REST : endpoint : api/storage/luns metric : space.used template : conf/rest/9.12.0/lun.yaml - Harvest Metric : lun_unmap_reqs Description : Number of unmap command requests ZAPI : endpoint : perf-object-get-instances lun metric : unmap_reqs template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/lun metric : unmap_requests template : conf/restperf/9.12.0/lun.yaml unit : none type : rate - Harvest Metric : lun_write_align_histo Description : Histogram of WAFL write alignment (number of sectors off WAFL block start) ZAPI : endpoint : perf-object-get-instances lun metric : write_align_histo template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : percent type : percent base : write_ops_sent REST : endpoint : api/cluster/counter/tables/lun metric : write_align_histogram template : conf/restperf/9.12.0/lun.yaml unit : percent type : percent base : write_ops_sent - Harvest Metric : lun_write_data Description : Write bytes ZAPI : endpoint : perf-object-get-instances lun metric : write_data template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : write_data template : conf/restperf/9.12.0/lun.yaml unit : b_per_sec type : rate - Harvest Metric : lun_write_ops Description : Number of write operations ZAPI : endpoint : perf-object-get-instances lun metric : write_ops template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : write_ops template : conf/restperf/9.12.0/lun.yaml unit : per_sec type : rate - Harvest Metric : lun_write_partial_blocks Description : Percentage of writes whose size is not a multiple of WAFL block size ZAPI : endpoint : perf-object-get-instances lun metric : write_partial_blocks template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : percent type : percent base : write_ops REST : endpoint : api/cluster/counter/tables/lun metric : write_partial_blocks template : conf/restperf/9.12.0/lun.yaml unit : percent type : percent base : write_ops - Harvest Metric : lun_writesame_reqs Description : Number of write same command requests ZAPI : endpoint : perf-object-get-instances lun metric : writesame_reqs template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/lun metric : writesame_requests template : conf/restperf/9.12.0/lun.yaml unit : none type : rate - Harvest Metric : lun_writesame_unmap_reqs Description : Number of write same commands requests with unmap bit set ZAPI : endpoint : perf-object-get-instances lun metric : writesame_unmap_reqs template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/lun metric : writesame_unmap_requests template : conf/restperf/9.12.0/lun.yaml unit : none type : rate - Harvest Metric : lun_xcopy_reqs Description : Total number of xcopy operations on the LUN ZAPI : endpoint : perf-object-get-instances lun metric : xcopy_reqs template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/lun metric : xcopy_requests template : conf/restperf/9.12.0/lun.yaml unit : none type : rate - Harvest Metric : namespace_avg_other_latency Description : Average other ops latency in microseconds for all operations on the Namespace ZAPI : endpoint : perf-object-get-instances namespace metric : avg_other_latency template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/namespace metric : average_other_latency template : conf/restperf/9.12.0/namespace.yaml unit : microsec type : average base : other_ops - Harvest Metric : namespace_avg_read_latency Description : Average read latency in microseconds for all operations on the Namespace ZAPI : endpoint : perf-object-get-instances namespace metric : avg_read_latency template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/namespace metric : average_read_latency template : conf/restperf/9.12.0/namespace.yaml unit : microsec type : average base : read_ops - Harvest Metric : namespace_avg_write_latency Description : Average write latency in microseconds for all operations on the Namespace ZAPI : endpoint : perf-object-get-instances namespace metric : avg_write_latency template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/namespace metric : average_write_latency template : conf/restperf/9.12.0/namespace.yaml unit : microsec type : average base : write_ops - Harvest Metric : namespace_other_ops Description : Number of other operations ZAPI : endpoint : perf-object-get-instances namespace metric : other_ops template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/namespace metric : other_ops template : conf/restperf/9.12.0/namespace.yaml unit : per_sec type : rate - Harvest Metric : namespace_read_data Description : Read bytes ZAPI : endpoint : perf-object-get-instances namespace metric : read_data template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/namespace metric : read_data template : conf/restperf/9.12.0/namespace.yaml unit : b_per_sec type : rate - Harvest Metric : namespace_read_ops Description : Number of read operations ZAPI : endpoint : perf-object-get-instances namespace metric : read_ops template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/namespace metric : read_ops template : conf/restperf/9.12.0/namespace.yaml unit : per_sec type : rate - Harvest Metric : namespace_remote_bytes Description : Remote read bytes ZAPI : endpoint : perf-object-get-instances namespace metric : remote_bytes template : conf/zapiperf/cdot/9.10.1/namespace.yaml REST : endpoint : api/cluster/counter/tables/namespace metric : remote.read_data template : conf/restperf/9.12.0/namespace.yaml unit : b_per_sec type : rate - Harvest Metric : namespace_remote_ops Description : Number of remote read operations ZAPI : endpoint : perf-object-get-instances namespace metric : remote_ops template : conf/zapiperf/cdot/9.10.1/namespace.yaml REST : endpoint : api/cluster/counter/tables/namespace metric : remote.read_ops template : conf/restperf/9.12.0/namespace.yaml unit : per_sec type : rate - Harvest Metric : namespace_write_data Description : Write bytes ZAPI : endpoint : perf-object-get-instances namespace metric : write_data template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/namespace metric : write_data template : conf/restperf/9.12.0/namespace.yaml unit : b_per_sec type : rate - Harvest Metric : namespace_write_ops Description : Number of write operations ZAPI : endpoint : perf-object-get-instances namespace metric : write_ops template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/namespace metric : write_ops template : conf/restperf/9.12.0/namespace.yaml unit : per_sec type : rate - Harvest Metric : net_port_mtu Description : Maximum transmission unit, largest packet size on this network ZAPI : endpoint : net-port-get-iter metric : net-port-info.mtu template : conf/zapi/cdot/9.8.0/netPort.yaml REST : endpoint : api/network/ethernet/ports metric : mtu template : conf/rest/9.12.0/netPort.yaml - Harvest Metric : netstat_bytes_recvd Description : Number of bytes received by a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : bytes_recvd template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_bytes_sent Description : Number of bytes sent by a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : bytes_sent template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_cong_win Description : Congestion window of a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : cong_win template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_cong_win_th Description : Congestion window threshold of a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : cong_win_th template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_ooorcv_pkts Description : Number of out-of-order packets received by this TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : ooorcv_pkts template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_recv_window Description : Receive window size of a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : recv_window template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_rexmit_pkts Description : Number of packets retransmitted by this TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : rexmit_pkts template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_send_window Description : Send window size of a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : send_window template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : nfs_clients_idle_duration Description : Specifies an ISO-8601 format of date and time to retrieve the idle time duration in hours, minutes, and seconds format. REST : endpoint : api/protocols/nfs/connected-clients metric : idle_duration template : conf/rest/9.7.0/nfs_clients.yaml - Harvest Metric : nfs_diag_storePool_ByteLockAlloc Description : Current number of byte range lock objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ByteLockAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.byte_lock_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_ByteLockMax Description : Maximum number of byte range lock objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ByteLockMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.byte_lock_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_ClientAlloc Description : Current number of client objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ClientAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.client_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_ClientMax Description : Maximum number of client objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ClientMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.client_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_ConnectionParentSessionReferenceAlloc Description : Current number of connection parent session reference objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ConnectionParentSessionReferenceAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.connection_parent_session_reference_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_ConnectionParentSessionReferenceMax Description : Maximum number of connection parent session reference objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ConnectionParentSessionReferenceMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.connection_parent_session_reference_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_CopyStateAlloc Description : Current number of copy state objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_CopyStateAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.copy_state_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_CopyStateMax Description : Maximum number of copy state objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_CopyStateMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.copy_state_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_DelegAlloc Description : Current number of delegation lock objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_DelegAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.delegation_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_DelegMax Description : Maximum number delegation lock objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_DelegMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.delegation_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_DelegStateAlloc Description : Current number of delegation state objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_DelegStateAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.delegation_state_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_DelegStateMax Description : Maximum number of delegation state objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_DelegStateMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.delegation_state_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LayoutAlloc Description : Current number of layout objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LayoutAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.layout_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LayoutMax Description : Maximum number of layout objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LayoutMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.layout_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LayoutStateAlloc Description : Current number of layout state objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LayoutStateAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.layout_state_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LayoutStateMax Description : Maximum number of layout state objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LayoutStateMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.layout_state_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LockStateAlloc Description : Current number of lock state objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LockStateAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.lock_state_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LockStateMax Description : Maximum number of lock state objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LockStateMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.lock_state_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OpenAlloc Description : Current number of share objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OpenAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.open_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OpenMax Description : Maximum number of share lock objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OpenMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.open_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OpenStateAlloc Description : Current number of open state objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OpenStateAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.openstate_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OpenStateMax Description : Maximum number of open state objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OpenStateMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.openstate_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OwnerAlloc Description : Current number of owner objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OwnerAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.owner_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OwnerMax Description : Maximum number of owner objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OwnerMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.owner_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionAlloc Description : Current number of session objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionConnectionHolderAlloc Description : Current number of session connection holder objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionConnectionHolderAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_connection_holder_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionConnectionHolderMax Description : Maximum number of session connection holder objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionConnectionHolderMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_connection_holder_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionHolderAlloc Description : Current number of session holder objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionHolderAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_holder_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionHolderMax Description : Maximum number of session holder objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionHolderMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_holder_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionMax Description : Maximum number of session objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_StateRefHistoryAlloc Description : Current number of state reference callstack history objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_StateRefHistoryAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.state_reference_history_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_StateRefHistoryMax Description : Maximum number of state reference callstack history objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_StateRefHistoryMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.state_reference_history_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_StringAlloc Description : Current number of string objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_StringAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.string_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_StringMax Description : Maximum number of string objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_StringMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.string_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nic_link_up_to_downs Description : Number of link state change from UP to DOWN. ZAPI : endpoint : perf-object-get-instances nic_common metric : link_up_to_downs template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : link_up_to_down template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_rx_alignment_errors Description : Alignment errors detected on received packets ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_alignment_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_alignment_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_rx_bytes Description : Bytes received ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_bytes template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_bytes template : conf/restperf/9.12.0/nic_common.yaml unit : b_per_sec type : rate - Harvest Metric : nic_rx_crc_errors Description : CRC errors detected on received packets ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_crc_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_crc_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_rx_errors Description : Error received ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_errors template : conf/restperf/9.12.0/nic_common.yaml unit : b_per_sec type : rate - Harvest Metric : nic_rx_length_errors Description : Length errors detected on received packets ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_length_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_length_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_rx_total_errors Description : Total errors received ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_total_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_total_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_tx_bytes Description : Bytes sent ZAPI : endpoint : perf-object-get-instances nic_common metric : tx_bytes template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nic_common metric : transmit_bytes template : conf/restperf/9.12.0/nic_common.yaml unit : b_per_sec type : rate - Harvest Metric : nic_tx_errors Description : Error sent ZAPI : endpoint : perf-object-get-instances nic_common metric : tx_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nic_common metric : transmit_errors template : conf/restperf/9.12.0/nic_common.yaml unit : b_per_sec type : rate - Harvest Metric : nic_tx_hw_errors Description : Transmit errors reported by hardware ZAPI : endpoint : perf-object-get-instances nic_common metric : tx_hw_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : transmit_hw_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_tx_total_errors Description : Total errors sent ZAPI : endpoint : perf-object-get-instances nic_common metric : tx_total_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : transmit_total_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : node_avg_processor_busy Description : Average processor utilization across all processors in the system ZAPI : endpoint : perf-object-get-instances system:node metric : avg_processor_busy template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time REST : endpoint : api/cluster/counter/tables/system:node metric : average_processor_busy_percent template : conf/restperf/9.12.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time - Harvest Metric : node_cifs_connections Description : Number of connections ZAPI : endpoint : perf-object-get-instances cifs:node metric : connections template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : connections template : conf/restperf/9.12.0/cifs_node.yaml unit : none type : raw - Harvest Metric : node_cifs_established_sessions Description : Number of established SMB and SMB2 sessions ZAPI : endpoint : perf-object-get-instances cifs:node metric : established_sessions template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : established_sessions template : conf/restperf/9.12.0/cifs_node.yaml unit : none type : raw - Harvest Metric : node_cifs_latency Description : Average latency for CIFS operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_latency template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : microsec type : average base : cifs_latency_base REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : latency template : conf/restperf/9.12.0/cifs_node.yaml unit : microsec type : average base : latency_base - Harvest Metric : node_cifs_op_count Description : Array of select CIFS operation counts ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_op_count template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : op_count template : conf/restperf/9.12.0/cifs_node.yaml unit : none type : rate - Harvest Metric : node_cifs_open_files Description : Number of open files over SMB and SMB2 ZAPI : endpoint : perf-object-get-instances cifs:node metric : open_files template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : open_files template : conf/restperf/9.12.0/cifs_node.yaml unit : none type : raw - Harvest Metric : node_cifs_ops Description : Number of CIFS operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : cifs_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : cifs_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_cifs_read_latency Description : Average latency for CIFS read operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_read_latency template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : microsec type : average base : cifs_read_ops REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : average_read_latency template : conf/restperf/9.12.0/cifs_node.yaml unit : microsec type : average base : total_read_ops - Harvest Metric : node_cifs_read_ops Description : Total number of CIFS read operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_read_ops template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : total_read_ops template : conf/restperf/9.12.0/cifs_node.yaml unit : per_sec type : rate - Harvest Metric : node_cifs_total_ops Description : Total number of CIFS operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_ops template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : total_ops template : conf/restperf/9.12.0/cifs_node.yaml unit : per_sec type : rate - Harvest Metric : node_cifs_write_latency Description : Average latency for CIFS write operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_write_latency template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : microsec type : average base : cifs_write_ops REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : average_write_latency template : conf/restperf/9.12.0/cifs_node.yaml unit : microsec type : average base : total_write_ops - Harvest Metric : node_cifs_write_ops Description : Total number of CIFS write operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_write_ops template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : total_write_ops template : conf/restperf/9.12.0/cifs_node.yaml unit : per_sec type : rate - Harvest Metric : node_cpu_busy Description : System CPU resource utilization. Returns a computed percentage for the default CPU field. Basically computes a 'cpu usage summary' value which indicates how 'busy' the system is based upon the most heavily utilized domain. The idea is to determine the amount of available CPU until we're limited by either a domain maxing out OR we exhaust all available idle CPU cycles, whichever occurs first. ZAPI : endpoint : perf-object-get-instances system:node metric : cpu_busy template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time REST : endpoint : api/cluster/counter/tables/system:node metric : cpu_busy template : conf/restperf/9.12.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time - Harvest Metric : node_cpu_busytime Description : The time (in hundredths of a second) that the CPU has been doing useful work since the last boot ZAPI : endpoint : system-node-get-iter metric : node-details-info.cpu-busytime template : conf/zapi/cdot/9.8.0/node.yaml REST : endpoint : api/private/cli/node metric : cpu_busy_time template : conf/rest/9.12.0/node.yaml - Harvest Metric : node_cpu_domain_busy Description : Array of processor time in percentage spent in various domains ZAPI : endpoint : perf-object-get-instances system:node metric : domain_busy template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time REST : endpoint : api/cluster/counter/tables/system:node metric : domain_busy template : conf/restperf/9.12.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time - Harvest Metric : node_cpu_elapsed_time Description : Elapsed time since boot ZAPI : endpoint : perf-object-get-instances system:node metric : cpu_elapsed_time template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : none type : delta,no-display REST : endpoint : api/cluster/counter/tables/system:node metric : cpu_elapsed_time template : conf/restperf/9.12.0/system_node.yaml unit : microsec type : delta - Harvest Metric : node_disk_data_read Description : Number of disk kilobytes (KB) read per second ZAPI : endpoint : perf-object-get-instances system:node metric : disk_data_read template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : disk_data_read template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_disk_data_written Description : Number of disk kilobytes (KB) written per second ZAPI : endpoint : perf-object-get-instances system:node metric : disk_data_written template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : disk_data_written template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_failed_fan Description : Specifies a count of the number of chassis fans that are not operating within the recommended RPM range. ZAPI : endpoint : system-node-get-iter metric : node-details-info.env-failed-fan-count template : conf/zapi/cdot/9.8.0/node.yaml REST : endpoint : api/cluster/nodes metric : controller.failed_fan.count template : conf/rest/9.12.0/node.yaml - Harvest Metric : node_failed_power Description : Number of failed power supply units. ZAPI : endpoint : system-node-get-iter metric : node-details-info.env-failed-power-supply-count template : conf/zapi/cdot/9.8.0/node.yaml REST : endpoint : api/cluster/nodes metric : controller.failed_power_supply.count template : conf/rest/9.12.0/node.yaml - Harvest Metric : node_fcp_data_recv Description : Number of FCP kilobytes (KB) received per second ZAPI : endpoint : perf-object-get-instances system:node metric : fcp_data_recv template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : fcp_data_received template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_fcp_data_sent Description : Number of FCP kilobytes (KB) sent per second ZAPI : endpoint : perf-object-get-instances system:node metric : fcp_data_sent template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : fcp_data_sent template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_fcp_ops Description : Number of FCP operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : fcp_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : fcp_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_hdd_data_read Description : Number of HDD Disk kilobytes (KB) read per second ZAPI : endpoint : perf-object-get-instances system:node metric : hdd_data_read template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : hdd_data_read template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_hdd_data_written Description : Number of HDD kilobytes (KB) written per second ZAPI : endpoint : perf-object-get-instances system:node metric : hdd_data_written template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : hdd_data_written template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_iscsi_ops Description : Number of iSCSI operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : iscsi_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : iscsi_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_memory Description : Total memory in megabytes (MB) ZAPI : endpoint : perf-object-get-instances system:node metric : memory template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/system:node metric : memory template : conf/restperf/9.12.0/system_node.yaml unit : none type : raw - Harvest Metric : node_net_data_recv Description : Number of network kilobytes (KB) received per second ZAPI : endpoint : perf-object-get-instances system:node metric : net_data_recv template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : network_data_received template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_net_data_sent Description : Number of network kilobytes (KB) sent per second ZAPI : endpoint : perf-object-get-instances system:node metric : net_data_sent template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : network_data_sent template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_nfs_access_avg_latency Description : Average latency of ACCESS procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : access_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : access_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : access.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : access.total - Harvest Metric : node_nfs_access_total Description : Total number of ACCESS procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : access_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : access.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_backchannel_ctl_avg_latency Description : Average latency of NFSv4.2 BACKCHANNEL_CTL operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : backchannel_ctl_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : backchannel_ctl_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : backchannel_ctl.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : backchannel_ctl.total - Harvest Metric : node_nfs_backchannel_ctl_total Description : Total number of NFSv4.2 BACKCHANNEL_CTL operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : backchannel_ctl_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : backchannel_ctl.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_bind_conn_to_session_avg_latency Description : Average latency of NFSv4.2 BIND_CONN_TO_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : bind_conn_to_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : bind_conn_to_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : bind_conn_to_session.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : bind_conn_to_session.total - Harvest Metric : node_nfs_bind_conn_to_session_total Description : Total number of NFSv4.2 BIND_CONN_TO_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : bind_conn_to_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : bind_conn_to_session.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : delta - Harvest Metric : node_nfs_close_avg_latency Description : Average latency of CLOSE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : close_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : close_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : close.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : close.total - Harvest Metric : node_nfs_close_total Description : Total number of CLOSE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : close_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : close.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_commit_avg_latency Description : Average latency of COMMIT procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : commit_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : commit_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : commit.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : commit.total - Harvest Metric : node_nfs_commit_total Description : Total number of COMMIT procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : commit_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : commit.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_create_avg_latency Description : Average latency of CREATE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : create_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : create_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : create.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : create.total - Harvest Metric : node_nfs_create_session_avg_latency Description : Average latency of NFSv4.2 CREATE_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : create_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : create_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : create_session.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : create_session.total - Harvest Metric : node_nfs_create_session_total Description : Total number of NFSv4.2 CREATE_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : create_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : create_session.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_create_total Description : Total number of CREATE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : create_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : create.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_delegpurge_avg_latency Description : Average latency of DELEGPURGE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : delegpurge_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : delegpurge_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : delegpurge.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : delegpurge.total - Harvest Metric : node_nfs_delegpurge_total Description : Total number of DELEGPURGE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : delegpurge_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : delegpurge.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_delegreturn_avg_latency Description : Average latency of DELEGRETURN procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : delegreturn_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : delegreturn_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : delegreturn.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : delegreturn.total - Harvest Metric : node_nfs_delegreturn_total Description : Total number of DELEGRETURN procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : delegreturn_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : delegreturn.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_destroy_clientid_avg_latency Description : Average latency of NFSv4.2 DESTROY_CLIENTID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : destroy_clientid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : destroy_clientid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : destroy_clientid.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : destroy_clientid.total - Harvest Metric : node_nfs_destroy_clientid_total Description : Total number of NFSv4.2 DESTROY_CLIENTID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : destroy_clientid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : destroy_clientid.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_destroy_session_avg_latency Description : Average latency of NFSv4.2 DESTROY_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : destroy_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : destroy_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : destroy_session.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : destroy_session.total - Harvest Metric : node_nfs_destroy_session_total Description : Total number of NFSv4.2 DESTROY_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : destroy_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : destroy_session.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_exchange_id_avg_latency Description : Average latency of NFSv4.2 EXCHANGE_ID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : exchange_id_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : exchange_id_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : exchange_id.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : exchange_id.total - Harvest Metric : node_nfs_exchange_id_total Description : Total number of NFSv4.2 EXCHANGE_ID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : exchange_id_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : exchange_id.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_free_stateid_avg_latency Description : Average latency of NFSv4.2 FREE_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : free_stateid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : free_stateid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : free_stateid.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : free_stateid.total - Harvest Metric : node_nfs_free_stateid_total Description : Total number of NFSv4.2 FREE_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : free_stateid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : free_stateid.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_fsinfo_avg_latency Description : Average latency of FSInfo procedure requests. The counter keeps track of the average response time of FSInfo requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : fsinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : fsinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : fsinfo.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : fsinfo.total - Harvest Metric : node_nfs_fsinfo_total Description : Total number FSInfo of procedure requests. It is the total number of FSInfo success and FSInfo error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : fsinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : fsinfo.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_fsstat_avg_latency Description : Average latency of FSStat procedure requests. The counter keeps track of the average response time of FSStat requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : fsstat_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : fsstat_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : fsstat.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : fsstat.total - Harvest Metric : node_nfs_fsstat_total Description : Total number FSStat of procedure requests. It is the total number of FSStat success and FSStat error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : fsstat_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : fsstat.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_get_dir_delegation_avg_latency Description : Average latency of NFSv4.2 GET_DIR_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : get_dir_delegation_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : get_dir_delegation_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : get_dir_delegation.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : get_dir_delegation.total - Harvest Metric : node_nfs_get_dir_delegation_total Description : Total number of NFSv4.2 GET_DIR_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : get_dir_delegation_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : get_dir_delegation.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_getattr_avg_latency Description : Average latency of GETATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : getattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : getattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : getattr.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : getattr.total - Harvest Metric : node_nfs_getattr_total Description : Total number of GETATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : getattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : getattr.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_getdeviceinfo_avg_latency Description : Average latency of NFSv4.2 GETDEVICEINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : getdeviceinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : getdeviceinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : getdeviceinfo.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : getdeviceinfo.total - Harvest Metric : node_nfs_getdeviceinfo_total Description : Total number of NFSv4.2 GETDEVICEINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : getdeviceinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : getdeviceinfo.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_getdevicelist_avg_latency Description : Average latency of NFSv4.2 GETDEVICELIST operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : getdevicelist_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : getdevicelist_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : getdevicelist.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : getdevicelist.total - Harvest Metric : node_nfs_getdevicelist_total Description : Total number of NFSv4.2 GETDEVICELIST operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : getdevicelist_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : getdevicelist.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_getfh_avg_latency Description : Average latency of GETFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : getfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : getfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : getfh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : getfh.total - Harvest Metric : node_nfs_getfh_total Description : Total number of GETFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : getfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : getfh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_latency Description : Average latency of NFSv4 requests. This counter keeps track of the average response time of NFSv4 requests. ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : total_ops REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : total_ops - Harvest Metric : node_nfs_layoutcommit_avg_latency Description : Average latency of NFSv4.2 LAYOUTCOMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutcommit_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : layoutcommit_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutcommit.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : layoutcommit.total - Harvest Metric : node_nfs_layoutcommit_total Description : Total number of NFSv4.2 LAYOUTCOMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutcommit_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutcommit.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_layoutget_avg_latency Description : Average latency of NFSv4.2 LAYOUTGET operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutget_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : layoutget_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutget.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : layoutget.total - Harvest Metric : node_nfs_layoutget_total Description : Total number of NFSv4.2 LAYOUTGET operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutget_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutget.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_layoutreturn_avg_latency Description : Average latency of NFSv4.2 LAYOUTRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutreturn_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : layoutreturn_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutreturn.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : layoutreturn.total - Harvest Metric : node_nfs_layoutreturn_total Description : Total number of NFSv4.2 LAYOUTRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutreturn_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutreturn.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_link_avg_latency Description : Average latency of LINK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : link_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : link_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : link.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : link.total - Harvest Metric : node_nfs_link_total Description : Total number of LINK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : link_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : link.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_lock_avg_latency Description : Average latency of LOCK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lock_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : lock_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lock.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : lock.total - Harvest Metric : node_nfs_lock_total Description : Total number of LOCK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lock_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lock.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_lockt_avg_latency Description : Average latency of LOCKT procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lockt_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : lockt_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lockt.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : lockt.total - Harvest Metric : node_nfs_lockt_total Description : Total number of LOCKT procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lockt_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lockt.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_locku_avg_latency Description : Average latency of LOCKU procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : locku_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : locku_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : locku.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : locku.total - Harvest Metric : node_nfs_locku_total Description : Total number of LOCKU procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : locku_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : locku.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_lookup_avg_latency Description : Average latency of LOOKUP procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lookup_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : lookup_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lookup.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : lookup.total - Harvest Metric : node_nfs_lookup_total Description : Total number of LOOKUP procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lookup_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lookup.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_lookupp_avg_latency Description : Average latency of LOOKUPP procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lookupp_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : lookupp_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lookupp.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : lookupp.total - Harvest Metric : node_nfs_lookupp_total Description : Total number of LOOKUPP procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lookupp_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lookupp.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_mkdir_avg_latency Description : Average latency of MkDir procedure requests. The counter keeps track of the average response time of MkDir requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : mkdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : mkdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : mkdir.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : mkdir.total - Harvest Metric : node_nfs_mkdir_total Description : Total number MkDir of procedure requests. It is the total number of MkDir success and MkDir error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : mkdir_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : mkdir.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_mknod_avg_latency Description : Average latency of MkNod procedure requests. The counter keeps track of the average response time of MkNod requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : mknod_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : mknod_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : mknod.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : mknod.total - Harvest Metric : node_nfs_mknod_total Description : Total number MkNod of procedure requests. It is the total number of MkNod success and MkNod error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : mknod_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : mknod.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_null_avg_latency Description : Average Latency of NULL procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : null_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : null_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : null.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : null.total - Harvest Metric : node_nfs_null_total Description : Total number of NULL procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : null_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : null.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_nverify_avg_latency Description : Average latency of NVERIFY procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : nverify_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : nverify_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : nverify.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : nverify.total - Harvest Metric : node_nfs_nverify_total Description : Total number of NVERIFY procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : nverify_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : nverify.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_open_avg_latency Description : Average latency of OPEN procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : open_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : open.total - Harvest Metric : node_nfs_open_confirm_avg_latency Description : Average latency of OPEN_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_confirm_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : open_confirm_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open_confirm.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : open_confirm.total - Harvest Metric : node_nfs_open_confirm_total Description : Total number of OPEN_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_confirm_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open_confirm.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_open_downgrade_avg_latency Description : Average latency of OPEN_DOWNGRADE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_downgrade_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : open_downgrade_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open_downgrade.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : open_downgrade.total - Harvest Metric : node_nfs_open_downgrade_total Description : Total number of OPEN_DOWNGRADE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_downgrade_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open_downgrade.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_open_total Description : Total number of OPEN procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_openattr_avg_latency Description : Average latency of OPENATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : openattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : openattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : openattr.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : openattr.total - Harvest Metric : node_nfs_openattr_total Description : Total number of OPENATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : openattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : openattr.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_ops Description : Number of NFS operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : nfs_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : nfs_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_pathconf_avg_latency Description : Average latency of PathConf procedure requests. The counter keeps track of the average response time of PathConf requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : pathconf_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : pathconf_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : pathconf.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : pathconf.total - Harvest Metric : node_nfs_pathconf_total Description : Total number PathConf of procedure requests. It is the total number of PathConf success and PathConf error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : pathconf_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : pathconf.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_putfh_avg_latency Description : Average latency of PUTFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : putfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putfh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : putfh.total - Harvest Metric : node_nfs_putfh_total Description : Total number of PUTFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putfh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_putpubfh_avg_latency Description : Average latency of PUTPUBFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putpubfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : putpubfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putpubfh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : putpubfh.total - Harvest Metric : node_nfs_putpubfh_total Description : Total number of PUTPUBFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putpubfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putpubfh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_putrootfh_avg_latency Description : Average latency of PUTROOTFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putrootfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : putrootfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putrootfh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : putrootfh.total - Harvest Metric : node_nfs_putrootfh_total Description : Total number of PUTROOTFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putrootfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putrootfh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_read_avg_latency Description : Average latency of READ procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : read_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : read_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : read.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : read.total - Harvest Metric : node_nfs_read_ops Description : Total observed NFSv3 read operations per second. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : nfsv3_read_ops template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : read_ops template : conf/restperf/9.12.0/nfsv3_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_read_symlink_avg_latency Description : Average latency of ReadSymLink procedure requests. The counter keeps track of the average response time of ReadSymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : read_symlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : read_symlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : read_symlink.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : read_symlink.total - Harvest Metric : node_nfs_read_symlink_total Description : Total number of ReadSymLink procedure requests. It is the total number of read symlink success and read symlink error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : read_symlink_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : read_symlink.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : delta - Harvest Metric : node_nfs_read_throughput Description : NFSv4 read data transfers ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : nfs4_read_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : total.read_throughput template : conf/restperf/9.12.0/nfsv4_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_read_total Description : Total number of READ procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : read_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : read.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_readdir_avg_latency Description : Average latency of READDIR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : readdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : readdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : readdir.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : readdir.total - Harvest Metric : node_nfs_readdir_total Description : Total number of READDIR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : readdir_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : readdir.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_readdirplus_avg_latency Description : Average latency of ReadDirPlus procedure requests. The counter keeps track of the average response time of ReadDirPlus requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : readdirplus_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : readdirplus_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : readdirplus.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : readdirplus.total - Harvest Metric : node_nfs_readdirplus_total Description : Total number ReadDirPlus of procedure requests. It is the total number of ReadDirPlus success and ReadDirPlus error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : readdirplus_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : readdirplus.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_readlink_avg_latency Description : Average latency of READLINK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : readlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : readlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : readlink.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : readlink.total - Harvest Metric : node_nfs_readlink_total Description : Total number of READLINK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : readlink_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : readlink.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_reclaim_complete_avg_latency Description : Average latency of NFSv4.2 RECLAIM_complete operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : reclaim_complete_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : reclaim_complete_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : reclaim_complete.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : reclaim_complete.total - Harvest Metric : node_nfs_reclaim_complete_total Description : Total number of NFSv4.2 RECLAIM_complete operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : reclaim_complete_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : reclaim_complete.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_release_lock_owner_avg_latency Description : Average Latency of RELEASE_LOCKOWNER procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : release_lock_owner_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : release_lock_owner_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : release_lock_owner.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : release_lock_owner.total - Harvest Metric : node_nfs_release_lock_owner_total Description : Total number of RELEASE_LOCKOWNER procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : release_lock_owner_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : release_lock_owner.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_remove_avg_latency Description : Average latency of REMOVE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : remove_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : remove_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : remove.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : remove.total - Harvest Metric : node_nfs_remove_total Description : Total number of REMOVE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : remove_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : remove.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_rename_avg_latency Description : Average latency of RENAME procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : rename_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : rename_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : rename.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : rename.total - Harvest Metric : node_nfs_rename_total Description : Total number of RENAME procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : rename_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : rename.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_renew_avg_latency Description : Average latency of RENEW procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : renew_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : renew_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : renew.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : renew.total - Harvest Metric : node_nfs_renew_total Description : Total number of RENEW procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : renew_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : renew.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_restorefh_avg_latency Description : Average latency of RESTOREFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : restorefh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : restorefh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : restorefh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : restorefh.total - Harvest Metric : node_nfs_restorefh_total Description : Total number of RESTOREFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : restorefh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : restorefh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_rmdir_avg_latency Description : Average latency of RmDir procedure requests. The counter keeps track of the average response time of RmDir requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : rmdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : rmdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : rmdir.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : rmdir.total - Harvest Metric : node_nfs_rmdir_total Description : Total number RmDir of procedure requests. It is the total number of RmDir success and RmDir error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : rmdir_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : rmdir.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_savefh_avg_latency Description : Average latency of SAVEFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : savefh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : savefh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : savefh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : savefh.total - Harvest Metric : node_nfs_savefh_total Description : Total number of SAVEFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : savefh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : savefh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_secinfo_avg_latency Description : Average latency of SECINFO procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : secinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : secinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : secinfo.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : secinfo.total - Harvest Metric : node_nfs_secinfo_no_name_avg_latency Description : Average latency of NFSv4.2 SECINFO_NO_NAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : secinfo_no_name_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : secinfo_no_name_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : secinfo_no_name.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : secinfo_no_name.total - Harvest Metric : node_nfs_secinfo_no_name_total Description : Total number of NFSv4.2 SECINFO_NO_NAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : secinfo_no_name_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : secinfo_no_name.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_secinfo_total Description : Total number of SECINFO procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : secinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : secinfo.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_sequence_avg_latency Description : Average latency of NFSv4.2 SEQUENCE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : sequence_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : sequence_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : sequence.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : sequence.total - Harvest Metric : node_nfs_sequence_total Description : Total number of NFSv4.2 SEQUENCE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : sequence_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : sequence.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_set_ssv_avg_latency Description : Average latency of NFSv4.2 SET_SSV operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : set_ssv_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : set_ssv_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : set_ssv.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : set_ssv.total - Harvest Metric : node_nfs_set_ssv_total Description : Total number of NFSv4.2 SET_SSV operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : set_ssv_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : set_ssv.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_setattr_avg_latency Description : Average latency of SETATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : setattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setattr.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : setattr.total - Harvest Metric : node_nfs_setattr_total Description : Total number of SETATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setattr.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_setclientid_avg_latency Description : Average latency of SETCLIENTID procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setclientid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : setclientid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setclientid.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : setclientid.total - Harvest Metric : node_nfs_setclientid_confirm_avg_latency Description : Average latency of SETCLIENTID_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setclientid_confirm_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : setclientid_confirm_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setclientid_confirm.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : setclientid_confirm.total - Harvest Metric : node_nfs_setclientid_confirm_total Description : Total number of SETCLIENTID_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setclientid_confirm_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setclientid_confirm.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_setclientid_total Description : Total number of SETCLIENTID procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setclientid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setclientid.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_symlink_avg_latency Description : Average latency of SymLink procedure requests. The counter keeps track of the average response time of SymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : symlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : symlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : symlink.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : symlink.total - Harvest Metric : node_nfs_symlink_total Description : Total number SymLink of procedure requests. It is the total number of SymLink success and create SymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : symlink_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : symlink.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_test_stateid_avg_latency Description : Average latency of NFSv4.2 TEST_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : test_stateid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : test_stateid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : test_stateid.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : test_stateid.total - Harvest Metric : node_nfs_test_stateid_total Description : Total number of NFSv4.2 TEST_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : test_stateid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : test_stateid.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_throughput Description : NFSv4 data transfers ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : nfs4_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : total.throughput template : conf/restperf/9.12.0/nfsv4_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_total_ops Description : Total number of NFSv4 requests per second. ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : total_ops template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : total_ops template : conf/restperf/9.12.0/nfsv4_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_verify_avg_latency Description : Average latency of VERIFY procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : verify_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : verify_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : verify.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : verify.total - Harvest Metric : node_nfs_verify_total Description : Total number of VERIFY procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : verify_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : verify.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_want_delegation_avg_latency Description : Average latency of NFSv4.2 WANT_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : want_delegation_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : want_delegation_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : want_delegation.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : want_delegation.total - Harvest Metric : node_nfs_want_delegation_total Description : Total number of NFSv4.2 WANT_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : want_delegation_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : want_delegation.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_write_avg_latency Description : Average Latency of WRITE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : write_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : write_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : write.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : write.total - Harvest Metric : node_nfs_write_ops Description : Total observed NFSv3 write operations per second. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : nfsv3_write_ops template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : write_ops template : conf/restperf/9.12.0/nfsv3_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_write_throughput Description : NFSv4 write data transfers ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : nfs4_write_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : total.write_throughput template : conf/restperf/9.12.0/nfsv4_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_write_total Description : Total number of WRITE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : write_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : write.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nvmf_data_recv Description : NVMe/FC kilobytes (KB) received per second ZAPI : endpoint : perf-object-get-instances system:node metric : nvmf_data_recv template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : nvme_fc_data_received template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_nvmf_data_sent Description : NVMe/FC kilobytes (KB) sent per second ZAPI : endpoint : perf-object-get-instances system:node metric : nvmf_data_sent template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : nvme_fc_data_sent template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_nvmf_ops Description : NVMe/FC operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : nvmf_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : nvme_fc_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_ssd_data_read Description : Number of SSD Disk kilobytes (KB) read per second ZAPI : endpoint : perf-object-get-instances system:node metric : ssd_data_read template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : ssd_data_read template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_ssd_data_written Description : Number of SSD Disk kilobytes (KB) written per second ZAPI : endpoint : perf-object-get-instances system:node metric : ssd_data_written template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : ssd_data_written template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_total_data Description : Total throughput in bytes ZAPI : endpoint : perf-object-get-instances system:node metric : total_data template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : total_data template : conf/restperf/9.12.0/system_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_total_latency Description : Average latency for all operations in the system in microseconds ZAPI : endpoint : perf-object-get-instances system:node metric : total_latency template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : microsec type : average base : total_ops REST : endpoint : api/cluster/counter/tables/system:node metric : total_latency template : conf/restperf/9.12.0/system_node.yaml unit : microsec type : average base : total_ops - Harvest Metric : node_total_ops Description : Total number of operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : total_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : total_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_uptime Description : The total time, in seconds, that the node has been up. ZAPI : endpoint : system-node-get-iter metric : node-details-info.node-uptime template : conf/zapi/cdot/9.8.0/node.yaml REST : endpoint : api/cluster/nodes metric : uptime template : conf/rest/9.12.0/node.yaml - Harvest Metric : node_vol_cifs_other_latency Description : Average time for the WAFL filesystem to process other CIFS operations to the volume; not including CIFS protocol request processing or network communication time which will also be included in client observed CIFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_other_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : cifs_other_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.other_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : cifs.other_ops - Harvest Metric : node_vol_cifs_other_ops Description : Number of other CIFS operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_other_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.other_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_cifs_read_data Description : Bytes read per second via CIFS ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_read_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.read_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_cifs_read_latency Description : Average time for the WAFL filesystem to process CIFS read requests to the volume; not including CIFS protocol request processing or network communication time which will also be included in client observed CIFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_read_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : cifs_read_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.read_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : cifs.read_ops - Harvest Metric : node_vol_cifs_read_ops Description : Number of CIFS read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_read_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.read_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_cifs_write_data Description : Bytes written per second via CIFS ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_write_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.write_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_cifs_write_latency Description : Average time for the WAFL filesystem to process CIFS write requests to the volume; not including CIFS protocol request processing or network communication time which will also be included in client observed CIFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_write_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : cifs_write_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.write_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : cifs.write_ops - Harvest Metric : node_vol_cifs_write_ops Description : Number of CIFS write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_write_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.write_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_fcp_other_latency Description : Average time for the WAFL filesystem to process other FCP protocol operations to the volume; not including FCP protocol request processing or network communication time which will also be included in client observed FCP protocol request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_other_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : fcp_other_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.other_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : fcp.other_ops - Harvest Metric : node_vol_fcp_other_ops Description : Number of other block protocol operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_other_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.other_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_fcp_read_data Description : Bytes read per second via block protocol ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_read_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.read_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_fcp_read_latency Description : Average time for the WAFL filesystem to process FCP protocol read operations to the volume; not including FCP protocol request processing or network communication time which will also be included in client observed FCP protocol request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_read_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : fcp_read_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.read_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : fcp.read_ops - Harvest Metric : node_vol_fcp_read_ops Description : Number of block protocol read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_read_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.read_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_fcp_write_data Description : Bytes written per second via block protocol ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_write_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.write_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_fcp_write_latency Description : Average time for the WAFL filesystem to process FCP protocol write operations to the volume; not including FCP protocol request processing or network communication time which will also be included in client observed FCP protocol request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_write_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : fcp_write_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.write_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : fcp.write_ops - Harvest Metric : node_vol_fcp_write_ops Description : Number of block protocol write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_write_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.write_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_iscsi_other_latency Description : Average time for the WAFL filesystem to process other iSCSI protocol operations to the volume; not including iSCSI protocol request processing or network communication time which will also be included in client observed iSCSI protocol request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_other_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : iscsi_other_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.other_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : iscsi.other_ops - Harvest Metric : node_vol_iscsi_other_ops Description : Number of other block protocol operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_other_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.other_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_iscsi_read_data Description : Bytes read per second via block protocol ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_read_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.read_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_iscsi_read_latency Description : Average time for the WAFL filesystem to process iSCSI protocol read operations to the volume; not including iSCSI protocol request processing or network communication time which will also be included in client observed iSCSI protocol request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_read_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : iscsi_read_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.read_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : iscsi.read_ops - Harvest Metric : node_vol_iscsi_read_ops Description : Number of block protocol read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_read_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.read_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_iscsi_write_data Description : Bytes written per second via block protocol ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_write_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.write_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_iscsi_write_latency Description : Average time for the WAFL filesystem to process iSCSI protocol write operations to the volume; not including iSCSI protocol request processing or network communication time which will also be included in client observed iSCSI request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_write_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : iscsi_write_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.write_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : iscsi.write_ops - Harvest Metric : node_vol_iscsi_write_ops Description : Number of block protocol write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_write_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.write_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_nfs_other_latency Description : Average time for the WAFL filesystem to process other NFS operations to the volume; not including NFS protocol request processing or network communication time which will also be included in client observed NFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_other_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : nfs_other_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.other_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : nfs.other_ops - Harvest Metric : node_vol_nfs_other_ops Description : Number of other NFS operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_other_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.other_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_nfs_read_data Description : Bytes read per second via NFS ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_read_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.read_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_nfs_read_latency Description : Average time for the WAFL filesystem to process NFS protocol read requests to the volume; not including NFS protocol request processing or network communication time which will also be included in client observed NFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_read_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : nfs_read_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.read_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : nfs.read_ops - Harvest Metric : node_vol_nfs_read_ops Description : Number of NFS read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_read_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.read_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_nfs_write_data Description : Bytes written per second via NFS ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_write_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.write_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_nfs_write_latency Description : Average time for the WAFL filesystem to process NFS protocol write requests to the volume; not including NFS protocol request processing or network communication time, which will also be included in client observed NFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_write_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : nfs_write_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.write_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : nfs.write_ops - Harvest Metric : node_vol_nfs_write_ops Description : Number of NFS write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_write_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.write_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_read_latency Description : Average latency in microseconds for the WAFL filesystem to process read request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:node metric : read_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : read_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : total_read_ops - Harvest Metric : node_vol_write_latency Description : Average latency in microseconds for the WAFL filesystem to process write request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:node metric : write_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : write_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : total_write_ops - Harvest Metric : nvme_lif_avg_latency Description : Average latency for NVMF operations ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : avg_latency template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : microsec type : average base : total_ops REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : average_latency template : conf/restperf/9.12.0/nvmf_lif.yaml unit : microsec type : average base : total_ops - Harvest Metric : nvme_lif_avg_other_latency Description : Average latency for operations other than read, write, compare or compare-and-write. ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : avg_other_latency template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : average_other_latency template : conf/restperf/9.12.0/nvmf_lif.yaml unit : microsec type : average base : other_ops - Harvest Metric : nvme_lif_avg_read_latency Description : Average latency for read operations ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : avg_read_latency template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : average_read_latency template : conf/restperf/9.12.0/nvmf_lif.yaml unit : microsec type : average base : read_ops - Harvest Metric : nvme_lif_avg_write_latency Description : Average latency for write operations ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : avg_write_latency template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : average_write_latency template : conf/restperf/9.12.0/nvmf_lif.yaml unit : microsec type : average base : write_ops - Harvest Metric : nvme_lif_other_ops Description : Number of operations that are not read, write, compare or compare-and-write. ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : other_ops template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : other_ops template : conf/restperf/9.12.0/nvmf_lif.yaml unit : per_sec type : rate - Harvest Metric : nvme_lif_read_data Description : Amount of data read from the storage system ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : read_data template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : read_data template : conf/restperf/9.12.0/nvmf_lif.yaml unit : b_per_sec type : rate - Harvest Metric : nvme_lif_read_ops Description : Number of read operations ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : read_ops template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : read_ops template : conf/restperf/9.12.0/nvmf_lif.yaml unit : per_sec type : rate - Harvest Metric : nvme_lif_total_ops Description : Total number of operations. ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : total_ops template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : total_ops template : conf/restperf/9.12.0/nvmf_lif.yaml unit : per_sec type : rate - Harvest Metric : nvme_lif_write_data Description : Amount of data written to the storage system ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : write_data template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : write_data template : conf/restperf/9.12.0/nvmf_lif.yaml unit : b_per_sec type : rate - Harvest Metric : nvme_lif_write_ops Description : Number of write operations ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : write_ops template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : write_ops template : conf/restperf/9.12.0/nvmf_lif.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_abort_multipart_upload_failed Description : Number of failed Abort Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : abort_multipart_upload_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_abort_multipart_upload_failed_client_close Description : Number of times Abort Multipart Upload operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : abort_multipart_upload_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_abort_multipart_upload_latency Description : Average latency for Abort Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : abort_multipart_upload_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : abort_multipart_upload_latency_base - Harvest Metric : ontaps3_abort_multipart_upload_rate Description : Number of Abort Multipart Upload operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : abort_multipart_upload_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_abort_multipart_upload_total Description : Number of Abort Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : abort_multipart_upload_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_allow_access Description : Number of times access was allowed. ZAPI : endpoint : perf-object-get-instances object_store_server metric : allow_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_anonymous_access Description : Number of times anonymous access was allowed. ZAPI : endpoint : perf-object-get-instances object_store_server metric : anonymous_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_anonymous_deny_access Description : Number of times anonymous access was denied. ZAPI : endpoint : perf-object-get-instances object_store_server metric : anonymous_deny_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_authentication_failures Description : Number of authentication failures. ZAPI : endpoint : perf-object-get-instances object_store_server metric : authentication_failures template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_chunked_upload_reqs Description : Total number of object store server chunked object upload requests ZAPI : endpoint : perf-object-get-instances object_store_server metric : chunked_upload_reqs template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_complete_multipart_upload_failed Description : Number of failed Complete Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : complete_multipart_upload_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_complete_multipart_upload_failed_client_close Description : Number of times Complete Multipart Upload operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : complete_multipart_upload_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_complete_multipart_upload_latency Description : Average latency for Complete Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : complete_multipart_upload_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : complete_multipart_upload_latency_base - Harvest Metric : ontaps3_complete_multipart_upload_rate Description : Number of Complete Multipart Upload operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : complete_multipart_upload_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_complete_multipart_upload_total Description : Number of Complete Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : complete_multipart_upload_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_connected_connections Description : Number of object store server connections currently established ZAPI : endpoint : perf-object-get-instances object_store_server metric : connected_connections template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : raw - Harvest Metric : ontaps3_connections Description : Total number of object store server connections. ZAPI : endpoint : perf-object-get-instances object_store_server metric : connections template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_create_bucket_failed Description : Number of failed Create Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : create_bucket_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_create_bucket_failed_client_close Description : Number of times Create Bucket operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : create_bucket_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_create_bucket_latency Description : Average latency for Create Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : create_bucket_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average,no-zero-values base : create_bucket_latency_base - Harvest Metric : ontaps3_create_bucket_rate Description : Number of Create Bucket operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : create_bucket_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : ontaps3_create_bucket_total Description : Number of Create Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : create_bucket_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_default_deny_access Description : Number of times access was denied by default and not through any policy statement. ZAPI : endpoint : perf-object-get-instances object_store_server metric : default_deny_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_bucket_failed Description : Number of failed Delete Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_bucket_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_bucket_failed_client_close Description : Number of times Delete Bucket operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_bucket_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_bucket_latency Description : Average latency for Delete Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_bucket_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average,no-zero-values base : delete_bucket_latency_base - Harvest Metric : ontaps3_delete_bucket_rate Description : Number of Delete Bucket operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_bucket_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : ontaps3_delete_bucket_total Description : Number of Delete Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_bucket_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_object_failed Description : Number of failed DELETE object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_delete_object_failed_client_close Description : Number of times DELETE object operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_delete_object_latency Description : Average latency for DELETE object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : delete_object_latency_base - Harvest Metric : ontaps3_delete_object_rate Description : Number of DELETE object operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_delete_object_tagging_failed Description : Number of failed DELETE object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_tagging_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_object_tagging_failed_client_close Description : Number of times DELETE object tagging operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_tagging_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_object_tagging_latency Description : Average latency for DELETE object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_tagging_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average,no-zero-values base : delete_object_tagging_latency_base - Harvest Metric : ontaps3_delete_object_tagging_rate Description : Number of DELETE object tagging operations per sec. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_tagging_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : ontaps3_delete_object_tagging_total Description : Number of DELETE object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_tagging_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_object_total Description : Number of DELETE object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_explicit_deny_access Description : Number of times access was denied explicitly by a policy statement. ZAPI : endpoint : perf-object-get-instances object_store_server metric : explicit_deny_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_get_bucket_acl_failed Description : Number of failed GET Bucket ACL operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_bucket_acl_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_bucket_acl_total Description : Number of GET Bucket ACL operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_bucket_acl_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_bucket_versioning_failed Description : Number of failed Get Bucket Versioning operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_bucket_versioning_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_bucket_versioning_total Description : Number of Get Bucket Versioning operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_bucket_versioning_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_data Description : Rate of GET object data transfers per second ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_data template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : b_per_sec type : rate - Harvest Metric : ontaps3_get_object_acl_failed Description : Number of failed GET Object ACL operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_acl_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_acl_total Description : Number of GET Object ACL operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_acl_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_failed Description : Number of failed GET object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_failed_client_close Description : Number of times GET object operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_lastbyte_latency Description : Average last-byte latency for GET object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_lastbyte_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : get_object_lastbyte_latency_base - Harvest Metric : ontaps3_get_object_latency Description : Average first-byte latency for GET object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : get_object_latency_base - Harvest Metric : ontaps3_get_object_rate Description : Number of GET object operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_get_object_tagging_failed Description : Number of failed GET object tagging operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_tagging_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_tagging_failed_client_close Description : Number of times GET object tagging operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_tagging_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_tagging_latency Description : Average latency for GET object tagging operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_tagging_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : get_object_tagging_latency_base - Harvest Metric : ontaps3_get_object_tagging_rate Description : Number of GET object tagging operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_tagging_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_get_object_tagging_total Description : Number of GET object tagging operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_tagging_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_total Description : Number of GET object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_group_policy_evaluated Description : Number of times group policies were evaluated. ZAPI : endpoint : perf-object-get-instances object_store_server metric : group_policy_evaluated template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_head_bucket_failed Description : Number of failed HEAD bucket operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_bucket_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_head_bucket_failed_client_close Description : Number of times HEAD bucket operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_bucket_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_head_bucket_latency Description : Average latency for HEAD bucket operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_bucket_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : head_bucket_latency_base - Harvest Metric : ontaps3_head_bucket_rate Description : Number of HEAD bucket operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_bucket_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_head_bucket_total Description : Number of HEAD bucket operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_bucket_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_head_object_failed Description : Number of failed HEAD Object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_object_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_head_object_failed_client_close Description : Number of times HEAD object operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_object_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_head_object_latency Description : Average latency for HEAD object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_object_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : head_object_latency_base - Harvest Metric : ontaps3_head_object_rate Description : Number of HEAD Object operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_object_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_head_object_total Description : Number of HEAD Object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_object_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_initiate_multipart_upload_failed Description : Number of failed Initiate Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : initiate_multipart_upload_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_initiate_multipart_upload_failed_client_close Description : Number of times Initiate Multipart Upload operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : initiate_multipart_upload_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_initiate_multipart_upload_latency Description : Average latency for Initiate Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : initiate_multipart_upload_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : initiate_multipart_upload_latency_base - Harvest Metric : ontaps3_initiate_multipart_upload_rate Description : Number of Initiate Multipart Upload operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : initiate_multipart_upload_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_initiate_multipart_upload_total Description : Number of Initiate Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : initiate_multipart_upload_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_input_flow_control_entry Description : Number of times input flow control was entered. ZAPI : endpoint : perf-object-get-instances object_store_server metric : input_flow_control_entry template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_input_flow_control_exit Description : Number of times input flow control was exited. ZAPI : endpoint : perf-object-get-instances object_store_server metric : input_flow_control_exit template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_buckets_failed Description : Number of failed LIST Buckets operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_buckets_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_buckets_failed_client_close Description : Number of times LIST Bucket operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_buckets_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_buckets_latency Description : Average latency for LIST Buckets operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_buckets_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : head_object_latency_base - Harvest Metric : ontaps3_list_buckets_rate Description : Number of LIST Buckets operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_buckets_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_list_buckets_total Description : Number of LIST Buckets operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_buckets_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_object_versions_failed Description : Number of failed LIST object versions operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_object_versions_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_object_versions_failed_client_close Description : Number of times LIST object versions operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_object_versions_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_list_object_versions_latency Description : Average latency for LIST Object versions operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_object_versions_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average,no-zero-values base : list_object_versions_latency_base - Harvest Metric : ontaps3_list_object_versions_rate Description : Number of LIST Object Versions operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_object_versions_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : ontaps3_list_object_versions_total Description : Number of LIST Object Versions operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_object_versions_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_objects_failed Description : Number of failed LIST objects operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_objects_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_objects_failed_client_close Description : Number of times LIST objects operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_objects_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_objects_latency Description : Average latency for LIST Objects operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_objects_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : list_objects_latency_base - Harvest Metric : ontaps3_list_objects_rate Description : Number of LIST Objects operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_objects_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_list_objects_total Description : Number of LIST Objects operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_objects_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_uploads_failed Description : Number of failed LIST Uploads operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_uploads_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_uploads_failed_client_close Description : Number of times LIST Uploads operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_uploads_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_uploads_latency Description : Average latency for LIST Uploads operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_uploads_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : list_uploads_latency_base - Harvest Metric : ontaps3_list_uploads_rate Description : Number of LIST Uploads operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_uploads_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_list_uploads_total Description : Number of LIST Uploads operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_uploads_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_logical_used_size REST : endpoint : api/protocols/s3/buckets metric : logical_used_size template : conf/rest/9.7.0/ontap_s3.yaml - Harvest Metric : ontaps3_max_cmds_per_connection Description : Maximum commands pipelined at any instance on a connection. ZAPI : endpoint : perf-object-get-instances object_store_server metric : max_cmds_per_connection template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_max_connected_connections Description : Maximum number of object store server connections established at one time ZAPI : endpoint : perf-object-get-instances object_store_server metric : max_connected_connections template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : raw - Harvest Metric : ontaps3_max_requests_outstanding Description : Maximum number of object store server requests in process at one time ZAPI : endpoint : perf-object-get-instances object_store_server metric : max_requests_outstanding template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : raw - Harvest Metric : ontaps3_multi_delete_reqs Description : Total number of object store server multiple object delete requests ZAPI : endpoint : perf-object-get-instances object_store_server metric : multi_delete_reqs template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_output_flow_control_entry Description : Number of output flow control was entered. ZAPI : endpoint : perf-object-get-instances object_store_server metric : output_flow_control_entry template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_output_flow_control_exit Description : Number of times output flow control was exited. ZAPI : endpoint : perf-object-get-instances object_store_server metric : output_flow_control_exit template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_presigned_url_reqs Description : Total number of presigned object store server URL requests. ZAPI : endpoint : perf-object-get-instances object_store_server metric : presigned_url_reqs template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_put_bucket_versioning_failed Description : Number of failed Put Bucket Versioning operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_bucket_versioning_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_put_bucket_versioning_total Description : Number of Put Bucket Versioning operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_bucket_versioning_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_put_data Description : Rate of PUT object data transfers per second ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_data template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : b_per_sec type : rate - Harvest Metric : ontaps3_put_object_failed Description : Number of failed PUT object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_put_object_failed_client_close Description : Number of times PUT object operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_put_object_latency Description : Average latency for PUT object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : put_object_latency_base - Harvest Metric : ontaps3_put_object_rate Description : Number of PUT object operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_put_object_tagging_failed Description : Number of failed PUT object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_tagging_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_put_object_tagging_failed_client_close Description : Number of times PUT object tagging operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_tagging_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_put_object_tagging_latency Description : Average latency for PUT object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_tagging_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average,no-zero-values base : put_object_tagging_latency_base - Harvest Metric : ontaps3_put_object_tagging_rate Description : Number of PUT object tagging operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_tagging_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : ontaps3_put_object_tagging_total Description : Number of PUT object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_tagging_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_put_object_total Description : Number of PUT object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_request_parse_errors Description : Number of request parser errors due to malformed requests. ZAPI : endpoint : perf-object-get-instances object_store_server metric : request_parse_errors template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_requests Description : Total number of object store server requests ZAPI : endpoint : perf-object-get-instances object_store_server metric : requests template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_requests_outstanding Description : Number of object store server requests in process ZAPI : endpoint : perf-object-get-instances object_store_server metric : requests_outstanding template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : raw - Harvest Metric : ontaps3_root_user_access Description : Number of times access was done by root user. ZAPI : endpoint : perf-object-get-instances object_store_server metric : root_user_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_server_connection_close Description : Number of connection closes triggered by server due to fatal errors. ZAPI : endpoint : perf-object-get-instances object_store_server metric : server_connection_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_signature_v2_reqs Description : Total number of object store server signature V2 requests ZAPI : endpoint : perf-object-get-instances object_store_server metric : signature_v2_reqs template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_signature_v4_reqs Description : Total number of object store server signature V4 requests ZAPI : endpoint : perf-object-get-instances object_store_server metric : signature_v4_reqs template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_size REST : endpoint : api/protocols/s3/buckets metric : size template : conf/rest/9.7.0/ontap_s3.yaml - Harvest Metric : ontaps3_tagging Description : Number of requests with tagging specified. ZAPI : endpoint : perf-object-get-instances object_store_server metric : tagging template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_upload_part_failed Description : Number of failed Upload Part operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : upload_part_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_upload_part_failed_client_close Description : Number of times Upload Part operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : upload_part_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_upload_part_latency Description : Average latency for Upload Part operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : upload_part_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : upload_part_latency_base - Harvest Metric : ontaps3_upload_part_rate Description : Number of Upload Part operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : upload_part_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_upload_part_total Description : Number of Upload Part operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : upload_part_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : path_read_data Description : The average read throughput in kilobytes per second read from the indicated target port by the controller. ZAPI : endpoint : perf-object-get-instances path metric : read_data template : conf/zapiperf/cdot/9.8.0/path.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : read_data template : conf/restperf/9.12.0/path.yaml unit : kb_per_sec type : rate - Harvest Metric : path_read_iops Description : The number of I/O read operations sent from the initiator port to the indicated target port. ZAPI : endpoint : perf-object-get-instances path metric : read_iops template : conf/zapiperf/cdot/9.8.0/path.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : read_iops template : conf/restperf/9.12.0/path.yaml unit : per_sec type : rate - Harvest Metric : path_read_latency Description : The average latency of I/O read operations sent from this controller to the indicated target port. ZAPI : endpoint : perf-object-get-instances path metric : read_latency template : conf/zapiperf/cdot/9.8.0/path.yaml unit : microsec type : average base : read_iops REST : endpoint : api/cluster/counter/tables/path metric : read_latency template : conf/restperf/9.12.0/path.yaml unit : microsec type : average base : read_iops - Harvest Metric : path_total_data Description : The average throughput in kilobytes per second read and written from/to the indicated target port by the controller. ZAPI : endpoint : perf-object-get-instances path metric : total_data template : conf/zapiperf/cdot/9.8.0/path.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : total_data template : conf/restperf/9.12.0/path.yaml unit : kb_per_sec type : rate - Harvest Metric : path_total_iops Description : The number of total read/write I/O operations sent from the initiator port to the indicated target port. ZAPI : endpoint : perf-object-get-instances path metric : total_iops template : conf/zapiperf/cdot/9.8.0/path.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : total_iops template : conf/restperf/9.12.0/path.yaml unit : per_sec type : rate - Harvest Metric : path_write_data Description : The average write throughput in kilobytes per second written to the indicated target port by the controller. ZAPI : endpoint : perf-object-get-instances path metric : write_data template : conf/zapiperf/cdot/9.8.0/path.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : write_data template : conf/restperf/9.12.0/path.yaml unit : kb_per_sec type : rate - Harvest Metric : path_write_iops Description : The number of I/O write operations sent from the initiator port to the indicated target port. ZAPI : endpoint : perf-object-get-instances path metric : write_iops template : conf/zapiperf/cdot/9.8.0/path.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : write_iops template : conf/restperf/9.12.0/path.yaml unit : per_sec type : rate - Harvest Metric : path_write_latency Description : The average latency of I/O write operations sent from this controller to the indicated target port. ZAPI : endpoint : perf-object-get-instances path metric : write_latency template : conf/zapiperf/cdot/9.8.0/path.yaml unit : microsec type : average base : write_iops REST : endpoint : api/cluster/counter/tables/path metric : write_latency template : conf/restperf/9.12.0/path.yaml unit : microsec type : average base : write_iops - Harvest Metric : qos_concurrency Description : This is the average number of concurrent requests for the workload. ZAPI : endpoint : perf-object-get-instances workload metric : concurrency template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : none type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : concurrency template : conf/restperf/9.12.0/workload.yaml unit : none type : rate - Harvest Metric : qos_detail_service_time Description : The workload's average service time per visit to the service center. ZAPI : endpoint : perf-object-get-instances workload_detail metric : service_time template : conf/zapiperf/cdot/9.8.0/workload_detail.yaml unit : microsec type : average,no-zero-values base : visits REST : endpoint : api/cluster/counter/tables/qos_detail metric : service_time template : conf/restperf/9.12.0/workload_detail.yaml unit : microsec type : average base : visits - Harvest Metric : qos_detail_visits Description : The number of visits that the workload made to the service center; measured in visits per second. ZAPI : endpoint : perf-object-get-instances workload_detail metric : visits template : conf/zapiperf/cdot/9.8.0/workload_detail.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_detail metric : visits template : conf/restperf/9.12.0/workload_detail.yaml unit : per_sec type : rate - Harvest Metric : qos_detail_volume_service_time Description : The workload's average service time per visit to the service center. ZAPI : endpoint : perf-object-get-instances workload_detail_volume metric : service_time template : conf/zapiperf/cdot/9.8.0/workload_detail_volume.yaml unit : microsec type : average,no-zero-values base : visits REST : endpoint : api/cluster/counter/tables/qos_detail_volume metric : service_time template : conf/restperf/9.12.0/workload_detail_volume.yaml unit : microsec type : average base : visits - Harvest Metric : qos_detail_volume_visits Description : The number of visits that the workload made to the service center; measured in visits per second. ZAPI : endpoint : perf-object-get-instances workload_detail_volume metric : visits template : conf/zapiperf/cdot/9.8.0/workload_detail_volume.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_detail_volume metric : visits template : conf/restperf/9.12.0/workload_detail_volume.yaml unit : per_sec type : rate - Harvest Metric : qos_detail_volume_wait_time Description : The workload's average wait time per visit to the service center. ZAPI : endpoint : perf-object-get-instances workload_detail_volume metric : wait_time template : conf/zapiperf/cdot/9.8.0/workload_detail_volume.yaml unit : microsec type : average,no-zero-values base : visits REST : endpoint : api/cluster/counter/tables/qos_detail_volume metric : wait_time template : conf/restperf/9.12.0/workload_detail_volume.yaml unit : microsec type : average base : visits - Harvest Metric : qos_detail_wait_time Description : The workload's average wait time per visit to the service center. ZAPI : endpoint : perf-object-get-instances workload_detail metric : wait_time template : conf/zapiperf/cdot/9.8.0/workload_detail.yaml unit : microsec type : average,no-zero-values base : visits REST : endpoint : api/cluster/counter/tables/qos_detail metric : wait_time template : conf/restperf/9.12.0/workload_detail.yaml unit : microsec type : average base : visits - Harvest Metric : qos_latency Description : This is the average response time for requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : latency template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : microsec type : average,no-zero-values base : ops REST : endpoint : api/cluster/counter/tables/qos metric : latency template : conf/restperf/9.12.0/workload.yaml unit : microsec type : average base : ops - Harvest Metric : qos_ops Description : Workload operations executed per second. ZAPI : endpoint : perf-object-get-instances workload metric : ops template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : ops template : conf/restperf/9.12.0/workload.yaml unit : per_sec type : rate - Harvest Metric : qos_read_data Description : This is the amount of data read per second from the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : read_data template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : read_data template : conf/restperf/9.12.0/workload.yaml unit : b_per_sec type : rate - Harvest Metric : qos_read_io_type Description : This is the percentage of read requests served from various components (such as buffer cache, ext_cache, disk, etc.). ZAPI : endpoint : perf-object-get-instances workload metric : read_io_type template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : percent type : percent base : read_io_type_base REST : endpoint : api/cluster/counter/tables/qos metric : read_io_type_percent template : conf/restperf/9.12.0/workload.yaml unit : percent type : percent base : read_io_type_base - Harvest Metric : qos_read_latency Description : This is the average response time for read requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : read_latency template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : microsec type : average,no-zero-values base : read_ops REST : endpoint : api/cluster/counter/tables/qos metric : read_latency template : conf/restperf/9.12.0/workload.yaml unit : microsec type : average base : read_ops - Harvest Metric : qos_read_ops Description : This is the rate of this workload's read operations that completed during the measurement interval. ZAPI : endpoint : perf-object-get-instances workload metric : read_ops template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : read_ops template : conf/restperf/9.12.0/workload.yaml unit : per_sec type : rate - Harvest Metric : qos_sequential_reads Description : This is the percentage of reads, performed on behalf of the workload, that were sequential. ZAPI : endpoint : perf-object-get-instances workload metric : sequential_reads template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : percent type : percent,no-zero-values base : sequential_reads_base REST : endpoint : api/cluster/counter/tables/qos metric : sequential_reads_percent template : conf/restperf/9.12.0/workload.yaml unit : percent type : percent base : sequential_reads_base - Harvest Metric : qos_sequential_writes Description : This is the percentage of writes, performed on behalf of the workload, that were sequential. This counter is only available on platforms with more than 4GB of NVRAM. ZAPI : endpoint : perf-object-get-instances workload metric : sequential_writes template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : percent type : percent,no-zero-values base : sequential_writes_base REST : endpoint : api/cluster/counter/tables/qos metric : sequential_writes_percent template : conf/restperf/9.12.0/workload.yaml unit : percent type : percent base : sequential_writes_base - Harvest Metric : qos_total_data Description : This is the total amount of data read/written per second from/to the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : total_data template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : total_data template : conf/restperf/9.12.0/workload.yaml unit : b_per_sec type : rate - Harvest Metric : qos_volume_latency Description : This is the average response time for requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : latency template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : microsec type : average,no-zero-values base : ops REST : endpoint : api/cluster/counter/tables/qos_volume metric : latency template : conf/restperf/9.12.0/workload_volume.yaml unit : microsec type : average base : ops - Harvest Metric : qos_volume_ops Description : This field is the workload's rate of operations that completed during the measurement interval; measured per second. ZAPI : endpoint : perf-object-get-instances workload_volume metric : ops template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : ops template : conf/restperf/9.12.0/workload_volume.yaml unit : per_sec type : rate - Harvest Metric : qos_volume_read_data Description : This is the amount of data read per second from the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : read_data template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : read_data template : conf/restperf/9.12.0/workload_volume.yaml unit : b_per_sec type : rate - Harvest Metric : qos_volume_read_io_type Description : This is the percentage of read requests served from various components (such as buffer cache, ext_cache, disk, etc.). ZAPI : endpoint : perf-object-get-instances workload_volume metric : read_io_type template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : percent type : percent base : read_io_type_base REST : endpoint : api/cluster/counter/tables/qos_volume metric : read_io_type_percent template : conf/restperf/9.12.0/workload_volume.yaml unit : percent type : percent base : read_io_type_base - Harvest Metric : qos_volume_read_latency Description : This is the average response time for read requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : read_latency template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : microsec type : average,no-zero-values base : read_ops REST : endpoint : api/cluster/counter/tables/qos_volume metric : read_latency template : conf/restperf/9.12.0/workload_volume.yaml unit : microsec type : average base : read_ops - Harvest Metric : qos_volume_read_ops Description : This is the rate of this workload's read operations that completed during the measurement interval. ZAPI : endpoint : perf-object-get-instances workload_volume metric : read_ops template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : read_ops template : conf/restperf/9.12.0/workload_volume.yaml unit : per_sec type : rate - Harvest Metric : qos_volume_sequential_reads Description : This is the percentage of reads, performed on behalf of the workload, that were sequential. ZAPI : endpoint : perf-object-get-instances workload_volume metric : sequential_reads template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : percent type : percent,no-zero-values base : sequential_reads_base REST : endpoint : api/cluster/counter/tables/qos_volume metric : sequential_reads_percent template : conf/restperf/9.12.0/workload_volume.yaml unit : percent type : percent base : sequential_reads_base - Harvest Metric : qos_volume_sequential_writes Description : This is the percentage of writes, performed on behalf of the workload, that were sequential. This counter is only available on platforms with more than 4GB of NVRAM. ZAPI : endpoint : perf-object-get-instances workload_volume metric : sequential_writes template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : percent type : percent,no-zero-values base : sequential_writes_base REST : endpoint : api/cluster/counter/tables/qos_volume metric : sequential_writes_percent template : conf/restperf/9.12.0/workload_volume.yaml unit : percent type : percent base : sequential_writes_base - Harvest Metric : qos_volume_total_data Description : This is the total amount of data read/written per second from/to the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : total_data template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : total_data template : conf/restperf/9.12.0/workload_volume.yaml unit : b_per_sec type : rate - Harvest Metric : qos_volume_write_data Description : This is the amount of data written per second to the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : write_data template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : write_data template : conf/restperf/9.12.0/workload_volume.yaml unit : b_per_sec type : rate - Harvest Metric : qos_volume_write_latency Description : This is the average response time for write requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : write_latency template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : microsec type : average,no-zero-values base : write_ops REST : endpoint : api/cluster/counter/tables/qos_volume metric : write_latency template : conf/restperf/9.12.0/workload_volume.yaml unit : microsec type : average base : write_ops - Harvest Metric : qos_volume_write_ops Description : This is the workload's write operations that completed during the measurement interval; measured per second. ZAPI : endpoint : perf-object-get-instances workload_volume metric : write_ops template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : write_ops template : conf/restperf/9.12.0/workload_volume.yaml unit : per_sec type : rate - Harvest Metric : qos_write_data Description : This is the amount of data written per second to the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : write_data template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : write_data template : conf/restperf/9.12.0/workload.yaml unit : b_per_sec type : rate - Harvest Metric : qos_write_latency Description : This is the average response time for write requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : write_latency template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : microsec type : average,no-zero-values base : write_ops REST : endpoint : api/cluster/counter/tables/qos metric : write_latency template : conf/restperf/9.12.0/workload.yaml unit : microsec type : average base : write_ops - Harvest Metric : qos_write_ops Description : This is the workload's write operations that completed during the measurement interval; measured per second. ZAPI : endpoint : perf-object-get-instances workload metric : write_ops template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : write_ops template : conf/restperf/9.12.0/workload.yaml unit : per_sec type : rate - Harvest Metric : qtree_cifs_ops Description : Number of CIFS operations per second to the qtree ZAPI : endpoint : perf-object-get-instances qtree metric : cifs_ops template : conf/zapiperf/cdot/9.8.0/qtree.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/qtree metric : cifs_ops template : conf/restperf/9.12.0/qtree.yaml unit : per_sec type : rate - Harvest Metric : qtree_id Description : The identifier for the qtree, unique within the qtree's volume. REST : endpoint : api/storage/qtrees metric : id template : conf/rest/9.12.0/qtree.yaml - Harvest Metric : qtree_internal_ops Description : Number of internal operations generated by activites such as snapmirror and backup per second to the qtree ZAPI : endpoint : perf-object-get-instances qtree metric : internal_ops template : conf/zapiperf/cdot/9.8.0/qtree.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/qtree metric : internal_ops template : conf/restperf/9.12.0/qtree.yaml unit : per_sec type : rate - Harvest Metric : qtree_nfs_ops Description : Number of NFS operations per second to the qtree ZAPI : endpoint : perf-object-get-instances qtree metric : nfs_ops template : conf/zapiperf/cdot/9.8.0/qtree.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/qtree metric : nfs_ops template : conf/restperf/9.12.0/qtree.yaml unit : per_sec type : rate - Harvest Metric : qtree_total_ops Description : Summation of NFS ops, CIFS ops, CSS ops and internal ops ZAPI : endpoint : perf-object-get-instances qtree metric : total_ops template : conf/zapiperf/cdot/9.8.0/qtree.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/qtree metric : total_ops template : conf/restperf/9.12.0/qtree.yaml unit : per_sec type : rate - Harvest Metric : security_audit_destination_port ZAPI : endpoint : cluster-log-forward-get-iter metric : cluster-log-forward-info.port template : conf/zapi/cdot/9.8.0/security_audit_dest.yaml - Harvest Metric : security_certificate_expiry_time Description : Certificate expiration time. Can be provided on POST if creating self-signed certificate. The expiration time range is between 1 day to 10 years. ZAPI : endpoint : security-certificate-get-iter metric : certificate-info.expiration-date template : conf/zapi/cdot/9.8.0/security_certificate.yaml REST : endpoint : api/security/certificates metric : expiry_time template : conf/rest/9.12.0/security_certificate.yaml - Harvest Metric : security_ssh_max_instances REST : endpoint : api/security/ssh metric : max_instances template : conf/rest/9.12.0/security_ssh.yaml - Harvest Metric : shelf_disk_count ZAPI : endpoint : storage-shelf-info-get-iter metric : storage-shelf-info.disk-count template : conf/zapi/cdot/9.8.0/shelf.yaml REST : endpoint : api/storage/shelves metric : disk_count template : conf/rest/9.12.0/shelf.yaml - Harvest Metric : snapmirror_break_failed_count Description : The number of failed SnapMirror break operations for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.break-failed-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : break_failed_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_break_successful_count Description : The number of successful SnapMirror break operations for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.break-successful-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : break_successful_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_lag_time Description : Amount of time since the last snapmirror transfer in seconds ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.lag-time template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : lag_time template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_last_transfer_duration Description : Duration of the last SnapMirror transfer in seconds ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.last-transfer-duration template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : last_transfer_duration template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_last_transfer_end_timestamp Description : The Timestamp of the end of the last transfer ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.last-transfer-end-timestamp template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : last_transfer_end_timestamp template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_last_transfer_size Description : Size in kilobytes (1024 bytes) of the last transfer ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.last-transfer-size template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : last_transfer_size template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_newest_snapshot_timestamp Description : The timestamp of the newest Snapshot copy on the destination volume ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.newest-snapshot-timestamp template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : newest_snapshot_timestamp template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_resync_failed_count Description : The number of failed SnapMirror resync operations for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.resync-failed-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : resync_failed_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_resync_successful_count Description : The number of successful SnapMirror resync operations for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.resync-successful-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : resync_successful_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_total_transfer_bytes Description : Cumulative bytes transferred for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.total-transfer-bytes template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : total_transfer_bytes template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_total_transfer_time_secs Description : Cumulative total transfer time in seconds for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.total-transfer-time-secs template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : total_transfer_time_secs template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_update_failed_count Description : The number of successful SnapMirror update operations for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.update-failed-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : update_failed_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_update_successful_count Description : Number of Successful Updates ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.update-successful-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : update_successful_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapshot_policy_total_schedules Description : Total Number of Schedules in this Policy ZAPI : endpoint : snapshot-policy-get-iter metric : snapshot-policy-info.total-schedules template : conf/zapi/cdot/9.8.0/snapshotPolicy.yaml REST : endpoint : api/private/cli/snapshot/policy metric : total_schedules template : conf/rest/9.12.0/snapshotPolicy.yaml - Harvest Metric : svm_cifs_connections Description : Number of connections ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : connections template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs metric : connections template : conf/restperf/9.12.0/cifs_svm.yaml unit : none type : raw - Harvest Metric : svm_cifs_established_sessions Description : Number of established SMB and SMB2 sessions ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : established_sessions template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs metric : established_sessions template : conf/restperf/9.12.0/cifs_svm.yaml unit : none type : raw - Harvest Metric : svm_cifs_latency Description : Average latency for CIFS operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_latency template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : microsec type : average base : cifs_latency_base REST : endpoint : api/cluster/counter/tables/svm_cifs metric : latency template : conf/restperf/9.12.0/cifs_svm.yaml unit : microsec type : average base : latency_base - Harvest Metric : svm_cifs_op_count Description : Array of select CIFS operation counts ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_op_count template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs metric : op_count template : conf/restperf/9.12.0/cifs_svm.yaml unit : none type : rate - Harvest Metric : svm_cifs_open_files Description : Number of open files over SMB and SMB2 ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : open_files template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs metric : open_files template : conf/restperf/9.12.0/cifs_svm.yaml unit : none type : raw - Harvest Metric : svm_cifs_ops Description : Total number of CIFS operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_ops template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs metric : total_ops template : conf/restperf/9.12.0/cifs_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_cifs_read_latency Description : Average latency for CIFS read operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_read_latency template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : microsec type : average base : cifs_read_ops REST : endpoint : api/cluster/counter/tables/svm_cifs metric : average_read_latency template : conf/restperf/9.12.0/cifs_svm.yaml unit : microsec type : average base : total_read_ops - Harvest Metric : svm_cifs_read_ops Description : Total number of CIFS read operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_read_ops template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs metric : total_read_ops template : conf/restperf/9.12.0/cifs_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_cifs_signed_sessions Description : Number of signed SMB and SMB2 sessions. ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : signed_sessions template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs metric : signed_sessions template : conf/restperf/9.12.0/cifs_svm.yaml unit : none type : raw - Harvest Metric : svm_cifs_write_latency Description : Average latency for CIFS write operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_write_latency template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : microsec type : average base : cifs_write_ops REST : endpoint : api/cluster/counter/tables/svm_cifs metric : average_write_latency template : conf/restperf/9.12.0/cifs_svm.yaml unit : microsec type : average base : total_write_ops - Harvest Metric : svm_cifs_write_ops Description : Total number of CIFS write operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_write_ops template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs metric : total_write_ops template : conf/restperf/9.12.0/cifs_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_access_avg_latency Description : Average latency of NFSv4.2 ACCESS operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : access_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : access_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : access.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : access.total - Harvest Metric : svm_nfs_access_total Description : Total number of NFSv4.2 ACCESS operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : access_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : access.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_backchannel_ctl_avg_latency Description : Average latency of NFSv4.2 BACKCHANNEL_CTL operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : backchannel_ctl_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : backchannel_ctl_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : backchannel_ctl.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : backchannel_ctl.total - Harvest Metric : svm_nfs_backchannel_ctl_total Description : Total number of NFSv4.2 BACKCHANNEL_CTL operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : backchannel_ctl_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : backchannel_ctl.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_bind_conn_to_session_avg_latency Description : Average latency of NFSv4.2 BIND_CONN_TO_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : bind_conn_to_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : bind_conn_to_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : bind_conn_to_session.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : bind_conn_to_session.total - Harvest Metric : svm_nfs_bind_conn_to_session_total Description : Total number of NFSv4.2 BIND_CONN_TO_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : bind_conn_to_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : bind_conn_to_session.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : delta - Harvest Metric : svm_nfs_close_avg_latency Description : Average latency of NFSv4.2 CLOSE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : close_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : close_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : close.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : close.total - Harvest Metric : svm_nfs_close_total Description : Total number of NFSv4.2 CLOSE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : close_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : close.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_commit_avg_latency Description : Average latency of NFSv4.2 COMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : commit_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : commit_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : commit.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : commit.total - Harvest Metric : svm_nfs_commit_total Description : Total number of NFSv4.2 COMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : commit_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : commit.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_create_avg_latency Description : Average latency of NFSv4.2 CREATE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : create_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : create_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : create.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : create.total - Harvest Metric : svm_nfs_create_session_avg_latency Description : Average latency of NFSv4.2 CREATE_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : create_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : create_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : create_session.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : create_session.total - Harvest Metric : svm_nfs_create_session_total Description : Total number of NFSv4.2 CREATE_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : create_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : create_session.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_create_total Description : Total number of NFSv4.2 CREATE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : create_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : create.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_delegpurge_avg_latency Description : Average latency of NFSv4.2 DELEGPURGE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : delegpurge_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : delegpurge_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : delegpurge.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : delegpurge.total - Harvest Metric : svm_nfs_delegpurge_total Description : Total number of NFSv4.2 DELEGPURGE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : delegpurge_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : delegpurge.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_delegreturn_avg_latency Description : Average latency of NFSv4.2 DELEGRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : delegreturn_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : delegreturn_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : delegreturn.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : delegreturn.total - Harvest Metric : svm_nfs_delegreturn_total Description : Total number of NFSv4.2 DELEGRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : delegreturn_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : delegreturn.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_destroy_clientid_avg_latency Description : Average latency of NFSv4.2 DESTROY_CLIENTID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : destroy_clientid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : destroy_clientid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : destroy_clientid.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : destroy_clientid.total - Harvest Metric : svm_nfs_destroy_clientid_total Description : Total number of NFSv4.2 DESTROY_CLIENTID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : destroy_clientid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : destroy_clientid.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_destroy_session_avg_latency Description : Average latency of NFSv4.2 DESTROY_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : destroy_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : destroy_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : destroy_session.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : destroy_session.total - Harvest Metric : svm_nfs_destroy_session_total Description : Total number of NFSv4.2 DESTROY_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : destroy_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : destroy_session.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_exchange_id_avg_latency Description : Average latency of NFSv4.2 EXCHANGE_ID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : exchange_id_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : exchange_id_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : exchange_id.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : exchange_id.total - Harvest Metric : svm_nfs_exchange_id_total Description : Total number of NFSv4.2 EXCHANGE_ID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : exchange_id_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : exchange_id.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_free_stateid_avg_latency Description : Average latency of NFSv4.2 FREE_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : free_stateid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : free_stateid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : free_stateid.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : free_stateid.total - Harvest Metric : svm_nfs_free_stateid_total Description : Total number of NFSv4.2 FREE_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : free_stateid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : free_stateid.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_fsinfo_avg_latency Description : Average latency of FSInfo procedure requests. The counter keeps track of the average response time of FSInfo requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : fsinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : fsinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : fsinfo.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : fsinfo.total - Harvest Metric : svm_nfs_fsinfo_total Description : Total number FSInfo of procedure requests. It is the total number of FSInfo success and FSInfo error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : fsinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : fsinfo.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_fsstat_avg_latency Description : Average latency of FSStat procedure requests. The counter keeps track of the average response time of FSStat requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : fsstat_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : fsstat_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : fsstat.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : fsstat.total - Harvest Metric : svm_nfs_fsstat_total Description : Total number FSStat of procedure requests. It is the total number of FSStat success and FSStat error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : fsstat_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : fsstat.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_get_dir_delegation_avg_latency Description : Average latency of NFSv4.2 GET_DIR_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : get_dir_delegation_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : get_dir_delegation_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : get_dir_delegation.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : get_dir_delegation.total - Harvest Metric : svm_nfs_get_dir_delegation_total Description : Total number of NFSv4.2 GET_DIR_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : get_dir_delegation_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : get_dir_delegation.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_getattr_avg_latency Description : Average latency of NFSv4.2 GETATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : getattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getattr.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : getattr.total - Harvest Metric : svm_nfs_getattr_total Description : Total number of NFSv4.2 GETATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getattr.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_getdeviceinfo_avg_latency Description : Average latency of NFSv4.2 GETDEVICEINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getdeviceinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : getdeviceinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getdeviceinfo.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : getdeviceinfo.total - Harvest Metric : svm_nfs_getdeviceinfo_total Description : Total number of NFSv4.2 GETDEVICEINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getdeviceinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getdeviceinfo.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_getdevicelist_avg_latency Description : Average latency of NFSv4.2 GETDEVICELIST operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getdevicelist_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : getdevicelist_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getdevicelist.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : getdevicelist.total - Harvest Metric : svm_nfs_getdevicelist_total Description : Total number of NFSv4.2 GETDEVICELIST operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getdevicelist_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getdevicelist.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_getfh_avg_latency Description : Average latency of NFSv4.2 GETFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : getfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getfh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : getfh.total - Harvest Metric : svm_nfs_getfh_total Description : Total number of NFSv4.2 GETFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getfh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_latency Description : Average latency of nfsv42 requests. This counter keeps track of the average response time of nfsv42 requests. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : total_ops REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : total_ops - Harvest Metric : svm_nfs_layoutcommit_avg_latency Description : Average latency of NFSv4.2 LAYOUTCOMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutcommit_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : layoutcommit_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutcommit.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : layoutcommit.total - Harvest Metric : svm_nfs_layoutcommit_total Description : Total number of NFSv4.2 LAYOUTCOMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutcommit_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutcommit.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_layoutget_avg_latency Description : Average latency of NFSv4.2 LAYOUTGET operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutget_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : layoutget_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutget.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : layoutget.total - Harvest Metric : svm_nfs_layoutget_total Description : Total number of NFSv4.2 LAYOUTGET operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutget_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutget.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_layoutreturn_avg_latency Description : Average latency of NFSv4.2 LAYOUTRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutreturn_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : layoutreturn_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutreturn.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : layoutreturn.total - Harvest Metric : svm_nfs_layoutreturn_total Description : Total number of NFSv4.2 LAYOUTRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutreturn_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutreturn.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_link_avg_latency Description : Average latency of NFSv4.2 LINK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : link_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : link_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : link.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : link.total - Harvest Metric : svm_nfs_link_total Description : Total number of NFSv4.2 LINK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : link_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : link.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_lock_avg_latency Description : Average latency of NFSv4.2 LOCK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lock_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : lock_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lock.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : lock.total - Harvest Metric : svm_nfs_lock_total Description : Total number of NFSv4.2 LOCK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lock_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lock.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_lockt_avg_latency Description : Average latency of NFSv4.2 LOCKT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lockt_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : lockt_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lockt.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : lockt.total - Harvest Metric : svm_nfs_lockt_total Description : Total number of NFSv4.2 LOCKT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lockt_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lockt.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_locku_avg_latency Description : Average latency of NFSv4.2 LOCKU operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : locku_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : locku_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : locku.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : locku.total - Harvest Metric : svm_nfs_locku_total Description : Total number of NFSv4.2 LOCKU operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : locku_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : locku.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_lookup_avg_latency Description : Average latency of NFSv4.2 LOOKUP operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lookup_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : lookup_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lookup.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : lookup.total - Harvest Metric : svm_nfs_lookup_total Description : Total number of NFSv4.2 LOOKUP operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lookup_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lookup.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_lookupp_avg_latency Description : Average latency of NFSv4.2 LOOKUPP operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lookupp_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : lookupp_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lookupp.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : lookupp.total - Harvest Metric : svm_nfs_lookupp_total Description : Total number of NFSv4.2 LOOKUPP operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lookupp_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lookupp.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_mkdir_avg_latency Description : Average latency of MkDir procedure requests. The counter keeps track of the average response time of MkDir requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : mkdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : mkdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : mkdir.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : mkdir.total - Harvest Metric : svm_nfs_mkdir_total Description : Total number MkDir of procedure requests. It is the total number of MkDir success and MkDir error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : mkdir_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : mkdir.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_mknod_avg_latency Description : Average latency of MkNod procedure requests. The counter keeps track of the average response time of MkNod requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : mknod_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : mknod_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : mknod.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : mknod.total - Harvest Metric : svm_nfs_mknod_total Description : Total number MkNod of procedure requests. It is the total number of MkNod success and MkNod error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : mknod_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : mknod.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_null_avg_latency Description : Average latency of NFSv4.2 NULL procedures. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : null_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : null_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : null.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : null.total - Harvest Metric : svm_nfs_null_total Description : Total number of NFSv4.2 NULL procedures. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : null_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : null.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_nverify_avg_latency Description : Average latency of NFSv4.2 NVERIFY operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : nverify_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : nverify_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : nverify.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : nverify.total - Harvest Metric : svm_nfs_nverify_total Description : Total number of NFSv4.2 NVERIFY operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : nverify_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : nverify.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_open_avg_latency Description : Average latency of NFSv4.2 OPEN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : open_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : open_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : open.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : open.total - Harvest Metric : svm_nfs_open_confirm_avg_latency Description : Average latency of OPEN_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : open_confirm_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : microsec type : average,no-zero-values base : open_confirm_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : open_confirm.average_latency template : conf/restperf/9.12.0/nfsv4.yaml unit : microsec type : average base : open_confirm.total - Harvest Metric : svm_nfs_open_confirm_total Description : Total number of OPEN_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : open_confirm_total template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : open_confirm.total template : conf/restperf/9.12.0/nfsv4.yaml unit : none type : rate - Harvest Metric : svm_nfs_open_downgrade_avg_latency Description : Average latency of NFSv4.2 OPEN_DOWNGRADE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : open_downgrade_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : open_downgrade_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : open_downgrade.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : open_downgrade.total - Harvest Metric : svm_nfs_open_downgrade_total Description : Total number of NFSv4.2 OPEN_DOWNGRADE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : open_downgrade_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : open_downgrade.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_open_total Description : Total number of NFSv4.2 OPEN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : open_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : open.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_openattr_avg_latency Description : Average latency of NFSv4.2 OPENATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : openattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : openattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : openattr.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : openattr.total - Harvest Metric : svm_nfs_openattr_total Description : Total number of NFSv4.2 OPENATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : openattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : openattr.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_ops Description : Total number of nfsv42 requests per sec. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : total_ops template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : total_ops template : conf/restperf/9.12.0/nfsv4_2.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_pathconf_avg_latency Description : Average latency of PathConf procedure requests. The counter keeps track of the average response time of PathConf requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : pathconf_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : pathconf_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : pathconf.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : pathconf.total - Harvest Metric : svm_nfs_pathconf_total Description : Total number PathConf of procedure requests. It is the total number of PathConf success and PathConf error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : pathconf_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : pathconf.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_putfh_avg_latency Description : Average latency of NFSv4.2 PUTFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : putfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putfh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : putfh.total - Harvest Metric : svm_nfs_putfh_total Description : Total number of NFSv4.2 PUTFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putfh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_putpubfh_avg_latency Description : Average latency of NFSv4.2 PUTPUBFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putpubfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : putpubfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putpubfh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : putpubfh.total - Harvest Metric : svm_nfs_putpubfh_total Description : Total number of NFSv4.2 PUTPUBFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putpubfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putpubfh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_putrootfh_avg_latency Description : Average latency of NFSv4.2 PUTROOTFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putrootfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : putrootfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putrootfh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : putrootfh.total - Harvest Metric : svm_nfs_putrootfh_total Description : Total number of NFSv4.2 PUTROOTFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putrootfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putrootfh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_read_avg_latency Description : Average latency of NFSv4.2 READ operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : read_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : read_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : read.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : read.total - Harvest Metric : svm_nfs_read_ops Description : Total observed NFSv3 read operations per second. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : nfsv3_read_ops template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : read_ops template : conf/restperf/9.12.0/nfsv3.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_read_symlink_avg_latency Description : Average latency of ReadSymLink procedure requests. The counter keeps track of the average response time of ReadSymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : read_symlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : read_symlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : read_symlink.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : read_symlink.total - Harvest Metric : svm_nfs_read_symlink_total Description : Total number of ReadSymLink procedure requests. It is the total number of read symlink success and read symlink error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : read_symlink_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : read_symlink.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : delta - Harvest Metric : svm_nfs_read_throughput Description : NFSv4.2 read data transfers. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : nfs41_read_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : total.read_throughput template : conf/restperf/9.12.0/nfsv4_2.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_read_total Description : Total number of NFSv4.2 READ operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : read_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : read.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_readdir_avg_latency Description : Average latency of NFSv4.2 READDIR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : readdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : readdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : readdir.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : readdir.total - Harvest Metric : svm_nfs_readdir_total Description : Total number of NFSv4.2 READDIR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : readdir_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : readdir.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_readdirplus_avg_latency Description : Average latency of ReadDirPlus procedure requests. The counter keeps track of the average response time of ReadDirPlus requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : readdirplus_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : readdirplus_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : readdirplus.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : readdirplus.total - Harvest Metric : svm_nfs_readdirplus_total Description : Total number ReadDirPlus of procedure requests. It is the total number of ReadDirPlus success and ReadDirPlus error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : readdirplus_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : readdirplus.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_readlink_avg_latency Description : Average latency of NFSv4.2 READLINK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : readlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : readlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : readlink.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : readlink.total - Harvest Metric : svm_nfs_readlink_total Description : Total number of NFSv4.2 READLINK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : readlink_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : readlink.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_reclaim_complete_avg_latency Description : Average latency of NFSv4.2 RECLAIM_complete operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : reclaim_complete_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : reclaim_complete_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : reclaim_complete.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : reclaim_complete.total - Harvest Metric : svm_nfs_reclaim_complete_total Description : Total number of NFSv4.2 RECLAIM_complete operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : reclaim_complete_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : reclaim_complete.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_release_lock_owner_avg_latency Description : Average Latency of RELEASE_LOCKOWNER procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : release_lock_owner_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : microsec type : average,no-zero-values base : release_lock_owner_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : release_lock_owner.average_latency template : conf/restperf/9.12.0/nfsv4.yaml unit : microsec type : average base : release_lock_owner.total - Harvest Metric : svm_nfs_release_lock_owner_total Description : Total number of RELEASE_LOCKOWNER procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : release_lock_owner_total template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : release_lock_owner.total template : conf/restperf/9.12.0/nfsv4.yaml unit : none type : rate - Harvest Metric : svm_nfs_remove_avg_latency Description : Average latency of NFSv4.2 REMOVE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : remove_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : remove_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : remove.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : remove.total - Harvest Metric : svm_nfs_remove_total Description : Total number of NFSv4.2 REMOVE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : remove_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : remove.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_rename_avg_latency Description : Average latency of NFSv4.2 RENAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : rename_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : rename_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : rename.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : rename.total - Harvest Metric : svm_nfs_rename_total Description : Total number of NFSv4.2 RENAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : rename_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : rename.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_renew_avg_latency Description : Average latency of RENEW procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : renew_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : microsec type : average,no-zero-values base : renew_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : renew.average_latency template : conf/restperf/9.12.0/nfsv4.yaml unit : microsec type : average base : renew.total - Harvest Metric : svm_nfs_renew_total Description : Total number of RENEW procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : renew_total template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : renew.total template : conf/restperf/9.12.0/nfsv4.yaml unit : none type : rate - Harvest Metric : svm_nfs_restorefh_avg_latency Description : Average latency of NFSv4.2 RESTOREFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : restorefh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : restorefh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : restorefh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : restorefh.total - Harvest Metric : svm_nfs_restorefh_total Description : Total number of NFSv4.2 RESTOREFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : restorefh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : restorefh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_rmdir_avg_latency Description : Average latency of RmDir procedure requests. The counter keeps track of the average response time of RmDir requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : rmdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : rmdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : rmdir.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : rmdir.total - Harvest Metric : svm_nfs_rmdir_total Description : Total number RmDir of procedure requests. It is the total number of RmDir success and RmDir error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : rmdir_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : rmdir.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_savefh_avg_latency Description : Average latency of NFSv4.2 SAVEFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : savefh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : savefh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : savefh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : savefh.total - Harvest Metric : svm_nfs_savefh_total Description : Total number of NFSv4.2 SAVEFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : savefh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : savefh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_secinfo_avg_latency Description : Average latency of NFSv4.2 SECINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : secinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : secinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : secinfo.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : secinfo.total - Harvest Metric : svm_nfs_secinfo_no_name_avg_latency Description : Average latency of NFSv4.2 SECINFO_NO_NAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : secinfo_no_name_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : secinfo_no_name_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : secinfo_no_name.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : secinfo_no_name.total - Harvest Metric : svm_nfs_secinfo_no_name_total Description : Total number of NFSv4.2 SECINFO_NO_NAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : secinfo_no_name_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : secinfo_no_name.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_secinfo_total Description : Total number of NFSv4.2 SECINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : secinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : secinfo.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_sequence_avg_latency Description : Average latency of NFSv4.2 SEQUENCE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : sequence_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : sequence_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : sequence.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : sequence.total - Harvest Metric : svm_nfs_sequence_total Description : Total number of NFSv4.2 SEQUENCE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : sequence_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : sequence.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_set_ssv_avg_latency Description : Average latency of NFSv4.2 SET_SSV operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : set_ssv_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : set_ssv_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : set_ssv.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : set_ssv.total - Harvest Metric : svm_nfs_set_ssv_total Description : Total number of NFSv4.2 SET_SSV operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : set_ssv_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : set_ssv.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_setattr_avg_latency Description : Average latency of NFSv4.2 SETATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : setattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : setattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : setattr.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : setattr.total - Harvest Metric : svm_nfs_setattr_total Description : Total number of NFSv4.2 SETATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : setattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : setattr.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_setclientid_avg_latency Description : Average latency of SETCLIENTID procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : setclientid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : microsec type : average,no-zero-values base : setclientid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : setclientid.average_latency template : conf/restperf/9.12.0/nfsv4.yaml unit : microsec type : average base : setclientid.total - Harvest Metric : svm_nfs_setclientid_confirm_avg_latency Description : Average latency of SETCLIENTID_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : setclientid_confirm_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : microsec type : average,no-zero-values base : setclientid_confirm_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : setclientid_confirm.average_latency template : conf/restperf/9.12.0/nfsv4.yaml unit : microsec type : average base : setclientid_confirm.total - Harvest Metric : svm_nfs_setclientid_confirm_total Description : Total number of SETCLIENTID_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : setclientid_confirm_total template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : setclientid_confirm.total template : conf/restperf/9.12.0/nfsv4.yaml unit : none type : rate - Harvest Metric : svm_nfs_setclientid_total Description : Total number of SETCLIENTID procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : setclientid_total template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : setclientid.total template : conf/restperf/9.12.0/nfsv4.yaml unit : none type : rate - Harvest Metric : svm_nfs_symlink_avg_latency Description : Average latency of SymLink procedure requests. The counter keeps track of the average response time of SymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : symlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : symlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : symlink.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : symlink.total - Harvest Metric : svm_nfs_symlink_total Description : Total number SymLink of procedure requests. It is the total number of SymLink success and create SymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : symlink_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : symlink.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_test_stateid_avg_latency Description : Average latency of NFSv4.2 TEST_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : test_stateid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : test_stateid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : test_stateid.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : test_stateid.total - Harvest Metric : svm_nfs_test_stateid_total Description : Total number of NFSv4.2 TEST_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : test_stateid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : test_stateid.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_throughput Description : NFSv4.2 write data transfers. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : nfs41_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : total.write_throughput template : conf/restperf/9.12.0/nfsv4_2.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_verify_avg_latency Description : Average latency of NFSv4.2 VERIFY operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : verify_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : verify_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : verify.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : verify.total - Harvest Metric : svm_nfs_verify_total Description : Total number of NFSv4.2 VERIFY operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : verify_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : verify.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_want_delegation_avg_latency Description : Average latency of NFSv4.2 WANT_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : want_delegation_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : want_delegation_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : want_delegation.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : want_delegation.total - Harvest Metric : svm_nfs_want_delegation_total Description : Total number of NFSv4.2 WANT_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : want_delegation_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : want_delegation.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_write_avg_latency Description : Average latency of NFSv4.2 WRITE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : write_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : write_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : write.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : write.total - Harvest Metric : svm_nfs_write_ops Description : Total observed NFSv3 write operations per second. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : nfsv3_write_ops template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : write_ops template : conf/restperf/9.12.0/nfsv3.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_write_throughput Description : NFSv4.2 data transfers. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : nfs41_write_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : total.throughput template : conf/restperf/9.12.0/nfsv4_2.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_write_total Description : Total number of NFSv4.2 WRITE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : write_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : write.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_vol_avg_latency Description : Average latency in microseconds for the WAFL filesystem to process all the operations on the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:vserver metric : avg_latency template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : microsec type : average base : total_ops REST : endpoint : api/cluster/counter/tables/volume:svm metric : average_latency template : conf/restperf/9.12.0/volume_svm.yaml unit : microsec type : average base : total_ops - Harvest Metric : svm_vol_other_latency Description : Average latency in microseconds for the WAFL filesystem to process other operations to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:vserver metric : other_latency template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/volume:svm metric : other_latency template : conf/restperf/9.12.0/volume_svm.yaml unit : microsec type : average base : total_other_ops - Harvest Metric : svm_vol_other_ops Description : Number of other operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:vserver metric : other_ops template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : total_other_ops template : conf/restperf/9.12.0/volume_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_vol_read_data Description : Bytes read per second ZAPI : endpoint : perf-object-get-instances volume:vserver metric : read_data template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : bytes_read template : conf/restperf/9.12.0/volume_svm.yaml unit : b_per_sec type : rate - Harvest Metric : svm_vol_read_latency Description : Average latency in microseconds for the WAFL filesystem to process read request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:vserver metric : read_latency template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/volume:svm metric : read_latency template : conf/restperf/9.12.0/volume_svm.yaml unit : microsec type : average base : total_read_ops - Harvest Metric : svm_vol_read_ops Description : Number of read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume:vserver metric : read_ops template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : total_read_ops template : conf/restperf/9.12.0/volume_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_vol_total_ops Description : Number of operations per second serviced by the volume ZAPI : endpoint : perf-object-get-instances volume:vserver metric : total_ops template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : total_ops template : conf/restperf/9.12.0/volume_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_vol_write_data Description : Bytes written per second ZAPI : endpoint : perf-object-get-instances volume:vserver metric : write_data template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : bytes_written template : conf/restperf/9.12.0/volume_svm.yaml unit : b_per_sec type : rate - Harvest Metric : svm_vol_write_latency Description : Average latency in microseconds for the WAFL filesystem to process write request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:vserver metric : write_latency template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/volume:svm metric : write_latency template : conf/restperf/9.12.0/volume_svm.yaml unit : microsec type : average base : total_write_ops - Harvest Metric : svm_vol_write_ops Description : Number of write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:vserver metric : write_ops template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : total_write_ops template : conf/restperf/9.12.0/volume_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_vscan_connections_active Description : Total number of current active connections ZAPI : endpoint : perf-object-get-instances offbox_vscan metric : connections_active template : conf/zapiperf/cdot/9.8.0/vscan_svm.yaml unit : none type : raw - Harvest Metric : svm_vscan_dispatch_latency Description : Average dispatch latency ZAPI : endpoint : perf-object-get-instances offbox_vscan metric : dispatch_latency template : conf/zapiperf/cdot/9.8.0/vscan_svm.yaml unit : microsec type : average base : dispatch_latency_base - Harvest Metric : svm_vscan_scan_latency Description : Average scan latency ZAPI : endpoint : perf-object-get-instances offbox_vscan metric : scan_latency template : conf/zapiperf/cdot/9.8.0/vscan_svm.yaml unit : microsec type : average base : scan_latency_base - Harvest Metric : svm_vscan_scan_noti_received_rate Description : Total number of scan notifications received by the dispatcher per second ZAPI : endpoint : perf-object-get-instances offbox_vscan metric : scan_noti_received_rate template : conf/zapiperf/cdot/9.8.0/vscan_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_vscan_scan_request_dispatched_rate Description : Total number of scan requests sent to the Vscanner per second ZAPI : endpoint : perf-object-get-instances offbox_vscan metric : scan_request_dispatched_rate template : conf/zapiperf/cdot/9.8.0/vscan_svm.yaml unit : per_sec type : rate - Harvest Metric : token_copy_bytes Description : Total number of bytes copied. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_copy_bytes template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/token_manager metric : token_copy.bytes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : rate - Harvest Metric : token_copy_failure Description : Number of failed token copy requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_copy_failure template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_copy.failures template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : token_copy_success Description : Number of successful token copy requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_copy_success template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_copy.successes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : token_create_bytes Description : Total number of bytes for which tokens are created. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_create_bytes template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/token_manager metric : token_create.bytes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : rate - Harvest Metric : token_create_failure Description : Number of failed token create requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_create_failure template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_create.failures template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : token_create_success Description : Number of successful token create requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_create_success template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_create.successes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : token_zero_bytes Description : Total number of bytes zeroed. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_zero_bytes template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/token_manager metric : token_zero.bytes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : rate - Harvest Metric : token_zero_failure Description : Number of failed token zero requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_zero_failure template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_zero.failures template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : token_zero_success Description : Number of successful token zero requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_zero_success template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_zero.successes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : volume_autosize_grow_threshold_percent Description : Used space threshold size, in percentage, for the automatic growth of the volume. When the amount of used space in the volume becomes greater than this threhold, the volume automatically grows unless it has reached the maximum size. The volume grows when 'space.used' is greater than this percent of 'space.size'. The 'grow_threshold' size cannot be less than or equal to the 'shrink_threshold' size.. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-autosize-attributes.grow-threshold-percent template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : autosize.grow_threshold template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_autosize_maximum_size Description : Maximum size in bytes up to which a volume grows automatically. This size cannot be less than the current volume size, or less than or equal to the minimum size of volume. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-autosize-attributes.maximum-size template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : autosize.maximum template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_avg_latency Description : Average latency in microseconds for the WAFL filesystem to process all the operations on the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume metric : avg_latency template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : microsec type : average base : total_ops REST : endpoint : api/cluster/counter/tables/volume metric : average_latency template : conf/restperf/9.12.0/volume.yaml unit : microsec type : average base : total_ops - Harvest Metric : volume_filesystem_size Description : Total usable size of the volume, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.filesystem-size template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.filesystem_size template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_inode_files_total Description : Total user-visible file (inode) count, i.e., current maximum number of user-visible files (inodes) that this volume can currently hold. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-inode-attributes.files-total template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : files template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_inode_files_used Description : Number of user-visible files (inodes) used. This field is valid only when the volume is online. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-inode-attributes.files-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : files_used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_other_latency Description : Average latency in microseconds for the WAFL filesystem to process other operations to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume metric : other_latency template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/volume metric : other_latency template : conf/restperf/9.12.0/volume.yaml unit : microsec type : average base : total_other_ops - Harvest Metric : volume_other_ops Description : Number of other operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume metric : other_ops template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : total_other_ops template : conf/restperf/9.12.0/volume.yaml unit : per_sec type : rate - Harvest Metric : volume_read_data Description : Bytes read per second ZAPI : endpoint : perf-object-get-instances volume metric : read_data template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : bytes_read template : conf/restperf/9.12.0/volume.yaml unit : b_per_sec type : rate - Harvest Metric : volume_read_latency Description : Average latency in microseconds for the WAFL filesystem to process read request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume metric : read_latency template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/volume metric : read_latency template : conf/restperf/9.12.0/volume.yaml unit : microsec type : average base : total_read_ops - Harvest Metric : volume_read_ops Description : Number of read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume metric : read_ops template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : total_read_ops template : conf/restperf/9.12.0/volume.yaml unit : per_sec type : rate - Harvest Metric : volume_sis_compress_saved Description : The total disk space (in bytes) that is saved by compressing blocks on the referenced file system. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.compression-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : compression_space_saved template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_sis_compress_saved_percent Description : Percentage of the total disk space that is saved by compressing blocks on the referenced file system ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.percentage-compression-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : compression_space_saved_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_sis_dedup_saved Description : The total disk space (in bytes) that is saved by deduplication and file cloning. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.deduplication-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : dedupe_space_saved template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_sis_dedup_saved_percent Description : Percentage of the total disk space that is saved by deduplication and file cloning. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.percentage-deduplication-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : dedupe_space_saved_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_sis_total_saved Description : Total space saved (in bytes) in the volume due to deduplication, compression, and file cloning. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.total-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : sis_space_saved template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_sis_total_saved_percent Description : Percentage of total disk space that is saved by compressing blocks, deduplication and file cloning. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.percentage-total-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : sis_space_saved_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_size Description : Total provisioned size. The default size is equal to the minimum size of 20MB, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.size template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_size_available Description : The available space, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size-available template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.available template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_size_total Description : Total size of AFS, excluding snap-reserve, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size-total template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.afs_total template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_size_used Description : The virtual space used (includes volume reserves) before storage efficiency, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_size_used_percent Description : Percentage of the volume size that is used. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.percentage-size-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.percent_used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshot_count Description : Number of Snapshot copies in the volume. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-snapshot-attributes.snapshot-count template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : snapshot_count template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshot_reserve_available Description : Size available for Snapshot copies within the Snapshot copy reserve, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.snapshot-reserve-available template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.snapshot.reserve_available template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshot_reserve_percent Description : The space that has been set aside as a reserve for Snapshot copy usage, in percent. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.percentage-snapshot-reserve template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.snapshot.reserve_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshot_reserve_size Description : Size in the volume that has been set aside as a reserve for Snapshot copy usage, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.snapshot-reserve-size template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.snapshot.reserve_size template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshot_reserve_used_percent Description : Percentage of snapshot reserve size that has been used. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.percentage-snapshot-reserve-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.snapshot.space_used_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshots_size_available Description : Available space for Snapshot copies from snap-reserve, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size-available-for-snapshots template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.size_available_for_snapshots template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshots_size_used Description : The total space used by Snapshot copies in the volume, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size-used-by-snapshots template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.snapshot.used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_expected_available Description : Size that should be available for the volume, irrespective of available size in the aggregate, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.expected-available template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.expected_available template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_logical_available Description : The amount of space available in this volume with storage efficiency space considered used, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.logical-available template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.logical_space.available template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_logical_used Description : SUM of (physical-used, shared_refs, compression_saved_in_plane0, vbn_zero, future_blk_cnt), in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.logical-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.logical_space.used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_logical_used_by_afs Description : The virtual space used by AFS alone (includes volume reserves) and along with storage efficiency, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.logical-used-by-afs template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.logical_space.used_by_afs template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_logical_used_by_snapshots Description : Size that is logically used across all Snapshot copies in the volume, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.logical-used-by-snapshots template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.logical_space.used_by_snapshots template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_logical_used_percent Description : SUM of (physical-used, shared_refs, compression_saved_in_plane0, vbn_zero, future_blk_cnt), as a percentage. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.logical-used-percent template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.logical_space.used_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_physical_used Description : Size that is physically used in the volume, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.physical-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.physical_used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_physical_used_percent Description : Size that is physically used in the volume, as a percentage. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.physical-used-percent template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.physical_used_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_total_ops Description : Number of operations per second serviced by the volume ZAPI : endpoint : perf-object-get-instances volume metric : total_ops template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : total_ops template : conf/restperf/9.12.0/volume.yaml unit : per_sec type : rate - Harvest Metric : volume_write_data Description : Bytes written per second ZAPI : endpoint : perf-object-get-instances volume metric : write_data template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : bytes_written template : conf/restperf/9.12.0/volume.yaml unit : b_per_sec type : rate - Harvest Metric : volume_write_latency Description : Average latency in microseconds for the WAFL filesystem to process write request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume metric : write_latency template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/volume metric : write_latency template : conf/restperf/9.12.0/volume.yaml unit : microsec type : average base : total_write_ops - Harvest Metric : volume_write_ops Description : Number of write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume metric : write_ops template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : total_write_ops template : conf/restperf/9.12.0/volume.yaml unit : per_sec type : rate - Harvest Metric : vscan_scan_latency Description : Average scan latency ZAPI : endpoint : perf-object-get-instances offbox_vscan_server metric : scan_latency template : conf/zapiperf/cdot/9.8.0/vscan.yaml unit : microsec type : average base : scan_latency_base - Harvest Metric : vscan_scan_request_dispatched_rate Description : Total number of scan requests sent to the Vscanner per second ZAPI : endpoint : perf-object-get-instances offbox_vscan_server metric : scan_request_dispatched_rate template : conf/zapiperf/cdot/9.8.0/vscan.yaml unit : per_sec type : rate - Harvest Metric : vscan_scanner_stats_pct_cpu_used Description : Percentage CPU utilization on scanner ZAPI : endpoint : perf-object-get-instances offbox_vscan_server metric : scanner_stats_pct_cpu_used template : conf/zapiperf/cdot/9.8.0/vscan.yaml unit : none type : raw - Harvest Metric : vscan_scanner_stats_pct_mem_used Description : Percentage RAM utilization on scanner ZAPI : endpoint : perf-object-get-instances offbox_vscan_server metric : scanner_stats_pct_mem_used template : conf/zapiperf/cdot/9.8.0/vscan.yaml unit : none type : raw - Harvest Metric : vscan_scanner_stats_pct_network_used Description : Percentage network utilization on scanner ZAPI : endpoint : perf-object-get-instances offbox_vscan_server metric : scanner_stats_pct_network_used template : conf/zapiperf/cdot/9.8.0/vscan.yaml unit : none type : raw - Harvest Metric : wafl_avg_msg_latency Description : Average turnaround time for WAFL messages in milliseconds. ZAPI : endpoint : perf-object-get-instances wafl metric : avg_wafl_msg_latency template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : millisec type : average base : wafl_msg_total REST : endpoint : api/cluster/counter/tables/wafl metric : average_msg_latency template : conf/restperf/9.12.0/wafl.yaml unit : millisec type : average base : msg_total - Harvest Metric : wafl_avg_non_wafl_msg_latency Description : Average turnaround time for non-WAFL messages in milliseconds. ZAPI : endpoint : perf-object-get-instances wafl metric : avg_non_wafl_msg_latency template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : millisec type : average base : non_wafl_msg_total REST : endpoint : api/cluster/counter/tables/wafl metric : average_non_wafl_msg_latency template : conf/restperf/9.12.0/wafl.yaml unit : millisec type : average base : non_wafl_msg_total - Harvest Metric : wafl_avg_repl_msg_latency Description : Average turnaround time for replication WAFL messages in milliseconds. ZAPI : endpoint : perf-object-get-instances wafl metric : avg_wafl_repl_msg_latency template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : millisec type : average base : wafl_repl_msg_total REST : endpoint : api/cluster/counter/tables/wafl metric : average_replication_msg_latency template : conf/restperf/9.12.0/wafl.yaml unit : millisec type : average base : replication_msg_total - Harvest Metric : wafl_cp_count Description : Array of counts of different types of Consistency Points (CP). ZAPI : endpoint : perf-object-get-instances wafl metric : cp_count template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : cp_count template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_cp_phase_times Description : Array of percentage time spent in different phases of Consistency Point (CP). ZAPI : endpoint : perf-object-get-instances wafl metric : cp_phase_times template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : percent type : percent base : total_cp_msecs REST : endpoint : api/cluster/counter/tables/wafl metric : cp_phase_times template : conf/restperf/9.12.0/wafl.yaml unit : percent type : percent base : total_cp_msecs - Harvest Metric : wafl_memory_free Description : The current WAFL memory available in the system. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_memory_free template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : mb type : raw REST : endpoint : api/cluster/counter/tables/wafl metric : memory_free template : conf/restperf/9.12.0/wafl.yaml unit : mb type : raw - Harvest Metric : wafl_memory_used Description : The current WAFL memory used in the system. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_memory_used template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : mb type : raw REST : endpoint : api/cluster/counter/tables/wafl metric : memory_used template : conf/restperf/9.12.0/wafl.yaml unit : mb type : raw - Harvest Metric : wafl_msg_total Description : Total number of WAFL messages per second. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_msg_total template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl metric : msg_total template : conf/restperf/9.12.0/wafl.yaml unit : per_sec type : rate - Harvest Metric : wafl_non_wafl_msg_total Description : Total number of non-WAFL messages per second. ZAPI : endpoint : perf-object-get-instances wafl metric : non_wafl_msg_total template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl metric : non_wafl_msg_total template : conf/restperf/9.12.0/wafl.yaml unit : per_sec type : rate - Harvest Metric : wafl_read_io_type Description : Percentage of reads served from buffer cache, external cache, or disk. ZAPI : endpoint : perf-object-get-instances wafl metric : read_io_type template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : percent type : percent base : read_io_type_base REST : endpoint : api/cluster/counter/tables/wafl metric : read_io_type template : conf/restperf/9.12.0/wafl.yaml unit : percent type : percent base : read_io_type_base - Harvest Metric : wafl_reads_from_cache Description : WAFL reads from cache. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_cache template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_cache template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_cloud Description : WAFL reads from cloud storage. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_cloud template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_cloud template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_cloud_s2c_bin Description : WAFL reads from cloud storage via s2c bin. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_cloud_s2c_bin template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_cloud_s2c_bin template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_disk Description : WAFL reads from disk. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_disk template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_disk template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_ext_cache Description : WAFL reads from external cache. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_ext_cache template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_external_cache template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_fc_miss Description : WAFL reads from remote volume for fc_miss. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_fc_miss template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_fc_miss template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_pmem Description : Wafl reads from persistent mmeory. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_pmem template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_ssd Description : WAFL reads from SSD. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_ssd template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_ssd template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_repl_msg_total Description : Total number of replication WAFL messages per second. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_repl_msg_total template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl metric : replication_msg_total template : conf/restperf/9.12.0/wafl.yaml unit : per_sec type : rate - Harvest Metric : wafl_total_cp_msecs Description : Milliseconds spent in Consistency Point (CP). ZAPI : endpoint : perf-object-get-instances wafl metric : total_cp_msecs template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : millisec type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : total_cp_msecs template : conf/restperf/9.12.0/wafl.yaml unit : millisec type : delta - Harvest Metric : wafl_total_cp_util Description : Percentage of time spent in a Consistency Point (CP). ZAPI : endpoint : perf-object-get-instances wafl metric : total_cp_util template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : percent type : percent base : cpu_elapsed_time REST : endpoint : api/cluster/counter/tables/wafl metric : total_cp_util template : conf/restperf/9.12.0/wafl.yaml unit : percent type : percent base : cpu_elapsed_time","title":"ONTAP Metrics"},{"location":"ontap-metrics/#understanding-the-structure","text":"Click on Details below to help understand the structure of Metrics . Details - Harvest Metric : Name of metric exported by Harvest Description : Description of the metric ZAPI : endpoint : Name of ZAPI invoked to generate this metric metric : Metric name in ZAPI template : ZAPI template path unit : Unit of the counter. Possible values= per_sec, b_per_sec (bytes/s), kb_per_sec (Kbytes/s),mb_per_sec (Mbytes/s), percent, millisec, microsec, sec, or none type : Comma separated list of properties of the counter. The counter properties determine how raw counter values should be interpreted.Possible values= raw, rate, delta, percent, string,no-display and no-zero-values. base : Name of the counter used as the denominator to calculate values of counters involving averages and percentages. REST : endpoint : Name of REST API invoked to generate this metric metric : Metric name in REST API template : REST template path unit : Unit of the counter. Possible values : per_sec, b_per_sec (bytes/s), kb_per_sec (Kbytes/s),mb_per_sec (Mbytes/s), percent, millisec, microsec, sec, or none type : Comma separated list of properties of the counter. The counter properties determine how raw counter values should be interpreted.Possible values : raw, rate, delta, percent, string,no-display and no-zero-values. base : Name of the counter used as the denominator to calculate values of counters involving averages and percentages.","title":"Understanding the structure"},{"location":"ontap-metrics/#metrics","text":"- Harvest Metric : aggr_efficiency_savings Description : Space saved by storage efficiencies (logical_used - used) REST : endpoint : api/storage/aggregates metric : space.efficiency.savings template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_efficiency_savings_wo_snapshots Description : Space saved by storage efficiencies (logical_used - used) REST : endpoint : api/storage/aggregates metric : space.efficiency_without_snapshots.savings template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_efficiency_savings_wo_snapshots_flexclones Description : Space saved by storage efficiencies (logical_used - used) REST : endpoint : api/storage/aggregates metric : space.efficiency_without_snapshots_flexclones.savings template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_hybrid_cache_size_total Description : Total usable space in bytes of SSD cache. Only provided when hybrid_cache.enabled is 'true'. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.hybrid-cache-size-total template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : block_storage.hybrid_cache.size template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_hybrid_disk_count Description : Number of disks used in the cache tier of the aggregate. Only provided when hybrid_cache.enabled is 'true'. REST : endpoint : api/storage/aggregates metric : block_storage.hybrid_cache.disk_count template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_files_private_used Description : Number of system metadata files used. If the referenced file system is restricted or offline, a value of 0 is returned.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either footprint or **. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.files-private-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.files_private_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_files_total Description : Maximum number of user-visible files that this referenced file system can currently hold. If the referenced file system is restricted or offline, a value of 0 is returned. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.files-total template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.files_total template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_files_used Description : Number of user-visible files used in the referenced file system. If the referenced file system is restricted or offline, a value of 0 is returned. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.files-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.files_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_inodefile_private_capacity Description : Number of files that can currently be stored on disk for system metadata files. This number will dynamically increase as more system files are created.This is an advanced property; there is an added computationl cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either footprint or **. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.inodefile-private-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.file_private_capacity template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_inodefile_public_capacity Description : Number of files that can currently be stored on disk for user-visible files. This number will dynamically increase as more user-visible files are created.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either footprint or **. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.inodefile-public-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.file_public_capacity template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_maxfiles_available Description : The count of the maximum number of user-visible files currently allowable on the referenced file system. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.maxfiles-available template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.max_files_available template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_maxfiles_possible Description : The largest value to which the maxfiles-available parameter can be increased by reconfiguration, on the referenced file system. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.maxfiles-possible template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.max_files_possible template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_maxfiles_used Description : The number of user-visible files currently in use on the referenced file system. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.maxfiles-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.max_files_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_inode_used_percent Description : The percentage of disk space currently in use based on user-visible file count on the referenced file system. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-inode-attributes.percent-inode-used-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : inode_attributes.used_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_logical_used_wo_snapshots Description : Logical used ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-logical-used-wo-snapshots template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : api/storage/aggregates metric : space.efficiency_without_snapshots.logical_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_logical_used_wo_snapshots_flexclones Description : Logical used ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-logical-used-wo-snapshots-flexclones template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : api/storage/aggregates metric : space.efficiency_without_snapshots_flexclones.logical_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_physical_used_wo_snapshots Description : Total Data Reduction Physical Used Without Snapshots ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-physical-used-wo-snapshots template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_physical_used_wo_snapshots_flexclones Description : Total Data Reduction Physical Used without snapshots and flexclones ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-data-reduction-physical-used-wo-snapshots-flexclones template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_primary_disk_count Description : Number of disks used in the aggregate. This includes parity disks, but excludes disks in the hybrid cache. REST : endpoint : api/storage/aggregates metric : block_storage.primary.disk_count template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_raid_disk_count Description : Number of disks in the aggregate. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-raid-attributes.disk-count template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_raid_plex_count Description : Number of plexes in the aggregate ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-raid-attributes.plex-count template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : block_storage.plexes.# template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_raid_size Description : Option to specify the maximum number of disks that can be included in a RAID group. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-raid-attributes.raid-size template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : block_storage.primary.raid_size template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_files_total Description : Total files allowed in Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.files-total template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : snapshot.files_total template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_files_used Description : Total files created in Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.files-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : snapshot.files_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_inode_used_percent ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.percent-inode-used-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml - Harvest Metric : aggr_snapshot_maxfiles_available Description : Maximum files available for Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.maxfiles-available template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : snapshot.max_files_available template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_maxfiles_possible Description : The largest value to which the maxfiles-available parameter can be increased by reconfiguration, on the referenced file system. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.maxfiles-possible template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_maxfiles_used Description : Files in use by Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.maxfiles-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : snapshot.max_files_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_reserve_percent Description : Percentage of space reserved for Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.snapshot-reserve-percent template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.snapshot.reserve_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_size_available Description : Available space for Snapshot copies in bytes ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.size-available template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.snapshot.available template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_size_total Description : Total space for Snapshot copies in bytes ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.size-total template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.snapshot.total template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_size_used Description : Space used by Snapshot copies in bytes ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.size-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.snapshot.used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_snapshot_used_percent Description : Percentage of disk space used by Snapshot copies ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-snapshot-attributes.percent-used-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.snapshot.used_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_available Description : Space available in bytes. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.size-available template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.available template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_capacity_tier_used Description : Used space in bytes in the cloud store. Only applicable for aggregates with a cloud store tier. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.capacity-tier-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.cloud_storage.used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_data_compacted_count Description : Amount of compacted data in bytes. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.data-compacted-count template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.data_compacted_count template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_data_compaction_saved Description : Space saved in bytes by compacting the data. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.data-compaction-space-saved template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.data_compaction_space_saved template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_data_compaction_saved_percent Description : Percentage saved by compacting the data. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.data-compaction-space-saved-percent template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.data_compaction_space_saved_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_performance_tier_inactive_user_data Description : The size that is physically used in the block storage and has a cold temperature, in bytes. This property is only supported if the aggregate is either attached to a cloud store or can be attached to a cloud store.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either block_storage.inactive_user_data or **. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.performance-tier-inactive-user-data template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.inactive_user_data template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_performance_tier_inactive_user_data_percent Description : The percentage of inactive user data in the block storage. This property is only supported if the aggregate is either attached to a cloud store or can be attached to a cloud store.This is an advanced property; there is an added computational cost to retrieving its value. The field is not populated for either a collection GET or an instance GET unless it is explicitly requested using the <i>fields</i> query parameter containing either block_storage.inactive_user_data_percent or **. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.performance-tier-inactive-user-data-percent template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.inactive_user_data_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_physical_used Description : Total physical used size of an aggregate in bytes. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.physical-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.physical_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_physical_used_percent Description : Physical used percentage. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.physical-used-percent template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.physical_used_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_reserved ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.total-reserved-space template : conf/zapi/cdot/9.8.0/aggr.yaml - Harvest Metric : aggr_space_sis_saved Description : Amount of space saved in bytes by storage efficiency. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.sis-space-saved template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.volume_deduplication_space_saved template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_sis_saved_percent Description : Percentage of space saved by storage efficiency. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.sis-space-saved-percent template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.volume_deduplication_space_saved_percent template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_sis_shared_count Description : Amount of shared bytes counted by storage efficiency. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.sis-shared-count template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.volume_deduplication_shared_count template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_total Description : Total usable space in bytes, not including WAFL reserve and aggregate Snapshot copy reserve. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.size-total template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.size template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_used Description : Space used or reserved in bytes. Includes volume guarantees and aggregate metadata. ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.size-used template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : space.block_storage.used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_space_used_percent Description : The percentage of disk space currently in use on the referenced file system ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-space-attributes.percent-used-capacity template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_total_logical_used Description : Logical used ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-logical-used template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : api/storage/aggregates metric : space.efficiency.logical_used template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_total_physical_used Description : Total Physical Used ZAPI : endpoint : aggr-efficiency-get-iter metric : aggr-efficiency-info.aggr-efficiency-cumulative-info.total-physical-used template : conf/zapi/cdot/9.9.0/aggr_efficiency.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : aggr_volume_count_flexvol ZAPI : endpoint : aggr-get-iter metric : aggr-attributes.aggr-volume-count-attributes.flexvol-count template : conf/zapi/cdot/9.8.0/aggr.yaml REST : endpoint : api/storage/aggregates metric : volume_count template : conf/rest/9.12.0/aggr.yaml - Harvest Metric : cluster_subsystem_outstanding_alerts Description : Number of outstanding alerts ZAPI : endpoint : diagnosis-subsystem-config-get-iter metric : diagnosis-subsystem-config-info.outstanding-alert-count template : conf/zapi/cdot/9.8.0/subsystem.yaml REST : endpoint : api/private/cli/system/health/subsystem metric : outstanding_alert_count template : conf/rest/9.12.0/subsystem.yaml - Harvest Metric : cluster_subsystem_suppressed_alerts Description : Number of suppressed alerts ZAPI : endpoint : diagnosis-subsystem-config-get-iter metric : diagnosis-subsystem-config-info.suppressed-alert-count template : conf/zapi/cdot/9.8.0/subsystem.yaml REST : endpoint : api/private/cli/system/health/subsystem metric : suppressed_alert_count template : conf/rest/9.12.0/subsystem.yaml - Harvest Metric : copy_manager_bce_copy_count_curr Description : Current number of copy requests being processed by the Block Copy Engine. ZAPI : endpoint : perf-object-get-instances copy_manager metric : bce_copy_count_curr template : conf/zapiperf/cdot/9.8.0/copy_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/copy_manager metric : block_copy_engine_current_copy_count template : conf/restperf/9.12.0/copy_manager.yaml unit : none type : delta - Harvest Metric : copy_manager_kb_copied Description : Sum of kilo-bytes copied. ZAPI : endpoint : perf-object-get-instances copy_manager metric : KB_copied template : conf/zapiperf/cdot/9.8.0/copy_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/copy_manager metric : KB_copied template : conf/restperf/9.12.0/copy_manager.yaml unit : none type : delta - Harvest Metric : copy_manager_ocs_copy_count_curr Description : Current number of copy requests being processed by the ONTAP copy subsystem. ZAPI : endpoint : perf-object-get-instances copy_manager metric : ocs_copy_count_curr template : conf/zapiperf/cdot/9.8.0/copy_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/copy_manager metric : ontap_copy_subsystem_current_copy_count template : conf/restperf/9.12.0/copy_manager.yaml unit : none type : delta - Harvest Metric : copy_manager_sce_copy_count_curr Description : Current number of copy requests being processed by the System Continuous Engineering. ZAPI : endpoint : perf-object-get-instances copy_manager metric : sce_copy_count_curr template : conf/zapiperf/cdot/9.8.0/copy_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/copy_manager metric : system_continuous_engineering_current_copy_count template : conf/restperf/9.12.0/copy_manager.yaml unit : none type : delta - Harvest Metric : copy_manager_spince_copy_count_curr Description : Current number of copy requests being processed by the SpinCE. ZAPI : endpoint : perf-object-get-instances copy_manager metric : spince_copy_count_curr template : conf/zapiperf/cdot/9.8.0/copy_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/copy_manager metric : spince_current_copy_count template : conf/restperf/9.12.0/copy_manager.yaml unit : none type : delta - Harvest Metric : disk_busy Description : The utilization percent of the disk ZAPI : endpoint : perf-object-get-instances disk:constituent metric : disk_busy template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : percent type : percent base : base_for_disk_busy REST : endpoint : api/cluster/counter/tables/disk:constituent metric : disk_busy_percent template : conf/restperf/9.12.0/disk.yaml unit : percent type : percent base : base_for_disk_busy - Harvest Metric : disk_bytes_per_sector Description : Bytes per sector. ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-inventory-info.bytes-per-sector template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/storage/disks metric : bytes_per_sector template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_capacity Description : Disk capacity in MB ZAPI : endpoint : perf-object-get-instances disk:constituent metric : disk_capacity template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : mb type : raw REST : endpoint : api/cluster/counter/tables/disk:constituent metric : capacity template : conf/restperf/9.12.0/disk.yaml unit : mb type : raw - Harvest Metric : disk_cp_read_chain Description : Average number of blocks transferred in each consistency point read operation during a CP ZAPI : endpoint : perf-object-get-instances disk:constituent metric : cp_read_chain template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : none type : average base : cp_reads REST : endpoint : api/cluster/counter/tables/disk:constituent metric : cp_read_chain template : conf/restperf/9.12.0/disk.yaml unit : none type : average base : cp_read_count - Harvest Metric : disk_cp_read_latency Description : Average latency per block in microseconds for consistency point read operations ZAPI : endpoint : perf-object-get-instances disk:constituent metric : cp_read_latency template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : microsec type : average base : cp_read_blocks REST : endpoint : api/cluster/counter/tables/disk:constituent metric : cp_read_latency template : conf/restperf/9.12.0/disk.yaml unit : microsec type : average base : cp_read_blocks - Harvest Metric : disk_cp_reads Description : Number of disk read operations initiated each second for consistency point processing ZAPI : endpoint : perf-object-get-instances disk:constituent metric : cp_reads template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : cp_read_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : disk_io_pending Description : Average number of I/Os issued to the disk for which we have not yet received the response ZAPI : endpoint : perf-object-get-instances disk:constituent metric : io_pending template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : none type : average base : base_for_disk_busy REST : endpoint : api/cluster/counter/tables/disk:constituent metric : io_pending template : conf/restperf/9.12.0/disk.yaml unit : none type : average base : base_for_disk_busy - Harvest Metric : disk_io_queued Description : Number of I/Os queued to the disk but not yet issued ZAPI : endpoint : perf-object-get-instances disk:constituent metric : io_queued template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : none type : average base : base_for_disk_busy REST : endpoint : api/cluster/counter/tables/disk:constituent metric : io_queued template : conf/restperf/9.12.0/disk.yaml unit : none type : average base : base_for_disk_busy - Harvest Metric : disk_power_on_hours Description : Hours powered on. REST : endpoint : api/storage/disks metric : stats.power_on_hours template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_sectors Description : Number of sectors on the disk. ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-inventory-info.capacity-sectors template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/storage/disks metric : sector_count template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_stats_average_latency Description : Average I/O latency across all active paths, in milliseconds. ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-stats-info.average-latency template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/storage/disks metric : stats.average_latency template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_stats_io_kbps Description : Total Disk Throughput in KBPS Across All Active Paths ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-stats-info.disk-io-kbps template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/private/cli/disk metric : disk_io_kbps_total template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_stats_sectors_read Description : Number of Sectors Read ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-stats-info.sectors-read template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/private/cli/disk metric : sectors_read template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_stats_sectors_written Description : Number of Sectors Written ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-stats-info.sectors-written template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : api/private/cli/disk metric : sectors_written template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_total_transfers Description : Total number of disk operations involving data transfer initiated per second ZAPI : endpoint : perf-object-get-instances disk:constituent metric : total_transfers template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : total_transfer_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : disk_uptime Description : Number of seconds the drive has been powered on ZAPI : endpoint : storage-disk-get-iter metric : storage-disk-info.disk-stats-info.power-on-time-interval template : conf/zapi/cdot/9.8.0/disk.yaml REST : endpoint : NA metric : Harvest Plugin Generated template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_usable_size REST : endpoint : api/storage/disks metric : usable_size template : conf/rest/9.12.0/disk.yaml - Harvest Metric : disk_user_read_blocks Description : Number of blocks transferred for user read operations per second ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_read_blocks template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_read_block_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : disk_user_read_chain Description : Average number of blocks transferred in each user read operation ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_read_chain template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : none type : average base : user_reads REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_read_chain template : conf/restperf/9.12.0/disk.yaml unit : none type : average base : user_read_count - Harvest Metric : disk_user_read_latency Description : Average latency per block in microseconds for user read operations ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_read_latency template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : microsec type : average base : user_read_blocks REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_read_latency template : conf/restperf/9.12.0/disk.yaml unit : microsec type : average base : user_read_block_count - Harvest Metric : disk_user_reads Description : Number of disk read operations initiated each second for retrieving data or metadata associated with user requests ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_reads template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_read_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : disk_user_write_blocks Description : Number of blocks transferred for user write operations per second ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_write_blocks template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_write_block_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : disk_user_write_chain Description : Average number of blocks transferred in each user write operation ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_write_chain template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : none type : average base : user_writes REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_write_chain template : conf/restperf/9.12.0/disk.yaml unit : none type : average base : user_write_count - Harvest Metric : disk_user_write_latency Description : Average latency per block in microseconds for user write operations ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_write_latency template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : microsec type : average base : user_write_blocks REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_write_latency template : conf/restperf/9.12.0/disk.yaml unit : microsec type : average base : user_write_block_count - Harvest Metric : disk_user_writes Description : Number of disk write operations initiated each second for storing data or metadata associated with user requests ZAPI : endpoint : perf-object-get-instances disk:constituent metric : user_writes template : conf/zapiperf/cdot/9.8.0/disk.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/disk:constituent metric : user_write_count template : conf/restperf/9.12.0/disk.yaml unit : per_sec type : rate - Harvest Metric : environment_sensor_threshold_value Description : Provides the sensor reading. ZAPI : endpoint : environment-sensors-get-iter metric : environment-sensors-info.threshold-sensor-value template : conf/zapi/cdot/9.8.0/sensor.yaml REST : endpoint : api/cluster/sensors metric : value template : conf/rest/9.12.0/sensor.yaml - Harvest Metric : fabricpool_average_latency Description : Note This counter is deprecated and will be removed in a future release. Average latencies executed during various phases of command execution. The execution-start latency represents the average time taken to start executing a operation. The request-prepare latency represent the average time taken to prepare the commplete request that needs to be sent to the server. The send latency represents the average time taken to send requests to the server. The execution-start-to-send-complete represents the average time taken to send a operation out since its execution started. The execution-start-to-first-byte-received represent the average time taken to to receive the first byte of a response since the command&apos;s request execution started. These counters can be used to identify performance bottlenecks within the object store client module. ZAPI : endpoint : perf-object-get-instances object_store_client_op metric : average_latency template : conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml unit : microsec type : average,no-zero-values base : ops - Harvest Metric : fabricpool_cloud_bin_op_latency_average Description : Cloud bin operation latency average in milliseconds. ZAPI : endpoint : perf-object-get-instances wafl_comp_aggr_vol_bin metric : cloud_bin_op_latency_average template : conf/zapiperf/cdot/9.8.0/wafl_comp_aggr_vol_bin.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/wafl_comp_aggr_vol_bin metric : cloud_bin_op_latency_average template : conf/restperf/9.12.0/wafl_comp_aggr_vol_bin.yaml unit : none type : raw - Harvest Metric : fabricpool_cloud_bin_operation Description : Cloud bin operation counters. ZAPI : endpoint : perf-object-get-instances wafl_comp_aggr_vol_bin metric : cloud_bin_operation template : conf/zapiperf/cdot/9.8.0/wafl_comp_aggr_vol_bin.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl_comp_aggr_vol_bin metric : cloud_bin_op template : conf/restperf/9.12.0/wafl_comp_aggr_vol_bin.yaml unit : none type : delta - Harvest Metric : fabricpool_get_throughput_bytes Description : Note This counter is deprecated and will be removed in a future release. Counter that indicates the throughput for GET command in bytes per second. ZAPI : endpoint : perf-object-get-instances object_store_client_op metric : get_throughput_bytes template : conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml unit : b_per_sec type : rate,no-zero-values - Harvest Metric : fabricpool_put_throughput_bytes Description : Note This counter is deprecated and will be removed in a future release. Counter that indicates the throughput for PUT command in bytes per second. ZAPI : endpoint : perf-object-get-instances object_store_client_op metric : put_throughput_bytes template : conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml unit : b_per_sec type : rate,no-zero-values - Harvest Metric : fabricpool_stats Description : Note This counter is deprecated and will be removed in a future release. Counter that indicates the number of object store operations sent, and their success and failure counts. The objstore_client_op_name array indicate the operation name such as PUT, GET, etc. The objstore_client_op_stats_name array contain the total number of operations, their success and failure counter for each operation. ZAPI : endpoint : perf-object-get-instances object_store_client_op metric : stats template : conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml unit : none type : delta,no-zero-values - Harvest Metric : fabricpool_throughput_ops Description : Counter that indicates the throughput for commands in ops per second. ZAPI : endpoint : perf-object-get-instances object_store_client_op metric : throughput_ops template : conf/zapiperf/cdot/9.8.0/object_store_client_op.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : fcp_avg_other_latency Description : Average latency for operations other than read and write ZAPI : endpoint : perf-object-get-instances fcp_port metric : avg_other_latency template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/fcp metric : average_other_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : other_ops - Harvest Metric : fcp_avg_read_latency Description : Average latency for read operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : avg_read_latency template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/fcp metric : average_read_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : read_ops - Harvest Metric : fcp_avg_write_latency Description : Average latency for write operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : avg_write_latency template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/fcp metric : average_write_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : write_ops - Harvest Metric : fcp_discarded_frames_count Description : Number of discarded frames. ZAPI : endpoint : perf-object-get-instances fcp_port metric : discarded_frames_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : discarded_frames_count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_int_count Description : Number of interrupts ZAPI : endpoint : perf-object-get-instances fcp_port metric : int_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : interrupt_count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_invalid_crc Description : Number of invalid cyclic redundancy checks (CRC count) ZAPI : endpoint : perf-object-get-instances fcp_port metric : invalid_crc template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : invalid.crc template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_invalid_transmission_word Description : Number of invalid transmission words ZAPI : endpoint : perf-object-get-instances fcp_port metric : invalid_transmission_word template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : invalid.transmission_word template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_isr_count Description : Number of interrupt responses ZAPI : endpoint : perf-object-get-instances fcp_port metric : isr_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : isr.count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_lif_avg_latency Description : Average latency for FCP operations ZAPI : endpoint : perf-object-get-instances fcp_lif metric : avg_latency template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : microsec type : average base : total_ops REST : endpoint : api/cluster/counter/tables/fcp_lif metric : average_latency template : conf/restperf/9.12.0/fcp_lif.yaml unit : microsec type : average base : total_ops - Harvest Metric : fcp_lif_avg_other_latency Description : Average latency for operations other than read and write ZAPI : endpoint : perf-object-get-instances fcp_lif metric : avg_other_latency template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/fcp_lif metric : average_other_latency template : conf/restperf/9.12.0/fcp_lif.yaml unit : microsec type : average base : other_ops - Harvest Metric : fcp_lif_avg_read_latency Description : Average latency for read operations ZAPI : endpoint : perf-object-get-instances fcp_lif metric : avg_read_latency template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/fcp_lif metric : average_read_latency template : conf/restperf/9.12.0/fcp_lif.yaml unit : microsec type : average base : read_ops - Harvest Metric : fcp_lif_avg_write_latency Description : Average latency for write operations ZAPI : endpoint : perf-object-get-instances fcp_lif metric : avg_write_latency template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/fcp_lif metric : average_write_latency template : conf/restperf/9.12.0/fcp_lif.yaml unit : microsec type : average base : write_ops - Harvest Metric : fcp_lif_other_ops Description : Number of operations that are not read or write. ZAPI : endpoint : perf-object-get-instances fcp_lif metric : other_ops template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : other_ops template : conf/restperf/9.12.0/fcp_lif.yaml unit : per_sec type : rate - Harvest Metric : fcp_lif_read_data Description : Amount of data read from the storage system ZAPI : endpoint : perf-object-get-instances fcp_lif metric : read_data template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : read_data template : conf/restperf/9.12.0/fcp_lif.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_lif_read_ops Description : Number of read operations ZAPI : endpoint : perf-object-get-instances fcp_lif metric : read_ops template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : read_ops template : conf/restperf/9.12.0/fcp_lif.yaml unit : per_sec type : rate - Harvest Metric : fcp_lif_total_ops Description : Total number of operations. ZAPI : endpoint : perf-object-get-instances fcp_lif metric : total_ops template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : total_ops template : conf/restperf/9.12.0/fcp_lif.yaml unit : per_sec type : rate - Harvest Metric : fcp_lif_write_data Description : Amount of data written to the storage system ZAPI : endpoint : perf-object-get-instances fcp_lif metric : write_data template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : write_data template : conf/restperf/9.12.0/fcp_lif.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_lif_write_ops Description : Number of write operations ZAPI : endpoint : perf-object-get-instances fcp_lif metric : write_ops template : conf/zapiperf/cdot/9.8.0/fcp_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp_lif metric : write_ops template : conf/restperf/9.12.0/fcp_lif.yaml unit : per_sec type : rate - Harvest Metric : fcp_link_down Description : Number of times the Fibre Channel link was lost ZAPI : endpoint : perf-object-get-instances fcp_port metric : link_down template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : link.down template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_link_failure Description : Number of link failures ZAPI : endpoint : perf-object-get-instances fcp_port metric : link_failure template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : link_failure template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_loss_of_signal Description : Number of times this port lost signal ZAPI : endpoint : perf-object-get-instances fcp_port metric : loss_of_signal template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : loss_of_signal template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_loss_of_sync Description : Number of times this port lost sync ZAPI : endpoint : perf-object-get-instances fcp_port metric : loss_of_sync template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : loss_of_sync template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_nvmf_avg_other_latency Description : Average latency for operations other than read and write (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_other_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_other_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_other_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf.other_ops - Harvest Metric : fcp_nvmf_avg_read_latency Description : Average latency for read operations (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_read_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_read_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_read_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf.read_ops - Harvest Metric : fcp_nvmf_avg_remote_other_latency Description : Average latency for remote operations other than read and write (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_remote_other_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_remote_other_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_remote_other_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf_remote.other_ops - Harvest Metric : fcp_nvmf_avg_remote_read_latency Description : Average latency for remote read operations (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_remote_read_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_remote_read_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_remote_read_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf_remote.read_ops - Harvest Metric : fcp_nvmf_avg_remote_write_latency Description : Average latency for remote write operations (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_remote_write_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_remote_write_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_remote_write_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf_remote.write_ops - Harvest Metric : fcp_nvmf_avg_write_latency Description : Average latency for write operations (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_avg_write_latency template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : microsec type : average base : nvmf_write_ops REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.average_write_latency template : conf/restperf/9.12.0/fcp.yaml unit : microsec type : average base : nvmf.write_ops - Harvest Metric : fcp_nvmf_caw_data Description : Amount of CAW data sent to the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_caw_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.caw_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_caw_ops Description : Number of FC-NVMe CAW operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_caw_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.caw_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_command_slots Description : Number of command slots that have been used by initiators logging into this port. This shows the command fan-in on the port. ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_command_slots template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.command_slots template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_other_ops Description : Number of NVMF operations that are not read or write. ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_other_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.other_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_read_data Description : Amount of data read from the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_read_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.read_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_read_ops Description : Number of FC-NVMe read operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_read_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.read_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_remote_caw_data Description : Amount of remote CAW data sent to the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_caw_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.caw_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_remote_caw_ops Description : Number of FC-NVMe remote CAW operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_caw_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.caw_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_remote_other_ops Description : Number of NVMF remote operations that are not read or write. ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_other_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.other_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_remote_read_data Description : Amount of remote data read from the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_read_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.read_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_remote_read_ops Description : Number of FC-NVMe remote read operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_read_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.read_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_remote_total_data Description : Amount of remote FC-NVMe traffic to and from the storage system ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_total_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.total_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_remote_total_ops Description : Total number of remote FC-NVMe operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_total_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.total_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_remote_write_data Description : Amount of remote data written to the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_write_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.write_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_remote_write_ops Description : Number of FC-NVMe remote write operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_remote_write_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf_remote.write_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_total_data Description : Amount of FC-NVMe traffic to and from the storage system ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_total_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.total_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_total_ops Description : Total number of FC-NVMe operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_total_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.total_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_nvmf_write_data Description : Amount of data written to the storage system (FC-NVMe) ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_write_data template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.write_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_nvmf_write_ops Description : Number of FC-NVMe write operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : nvmf_write_ops template : conf/zapiperf/cdot/9.10.1/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : nvmf.write_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_other_ops Description : Number of operations that are not read or write. ZAPI : endpoint : perf-object-get-instances fcp_port metric : other_ops template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : other_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_prim_seq_err Description : Number of primitive sequence errors ZAPI : endpoint : perf-object-get-instances fcp_port metric : prim_seq_err template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : primitive_seq_err template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_queue_full Description : Number of times a queue full condition occurred. ZAPI : endpoint : perf-object-get-instances fcp_port metric : queue_full template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta,no-zero-values REST : endpoint : api/cluster/counter/tables/fcp metric : queue_full template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_read_data Description : Amount of data read from the storage system ZAPI : endpoint : perf-object-get-instances fcp_port metric : read_data template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : read_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_read_ops Description : Number of read operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : read_ops template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : read_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_reset_count Description : Number of physical port resets ZAPI : endpoint : perf-object-get-instances fcp_port metric : reset_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : reset_count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_shared_int_count Description : Number of shared interrupts ZAPI : endpoint : perf-object-get-instances fcp_port metric : shared_int_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : shared_interrupt_count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_spurious_int_count Description : Number of spurious interrupts ZAPI : endpoint : perf-object-get-instances fcp_port metric : spurious_int_count template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/fcp metric : spurious_interrupt_count template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_threshold_full Description : Number of times the total number of outstanding commands on the port exceeds the threshold supported by this port. ZAPI : endpoint : perf-object-get-instances fcp_port metric : threshold_full template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : none type : delta,no-zero-values REST : endpoint : api/cluster/counter/tables/fcp metric : threshold_full template : conf/restperf/9.12.0/fcp.yaml unit : none type : delta - Harvest Metric : fcp_total_data Description : Amount of FCP traffic to and from the storage system ZAPI : endpoint : perf-object-get-instances fcp_port metric : total_data template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : total_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_total_ops Description : Total number of FCP operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : total_ops template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : total_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcp_write_data Description : Amount of data written to the storage system ZAPI : endpoint : perf-object-get-instances fcp_port metric : write_data template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : write_data template : conf/restperf/9.12.0/fcp.yaml unit : b_per_sec type : rate - Harvest Metric : fcp_write_ops Description : Number of write operations ZAPI : endpoint : perf-object-get-instances fcp_port metric : write_ops template : conf/zapiperf/cdot/9.8.0/fcp.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcp metric : write_ops template : conf/restperf/9.12.0/fcp.yaml unit : per_sec type : rate - Harvest Metric : fcvi_rdma_write_avg_latency Description : Average RDMA write I/O latency. ZAPI : endpoint : perf-object-get-instances fcvi metric : rdma_write_avg_latency template : conf/zapiperf/cdot/9.8.0/fcvi.yaml unit : microsec type : average base : rdma_write_ops REST : endpoint : api/cluster/counter/tables/fcvi metric : rdma.write_average_latency template : conf/restperf/9.12.0/fcvi.yaml unit : microsec type : average base : rdma.write_ops - Harvest Metric : fcvi_rdma_write_ops Description : Number of RDMA write I/Os issued per second. ZAPI : endpoint : perf-object-get-instances fcvi metric : rdma_write_ops template : conf/zapiperf/cdot/9.8.0/fcvi.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/fcvi metric : rdma.write_ops template : conf/restperf/9.12.0/fcvi.yaml unit : none type : rate - Harvest Metric : fcvi_rdma_write_throughput Description : RDMA write throughput in bytes per second. ZAPI : endpoint : perf-object-get-instances fcvi metric : rdma_write_throughput template : conf/zapiperf/cdot/9.8.0/fcvi.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/fcvi metric : rdma.write_throughput template : conf/restperf/9.12.0/fcvi.yaml unit : b_per_sec type : rate - Harvest Metric : flashcache_accesses Description : External cache accesses per second ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : accesses template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : accesses template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_disk_reads_replaced Description : Estimated number of disk reads per second replaced by cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : disk_reads_replaced template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : disk_reads_replaced template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_evicts Description : Number of blocks evicted from the external cache to make room for new blocks ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : evicts template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : evicts template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit Description : Number of WAFL buffers served off the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.total template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit_directory Description : Number of directory buffers served off the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit_directory template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.directory template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit_indirect Description : Number of indirect file buffers served off the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit_indirect template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.indirect template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit_metadata_file Description : Number of metadata file buffers served off the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit_metadata_file template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.metadata_file template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit_normal_lev0 Description : Number of normal level 0 WAFL buffers served off the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit_normal_lev0 template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.normal_level_zero template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_hit_percent Description : External cache hit rate ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : hit_percent template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : percent type : percent base : accesses REST : endpoint : api/cluster/counter/tables/external_cache metric : hit.percent template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : percent type : average base : accesses - Harvest Metric : flashcache_inserts Description : Number of WAFL buffers inserted into the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : inserts template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : inserts template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_invalidates Description : Number of blocks invalidated in the external cache ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : invalidates template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : invalidates template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_miss Description : External cache misses ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : miss template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : miss.total template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_miss_directory Description : External cache misses accessing directory buffers ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : miss_directory template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : miss.directory template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_miss_indirect Description : External cache misses accessing indirect file buffers ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : miss_indirect template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : miss.indirect template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_miss_metadata_file Description : External cache misses accessing metadata file buffers ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : miss_metadata_file template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : miss.metadata_file template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_miss_normal_lev0 Description : External cache misses accessing normal level 0 buffers ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : miss_normal_lev0 template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/external_cache metric : miss.normal_level_zero template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : per_sec type : rate - Harvest Metric : flashcache_usage Description : Percentage of blocks in external cache currently containing valid data ZAPI : endpoint : perf-object-get-instances ext_cache_obj metric : usage template : conf/zapiperf/cdot/9.8.0/ext_cache_obj.yaml unit : percent type : raw REST : endpoint : api/cluster/counter/tables/external_cache metric : usage template : conf/restperf/9.12.0/ext_cache_obj.yaml unit : percent type : raw - Harvest Metric : flashpool_cache_stats Description : Automated Working-set Analyzer (AWA) per-interval pseudo cache statistics for the most recent intervals. The number of intervals defined as recent is CM_WAFL_HYAS_INT_DIS_CNT. This array is a table with fields corresponding to the enum type of hyas_cache_stat_type_t. ZAPI : endpoint : perf-object-get-instances wafl_hya_sizer metric : cache_stats template : conf/zapiperf/cdot/9.8.0/wafl_hya_sizer.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/wafl_hya_sizer metric : cache_stats template : conf/restperf/9.12.0/wafl_hya_sizer.yaml unit : none type : raw - Harvest Metric : flashpool_evict_destage_rate Description : Number of block destage per second. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : evict_destage_rate template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : evict_destage_rate template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_evict_remove_rate Description : Number of block free per second. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : evict_remove_rate template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : evict_remove_rate template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_hya_read_hit_latency_average Description : Average of RAID I/O latency on read hit. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : hya_read_hit_latency_average template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_read_hit_latency_count REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : hya_read_hit_latency_average template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_read_hit_latency_count - Harvest Metric : flashpool_hya_read_miss_latency_average Description : Average read miss latency. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : hya_read_miss_latency_average template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_read_miss_latency_count REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : hya_read_miss_latency_average template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_read_miss_latency_count - Harvest Metric : flashpool_hya_write_hdd_latency_average Description : Average write latency to HDD. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : hya_write_hdd_latency_average template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_write_hdd_latency_count REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : hya_write_hdd_latency_average template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_write_hdd_latency_count - Harvest Metric : flashpool_hya_write_ssd_latency_average Description : Average of RAID I/O latency on write to SSD. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : hya_write_ssd_latency_average template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_write_ssd_latency_count REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : hya_write_ssd_latency_average template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : average base : hya_write_ssd_latency_count - Harvest Metric : flashpool_read_cache_ins_rate Description : Cache insert rate blocks/sec. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : read_cache_ins_rate template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : read_cache_insert_rate template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_read_ops_replaced Description : Number of HDD read operations replaced by SSD reads per second. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : read_ops_replaced template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : read_ops_replaced template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_read_ops_replaced_percent Description : Percentage of HDD read operations replace by SSD. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : read_ops_replaced_percent template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : percent type : average base : read_ops_total REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : read_ops_replaced_percent template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : percent type : average base : read_ops_total - Harvest Metric : flashpool_ssd_available Description : Total SSD blocks available. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : ssd_available template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : ssd_available template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : raw - Harvest Metric : flashpool_ssd_read_cached Description : Total read cached SSD blocks. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : ssd_read_cached template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : ssd_read_cached template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : raw - Harvest Metric : flashpool_ssd_total Description : Total SSD blocks. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : ssd_total template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : ssd_total template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : raw - Harvest Metric : flashpool_ssd_total_used Description : Total SSD blocks used. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : ssd_total_used template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : ssd_total_used template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : raw - Harvest Metric : flashpool_ssd_write_cached Description : Total write cached SSD blocks. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : ssd_write_cached template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : ssd_write_cached template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : none type : raw - Harvest Metric : flashpool_wc_write_blks_total Description : Number of write-cache blocks written per second. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : wc_write_blks_total template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : wc_write_blocks_total template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_write_blks_replaced Description : Number of HDD write blocks replaced by SSD writes per second. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : write_blks_replaced template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : write_blocks_replaced template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : per_sec type : rate - Harvest Metric : flashpool_write_blks_replaced_percent Description : Percentage of blocks overwritten to write-cache among all disk writes. ZAPI : endpoint : perf-object-get-instances wafl_hya_per_aggr metric : write_blks_replaced_percent template : conf/zapiperf/cdot/9.8.0/wafl_hya_per_aggr.yaml unit : percent type : average base : est_write_blks_total REST : endpoint : api/cluster/counter/tables/wafl_hya_per_aggregate metric : write_blocks_replaced_percent template : conf/restperf/9.12.0/wafl_hya_per_aggr.yaml unit : percent type : average base : estimated_write_blocks_total - Harvest Metric : headroom_aggr_current_latency Description : This is the storage aggregate average latency per message at the disk level. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : current_latency template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : microsec type : average base : current_ops REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : current_latency template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : microsec type : average base : current_ops - Harvest Metric : headroom_aggr_current_ops Description : Total number of I/Os processed by the aggregate per second. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : current_ops template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : per_sec type : rate REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : current_ops template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : per_sec type : rate - Harvest Metric : headroom_aggr_current_utilization Description : This is the storage aggregate average utilization of all the data disks in the aggregate. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : current_utilization template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : percent type : percent base : current_utilization_total REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : current_utilization template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : percent type : percent base : current_utilization_denominator - Harvest Metric : headroom_aggr_ewma_daily Description : Daily exponential weighted moving average. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : ewma_daily template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : ewma.daily template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : raw - Harvest Metric : headroom_aggr_ewma_hourly Description : Hourly exponential weighted moving average. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : ewma_hourly template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : ewma.hourly template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : raw - Harvest Metric : headroom_aggr_ewma_monthly Description : Monthly exponential weighted moving average. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : ewma_monthly template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : ewma.monthly template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : raw - Harvest Metric : headroom_aggr_ewma_weekly Description : Weekly exponential weighted moving average. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : ewma_weekly template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : ewma.weekly template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : raw - Harvest Metric : headroom_aggr_optimal_point_confidence_factor Description : The confidence factor for the optimal point value based on the observed resource latency and utilization. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : optimal_point_confidence_factor template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : optimal_point.confidence_factor template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : average base : optimal_point.samples - Harvest Metric : headroom_aggr_optimal_point_latency Description : The latency component of the optimal point of the latency/utilization curve. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : optimal_point_latency template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : microsec type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : optimal_point.latency template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : microsec type : average base : optimal_point.samples - Harvest Metric : headroom_aggr_optimal_point_ops Description : The ops component of the optimal point derived from the latency/utilzation curve. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : optimal_point_ops template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : per_sec type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : optimal_point.ops template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : per_sec type : average base : optimal_point.samples - Harvest Metric : headroom_aggr_optimal_point_utilization Description : The utilization component of the optimal point of the latency/utilization curve. ZAPI : endpoint : perf-object-get-instances resource_headroom_aggr metric : optimal_point_utilization template : conf/zapiperf/cdot/9.8.0/resource_headroom_aggr.yaml unit : none type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_aggregate metric : optimal_point.utilization template : conf/restperf/9.12.0/resource_headroom_aggr.yaml unit : none type : average base : optimal_point.samples - Harvest Metric : headroom_cpu_current_latency Description : Current operation latency of the resource. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : current_latency template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : microsec type : average base : current_ops REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : current_latency template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : microsec type : average base : current_ops - Harvest Metric : headroom_cpu_current_ops Description : Total number of operations per second (also referred to as dblade ops). ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : current_ops template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : per_sec type : rate REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : current_ops template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : per_sec type : rate - Harvest Metric : headroom_cpu_current_utilization Description : Average processor utilization across all processors in the system. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : current_utilization template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : percent type : percent base : current_utilization_total REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : current_utilization template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : percent type : percent base : elapsed_time - Harvest Metric : headroom_cpu_ewma_daily Description : Daily exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : ewma_daily template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : ewma.daily template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : raw - Harvest Metric : headroom_cpu_ewma_hourly Description : Hourly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : ewma_hourly template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : ewma.hourly template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : raw - Harvest Metric : headroom_cpu_ewma_monthly Description : Monthly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : ewma_monthly template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : ewma.monthly template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : raw - Harvest Metric : headroom_cpu_ewma_weekly Description : Weekly exponential weighted moving average for current_ops, optimal_point_ops, current_latency, optimal_point_latency, current_utilization, optimal_point_utilization and optimal_point_confidence_factor. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : ewma_weekly template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : raw REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : ewma.weekly template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : raw - Harvest Metric : headroom_cpu_optimal_point_confidence_factor Description : Confidence factor for the optimal point value based on the observed resource latency and utilization. The possible values are : 0 - unknown, 1 - low, 2 - medium, 3 - high. This counter can provide an average confidence factor over a range of time. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : optimal_point_confidence_factor template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : optimal_point.confidence_factor template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : average base : optimal_point.samples - Harvest Metric : headroom_cpu_optimal_point_latency Description : Latency component of the optimal point of the latency/utilization curve. This counter can provide an average latency over a range of time. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : optimal_point_latency template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : microsec type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : optimal_point.latency template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : microsec type : average base : optimal_point.samples - Harvest Metric : headroom_cpu_optimal_point_ops Description : Ops component of the optimal point derived from the latency/utilization curve. This counter can provide an average ops over a range of time. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : optimal_point_ops template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : per_sec type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : optimal_point.ops template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : per_sec type : average base : optimal_point.samples - Harvest Metric : headroom_cpu_optimal_point_utilization Description : Utilization component of the optimal point of the latency/utilization curve. This counter can provide an average utilization over a range of time. ZAPI : endpoint : perf-object-get-instances resource_headroom_cpu metric : optimal_point_utilization template : conf/zapiperf/cdot/9.8.0/resource_headroom_cpu.yaml unit : none type : average base : optimal_point_samples REST : endpoint : /api/cluster/counter/tables/headroom_cpu metric : optimal_point.utilization template : conf/restperf/9.12.0/resource_headroom_cpu.yaml unit : none type : average base : optimal_point.samples - Harvest Metric : hostadapter_bytes_read Description : Bytes read through a host adapter ZAPI : endpoint : perf-object-get-instances hostadapter metric : bytes_read template : conf/zapiperf/cdot/9.8.0/hostadapter.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/host_adapter metric : bytes_read template : conf/restperf/9.12.0/hostadapter.yaml unit : per_sec type : rate - Harvest Metric : hostadapter_bytes_written Description : Bytes written through a host adapter ZAPI : endpoint : perf-object-get-instances hostadapter metric : bytes_written template : conf/zapiperf/cdot/9.8.0/hostadapter.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/host_adapter metric : bytes_written template : conf/restperf/9.12.0/hostadapter.yaml unit : per_sec type : rate - Harvest Metric : iscsi_lif_avg_latency Description : Average latency for iSCSI operations ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : avg_latency template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : microsec type : average base : cmd_transfered REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : average_latency template : conf/restperf/9.12.0/iscsi_lif.yaml unit : microsec type : average base : cmd_transferred - Harvest Metric : iscsi_lif_avg_other_latency Description : Average latency for operations other than read and write (for example, Inquiry, Report LUNs, SCSI Task Management Functions) ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : avg_other_latency template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_other_ops REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : average_other_latency template : conf/restperf/9.12.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_other_ops - Harvest Metric : iscsi_lif_avg_read_latency Description : Average latency for read operations ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : avg_read_latency template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_read_ops REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : average_read_latency template : conf/restperf/9.12.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_read_ops - Harvest Metric : iscsi_lif_avg_write_latency Description : Average latency for write operations ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : avg_write_latency template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_write_ops REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : average_write_latency template : conf/restperf/9.12.0/iscsi_lif.yaml unit : microsec type : average base : iscsi_write_ops - Harvest Metric : iscsi_lif_cmd_transfered Description : Command transfered by this iSCSI conn ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : cmd_transfered template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : none type : rate - Harvest Metric : iscsi_lif_cmd_transferred Description : Command transferred by this iSCSI connection REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : cmd_transferred template : conf/restperf/9.12.0/iscsi_lif.yaml unit : none type : rate - Harvest Metric : iscsi_lif_iscsi_other_ops Description : iSCSI other operations per second on this logical interface (LIF) ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : iscsi_other_ops template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : iscsi_other_ops template : conf/restperf/9.12.0/iscsi_lif.yaml unit : per_sec type : rate - Harvest Metric : iscsi_lif_iscsi_read_ops Description : iSCSI read operations per second on this logical interface (LIF) ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : iscsi_read_ops template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : iscsi_read_ops template : conf/restperf/9.12.0/iscsi_lif.yaml unit : per_sec type : rate - Harvest Metric : iscsi_lif_iscsi_write_ops Description : iSCSI write operations per second on this logical interface (LIF) ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : iscsi_write_ops template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : iscsi_write_ops template : conf/restperf/9.12.0/iscsi_lif.yaml unit : per_sec type : rate - Harvest Metric : iscsi_lif_protocol_errors Description : Number of protocol errors from iSCSI sessions on this logical interface (LIF) ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : protocol_errors template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : protocol_errors template : conf/restperf/9.12.0/iscsi_lif.yaml unit : none type : delta - Harvest Metric : iscsi_lif_read_data Description : Amount of data read from the storage system in bytes ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : read_data template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : read_data template : conf/restperf/9.12.0/iscsi_lif.yaml unit : b_per_sec type : rate - Harvest Metric : iscsi_lif_write_data Description : Amount of data written to the storage system in bytes ZAPI : endpoint : perf-object-get-instances iscsi_lif metric : write_data template : conf/zapiperf/cdot/9.8.0/iscsi_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/iscsi_lif metric : write_data template : conf/restperf/9.12.0/iscsi_lif.yaml unit : b_per_sec type : rate - Harvest Metric : lif_recv_data Description : Number of bytes received per second ZAPI : endpoint : perf-object-get-instances lif metric : recv_data template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : received_data template : conf/restperf/9.12.0/lif.yaml unit : b_per_sec type : rate - Harvest Metric : lif_recv_errors Description : Number of received Errors per second ZAPI : endpoint : perf-object-get-instances lif metric : recv_errors template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : received_errors template : conf/restperf/9.12.0/lif.yaml unit : per_sec type : rate - Harvest Metric : lif_recv_packet Description : Number of packets received per second ZAPI : endpoint : perf-object-get-instances lif metric : recv_packet template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : received_packets template : conf/restperf/9.12.0/lif.yaml unit : per_sec type : rate - Harvest Metric : lif_sent_data Description : Number of bytes sent per second ZAPI : endpoint : perf-object-get-instances lif metric : sent_data template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : sent_data template : conf/restperf/9.12.0/lif.yaml unit : b_per_sec type : rate - Harvest Metric : lif_sent_errors Description : Number of sent errors per second ZAPI : endpoint : perf-object-get-instances lif metric : sent_errors template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : sent_errors template : conf/restperf/9.12.0/lif.yaml unit : per_sec type : rate - Harvest Metric : lif_sent_packet Description : Number of packets sent per second ZAPI : endpoint : perf-object-get-instances lif metric : sent_packet template : conf/zapiperf/cdot/9.8.0/lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lif metric : sent_packets template : conf/restperf/9.12.0/lif.yaml unit : per_sec type : rate - Harvest Metric : lun_avg_read_latency Description : Average read latency in microseconds for all operations on the LUN ZAPI : endpoint : perf-object-get-instances lun metric : avg_read_latency template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/lun metric : average_read_latency template : conf/restperf/9.12.0/lun.yaml unit : microsec type : average base : read_ops - Harvest Metric : lun_avg_write_latency Description : Average write latency in microseconds for all operations on the LUN ZAPI : endpoint : perf-object-get-instances lun metric : avg_write_latency template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/lun metric : average_write_latency template : conf/restperf/9.12.0/lun.yaml unit : microsec type : average base : write_ops - Harvest Metric : lun_avg_xcopy_latency Description : Average latency in microseconds for xcopy requests ZAPI : endpoint : perf-object-get-instances lun metric : avg_xcopy_latency template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : microsec type : average base : xcopy_reqs REST : endpoint : api/cluster/counter/tables/lun metric : average_xcopy_latency template : conf/restperf/9.12.0/lun.yaml unit : microsec type : average base : xcopy_requests - Harvest Metric : lun_caw_reqs Description : Number of compare and write requests ZAPI : endpoint : perf-object-get-instances lun metric : caw_reqs template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/lun metric : caw_requests template : conf/restperf/9.12.0/lun.yaml unit : none type : rate - Harvest Metric : lun_enospc Description : Number of operations receiving ENOSPC errors ZAPI : endpoint : perf-object-get-instances lun metric : enospc template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/lun metric : enospc template : conf/restperf/9.12.0/lun.yaml unit : none type : delta - Harvest Metric : lun_queue_full Description : Queue full responses ZAPI : endpoint : perf-object-get-instances lun metric : queue_full template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : queue_full template : conf/restperf/9.12.0/lun.yaml unit : per_sec type : rate - Harvest Metric : lun_read_align_histo Description : Histogram of WAFL read alignment (number sectors off WAFL block start) ZAPI : endpoint : perf-object-get-instances lun metric : read_align_histo template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : percent type : percent base : read_ops_sent REST : endpoint : api/cluster/counter/tables/lun metric : read_align_histogram template : conf/restperf/9.12.0/lun.yaml unit : percent type : percent base : read_ops_sent - Harvest Metric : lun_read_data Description : Read bytes ZAPI : endpoint : perf-object-get-instances lun metric : read_data template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : read_data template : conf/restperf/9.12.0/lun.yaml unit : b_per_sec type : rate - Harvest Metric : lun_read_ops Description : Number of read operations ZAPI : endpoint : perf-object-get-instances lun metric : read_ops template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : read_ops template : conf/restperf/9.12.0/lun.yaml unit : per_sec type : rate - Harvest Metric : lun_read_partial_blocks Description : Percentage of reads whose size is not a multiple of WAFL block size ZAPI : endpoint : perf-object-get-instances lun metric : read_partial_blocks template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : percent type : percent base : read_ops REST : endpoint : api/cluster/counter/tables/lun metric : read_partial_blocks template : conf/restperf/9.12.0/lun.yaml unit : percent type : percent base : read_ops - Harvest Metric : lun_remote_bytes Description : I/O to or from a LUN which is not owned by the storage system handling the I/O. ZAPI : endpoint : perf-object-get-instances lun metric : remote_bytes template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : remote_bytes template : conf/restperf/9.12.0/lun.yaml unit : b_per_sec type : rate - Harvest Metric : lun_remote_ops Description : Number of operations received by a storage system that does not own the LUN targeted by the operations. ZAPI : endpoint : perf-object-get-instances lun metric : remote_ops template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : remote_ops template : conf/restperf/9.12.0/lun.yaml unit : per_sec type : rate - Harvest Metric : lun_size Description : The total provisioned size of the LUN. The LUN size can be increased but not be made smaller using the REST interface.<br/>The maximum and minimum sizes listed here are the absolute maximum and absolute minimum sizes in bytes. The actual minimum and maxiumum sizes vary depending on the ONTAP version, ONTAP platform and the available space in the containing volume and aggregate.<br/>For more information, see _Size properties_ in the _docs_ section of the ONTAP REST API documentation. ZAPI : endpoint : lun-get-iter metric : lun-info.size template : conf/zapi/cdot/9.8.0/lun.yaml REST : endpoint : api/storage/luns metric : space.size template : conf/rest/9.12.0/lun.yaml - Harvest Metric : lun_size_used Description : The amount of space consumed by the main data stream of the LUN.<br/>This value is the total space consumed in the volume by the LUN, including filesystem overhead, but excluding prefix and suffix streams. Due to internal filesystem overhead and the many ways SAN filesystems and applications utilize blocks within a LUN, this value does not necessarily reflect actual consumption/availability from the perspective of the filesystem or application. Without specific knowledge of how the LUN blocks are utilized outside of ONTAP, this property should not be used as an indicator for an out-of-space condition.<br/>For more information, see _Size properties_ in the _docs_ section of the ONTAP REST API documentation. ZAPI : endpoint : lun-get-iter metric : lun-info.size-used template : conf/zapi/cdot/9.8.0/lun.yaml REST : endpoint : api/storage/luns metric : space.used template : conf/rest/9.12.0/lun.yaml - Harvest Metric : lun_unmap_reqs Description : Number of unmap command requests ZAPI : endpoint : perf-object-get-instances lun metric : unmap_reqs template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/lun metric : unmap_requests template : conf/restperf/9.12.0/lun.yaml unit : none type : rate - Harvest Metric : lun_write_align_histo Description : Histogram of WAFL write alignment (number of sectors off WAFL block start) ZAPI : endpoint : perf-object-get-instances lun metric : write_align_histo template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : percent type : percent base : write_ops_sent REST : endpoint : api/cluster/counter/tables/lun metric : write_align_histogram template : conf/restperf/9.12.0/lun.yaml unit : percent type : percent base : write_ops_sent - Harvest Metric : lun_write_data Description : Write bytes ZAPI : endpoint : perf-object-get-instances lun metric : write_data template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : write_data template : conf/restperf/9.12.0/lun.yaml unit : b_per_sec type : rate - Harvest Metric : lun_write_ops Description : Number of write operations ZAPI : endpoint : perf-object-get-instances lun metric : write_ops template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/lun metric : write_ops template : conf/restperf/9.12.0/lun.yaml unit : per_sec type : rate - Harvest Metric : lun_write_partial_blocks Description : Percentage of writes whose size is not a multiple of WAFL block size ZAPI : endpoint : perf-object-get-instances lun metric : write_partial_blocks template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : percent type : percent base : write_ops REST : endpoint : api/cluster/counter/tables/lun metric : write_partial_blocks template : conf/restperf/9.12.0/lun.yaml unit : percent type : percent base : write_ops - Harvest Metric : lun_writesame_reqs Description : Number of write same command requests ZAPI : endpoint : perf-object-get-instances lun metric : writesame_reqs template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/lun metric : writesame_requests template : conf/restperf/9.12.0/lun.yaml unit : none type : rate - Harvest Metric : lun_writesame_unmap_reqs Description : Number of write same commands requests with unmap bit set ZAPI : endpoint : perf-object-get-instances lun metric : writesame_unmap_reqs template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/lun metric : writesame_unmap_requests template : conf/restperf/9.12.0/lun.yaml unit : none type : rate - Harvest Metric : lun_xcopy_reqs Description : Total number of xcopy operations on the LUN ZAPI : endpoint : perf-object-get-instances lun metric : xcopy_reqs template : conf/zapiperf/cdot/9.8.0/lun.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/lun metric : xcopy_requests template : conf/restperf/9.12.0/lun.yaml unit : none type : rate - Harvest Metric : namespace_avg_other_latency Description : Average other ops latency in microseconds for all operations on the Namespace ZAPI : endpoint : perf-object-get-instances namespace metric : avg_other_latency template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/namespace metric : average_other_latency template : conf/restperf/9.12.0/namespace.yaml unit : microsec type : average base : other_ops - Harvest Metric : namespace_avg_read_latency Description : Average read latency in microseconds for all operations on the Namespace ZAPI : endpoint : perf-object-get-instances namespace metric : avg_read_latency template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/namespace metric : average_read_latency template : conf/restperf/9.12.0/namespace.yaml unit : microsec type : average base : read_ops - Harvest Metric : namespace_avg_write_latency Description : Average write latency in microseconds for all operations on the Namespace ZAPI : endpoint : perf-object-get-instances namespace metric : avg_write_latency template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/namespace metric : average_write_latency template : conf/restperf/9.12.0/namespace.yaml unit : microsec type : average base : write_ops - Harvest Metric : namespace_other_ops Description : Number of other operations ZAPI : endpoint : perf-object-get-instances namespace metric : other_ops template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/namespace metric : other_ops template : conf/restperf/9.12.0/namespace.yaml unit : per_sec type : rate - Harvest Metric : namespace_read_data Description : Read bytes ZAPI : endpoint : perf-object-get-instances namespace metric : read_data template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/namespace metric : read_data template : conf/restperf/9.12.0/namespace.yaml unit : b_per_sec type : rate - Harvest Metric : namespace_read_ops Description : Number of read operations ZAPI : endpoint : perf-object-get-instances namespace metric : read_ops template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/namespace metric : read_ops template : conf/restperf/9.12.0/namespace.yaml unit : per_sec type : rate - Harvest Metric : namespace_remote_bytes Description : Remote read bytes ZAPI : endpoint : perf-object-get-instances namespace metric : remote_bytes template : conf/zapiperf/cdot/9.10.1/namespace.yaml REST : endpoint : api/cluster/counter/tables/namespace metric : remote.read_data template : conf/restperf/9.12.0/namespace.yaml unit : b_per_sec type : rate - Harvest Metric : namespace_remote_ops Description : Number of remote read operations ZAPI : endpoint : perf-object-get-instances namespace metric : remote_ops template : conf/zapiperf/cdot/9.10.1/namespace.yaml REST : endpoint : api/cluster/counter/tables/namespace metric : remote.read_ops template : conf/restperf/9.12.0/namespace.yaml unit : per_sec type : rate - Harvest Metric : namespace_write_data Description : Write bytes ZAPI : endpoint : perf-object-get-instances namespace metric : write_data template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/namespace metric : write_data template : conf/restperf/9.12.0/namespace.yaml unit : b_per_sec type : rate - Harvest Metric : namespace_write_ops Description : Number of write operations ZAPI : endpoint : perf-object-get-instances namespace metric : write_ops template : conf/zapiperf/cdot/9.10.1/namespace.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/namespace metric : write_ops template : conf/restperf/9.12.0/namespace.yaml unit : per_sec type : rate - Harvest Metric : net_port_mtu Description : Maximum transmission unit, largest packet size on this network ZAPI : endpoint : net-port-get-iter metric : net-port-info.mtu template : conf/zapi/cdot/9.8.0/netPort.yaml REST : endpoint : api/network/ethernet/ports metric : mtu template : conf/rest/9.12.0/netPort.yaml - Harvest Metric : netstat_bytes_recvd Description : Number of bytes received by a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : bytes_recvd template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_bytes_sent Description : Number of bytes sent by a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : bytes_sent template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_cong_win Description : Congestion window of a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : cong_win template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_cong_win_th Description : Congestion window threshold of a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : cong_win_th template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_ooorcv_pkts Description : Number of out-of-order packets received by this TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : ooorcv_pkts template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_recv_window Description : Receive window size of a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : recv_window template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_rexmit_pkts Description : Number of packets retransmitted by this TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : rexmit_pkts template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : netstat_send_window Description : Send window size of a TCP connection ZAPI : endpoint : perf-object-get-instances netstat metric : send_window template : conf/zapiperf/cdot/9.8.0/netstat.yaml unit : none type : raw - Harvest Metric : nfs_clients_idle_duration Description : Specifies an ISO-8601 format of date and time to retrieve the idle time duration in hours, minutes, and seconds format. REST : endpoint : api/protocols/nfs/connected-clients metric : idle_duration template : conf/rest/9.7.0/nfs_clients.yaml - Harvest Metric : nfs_diag_storePool_ByteLockAlloc Description : Current number of byte range lock objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ByteLockAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.byte_lock_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_ByteLockMax Description : Maximum number of byte range lock objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ByteLockMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.byte_lock_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_ClientAlloc Description : Current number of client objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ClientAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.client_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_ClientMax Description : Maximum number of client objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ClientMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.client_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_ConnectionParentSessionReferenceAlloc Description : Current number of connection parent session reference objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ConnectionParentSessionReferenceAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.connection_parent_session_reference_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_ConnectionParentSessionReferenceMax Description : Maximum number of connection parent session reference objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_ConnectionParentSessionReferenceMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.connection_parent_session_reference_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_CopyStateAlloc Description : Current number of copy state objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_CopyStateAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.copy_state_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_CopyStateMax Description : Maximum number of copy state objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_CopyStateMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.copy_state_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_DelegAlloc Description : Current number of delegation lock objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_DelegAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.delegation_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_DelegMax Description : Maximum number delegation lock objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_DelegMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.delegation_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_DelegStateAlloc Description : Current number of delegation state objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_DelegStateAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.delegation_state_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_DelegStateMax Description : Maximum number of delegation state objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_DelegStateMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.delegation_state_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LayoutAlloc Description : Current number of layout objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LayoutAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.layout_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LayoutMax Description : Maximum number of layout objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LayoutMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.layout_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LayoutStateAlloc Description : Current number of layout state objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LayoutStateAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.layout_state_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LayoutStateMax Description : Maximum number of layout state objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LayoutStateMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.layout_state_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LockStateAlloc Description : Current number of lock state objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LockStateAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.lock_state_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_LockStateMax Description : Maximum number of lock state objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_LockStateMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.lock_state_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OpenAlloc Description : Current number of share objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OpenAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.open_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OpenMax Description : Maximum number of share lock objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OpenMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.open_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OpenStateAlloc Description : Current number of open state objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OpenStateAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.openstate_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OpenStateMax Description : Maximum number of open state objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OpenStateMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.openstate_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OwnerAlloc Description : Current number of owner objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OwnerAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.owner_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_OwnerMax Description : Maximum number of owner objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_OwnerMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.owner_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionAlloc Description : Current number of session objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionConnectionHolderAlloc Description : Current number of session connection holder objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionConnectionHolderAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_connection_holder_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionConnectionHolderMax Description : Maximum number of session connection holder objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionConnectionHolderMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_connection_holder_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionHolderAlloc Description : Current number of session holder objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionHolderAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_holder_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionHolderMax Description : Maximum number of session holder objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionHolderMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_holder_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_SessionMax Description : Maximum number of session objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_SessionMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.session_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_StateRefHistoryAlloc Description : Current number of state reference callstack history objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_StateRefHistoryAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.state_reference_history_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_StateRefHistoryMax Description : Maximum number of state reference callstack history objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_StateRefHistoryMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.state_reference_history_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_StringAlloc Description : Current number of string objects allocated. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_StringAlloc template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.string_allocated template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nfs_diag_storePool_StringMax Description : Maximum number of string objects. ZAPI : endpoint : perf-object-get-instances nfsv4_diag metric : storePool_StringMax template : conf/zapiperf/cdot/9.8.0/nfsv4_pool.yaml unit : none type : raw,no-zero-values REST : endpoint : api/cluster/counter/tables/nfs_v4_diag metric : storepool.string_maximum template : conf/restperf/9.12.0/nfsv4_pool.yaml unit : none type : raw - Harvest Metric : nic_link_up_to_downs Description : Number of link state change from UP to DOWN. ZAPI : endpoint : perf-object-get-instances nic_common metric : link_up_to_downs template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : link_up_to_down template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_rx_alignment_errors Description : Alignment errors detected on received packets ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_alignment_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_alignment_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_rx_bytes Description : Bytes received ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_bytes template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_bytes template : conf/restperf/9.12.0/nic_common.yaml unit : b_per_sec type : rate - Harvest Metric : nic_rx_crc_errors Description : CRC errors detected on received packets ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_crc_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_crc_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_rx_errors Description : Error received ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_errors template : conf/restperf/9.12.0/nic_common.yaml unit : b_per_sec type : rate - Harvest Metric : nic_rx_length_errors Description : Length errors detected on received packets ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_length_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_length_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_rx_total_errors Description : Total errors received ZAPI : endpoint : perf-object-get-instances nic_common metric : rx_total_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : receive_total_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_tx_bytes Description : Bytes sent ZAPI : endpoint : perf-object-get-instances nic_common metric : tx_bytes template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nic_common metric : transmit_bytes template : conf/restperf/9.12.0/nic_common.yaml unit : b_per_sec type : rate - Harvest Metric : nic_tx_errors Description : Error sent ZAPI : endpoint : perf-object-get-instances nic_common metric : tx_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nic_common metric : transmit_errors template : conf/restperf/9.12.0/nic_common.yaml unit : b_per_sec type : rate - Harvest Metric : nic_tx_hw_errors Description : Transmit errors reported by hardware ZAPI : endpoint : perf-object-get-instances nic_common metric : tx_hw_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : transmit_hw_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : nic_tx_total_errors Description : Total errors sent ZAPI : endpoint : perf-object-get-instances nic_common metric : tx_total_errors template : conf/zapiperf/cdot/9.8.0/nic_common.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/nic_common metric : transmit_total_errors template : conf/restperf/9.12.0/nic_common.yaml unit : none type : delta - Harvest Metric : node_avg_processor_busy Description : Average processor utilization across all processors in the system ZAPI : endpoint : perf-object-get-instances system:node metric : avg_processor_busy template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time REST : endpoint : api/cluster/counter/tables/system:node metric : average_processor_busy_percent template : conf/restperf/9.12.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time - Harvest Metric : node_cifs_connections Description : Number of connections ZAPI : endpoint : perf-object-get-instances cifs:node metric : connections template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : connections template : conf/restperf/9.12.0/cifs_node.yaml unit : none type : raw - Harvest Metric : node_cifs_established_sessions Description : Number of established SMB and SMB2 sessions ZAPI : endpoint : perf-object-get-instances cifs:node metric : established_sessions template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : established_sessions template : conf/restperf/9.12.0/cifs_node.yaml unit : none type : raw - Harvest Metric : node_cifs_latency Description : Average latency for CIFS operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_latency template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : microsec type : average base : cifs_latency_base REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : latency template : conf/restperf/9.12.0/cifs_node.yaml unit : microsec type : average base : latency_base - Harvest Metric : node_cifs_op_count Description : Array of select CIFS operation counts ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_op_count template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : op_count template : conf/restperf/9.12.0/cifs_node.yaml unit : none type : rate - Harvest Metric : node_cifs_open_files Description : Number of open files over SMB and SMB2 ZAPI : endpoint : perf-object-get-instances cifs:node metric : open_files template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : open_files template : conf/restperf/9.12.0/cifs_node.yaml unit : none type : raw - Harvest Metric : node_cifs_ops Description : Number of CIFS operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : cifs_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : cifs_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_cifs_read_latency Description : Average latency for CIFS read operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_read_latency template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : microsec type : average base : cifs_read_ops REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : average_read_latency template : conf/restperf/9.12.0/cifs_node.yaml unit : microsec type : average base : total_read_ops - Harvest Metric : node_cifs_read_ops Description : Total number of CIFS read operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_read_ops template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : total_read_ops template : conf/restperf/9.12.0/cifs_node.yaml unit : per_sec type : rate - Harvest Metric : node_cifs_total_ops Description : Total number of CIFS operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_ops template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : total_ops template : conf/restperf/9.12.0/cifs_node.yaml unit : per_sec type : rate - Harvest Metric : node_cifs_write_latency Description : Average latency for CIFS write operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_write_latency template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : microsec type : average base : cifs_write_ops REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : average_write_latency template : conf/restperf/9.12.0/cifs_node.yaml unit : microsec type : average base : total_write_ops - Harvest Metric : node_cifs_write_ops Description : Total number of CIFS write operations ZAPI : endpoint : perf-object-get-instances cifs:node metric : cifs_write_ops template : conf/zapiperf/cdot/9.8.0/cifs_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs:node metric : total_write_ops template : conf/restperf/9.12.0/cifs_node.yaml unit : per_sec type : rate - Harvest Metric : node_cpu_busy Description : System CPU resource utilization. Returns a computed percentage for the default CPU field. Basically computes a 'cpu usage summary' value which indicates how 'busy' the system is based upon the most heavily utilized domain. The idea is to determine the amount of available CPU until we're limited by either a domain maxing out OR we exhaust all available idle CPU cycles, whichever occurs first. ZAPI : endpoint : perf-object-get-instances system:node metric : cpu_busy template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time REST : endpoint : api/cluster/counter/tables/system:node metric : cpu_busy template : conf/restperf/9.12.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time - Harvest Metric : node_cpu_busytime Description : The time (in hundredths of a second) that the CPU has been doing useful work since the last boot ZAPI : endpoint : system-node-get-iter metric : node-details-info.cpu-busytime template : conf/zapi/cdot/9.8.0/node.yaml REST : endpoint : api/private/cli/node metric : cpu_busy_time template : conf/rest/9.12.0/node.yaml - Harvest Metric : node_cpu_domain_busy Description : Array of processor time in percentage spent in various domains ZAPI : endpoint : perf-object-get-instances system:node metric : domain_busy template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time REST : endpoint : api/cluster/counter/tables/system:node metric : domain_busy template : conf/restperf/9.12.0/system_node.yaml unit : percent type : percent base : cpu_elapsed_time - Harvest Metric : node_cpu_elapsed_time Description : Elapsed time since boot ZAPI : endpoint : perf-object-get-instances system:node metric : cpu_elapsed_time template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : none type : delta,no-display REST : endpoint : api/cluster/counter/tables/system:node metric : cpu_elapsed_time template : conf/restperf/9.12.0/system_node.yaml unit : microsec type : delta - Harvest Metric : node_disk_data_read Description : Number of disk kilobytes (KB) read per second ZAPI : endpoint : perf-object-get-instances system:node metric : disk_data_read template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : disk_data_read template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_disk_data_written Description : Number of disk kilobytes (KB) written per second ZAPI : endpoint : perf-object-get-instances system:node metric : disk_data_written template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : disk_data_written template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_failed_fan Description : Specifies a count of the number of chassis fans that are not operating within the recommended RPM range. ZAPI : endpoint : system-node-get-iter metric : node-details-info.env-failed-fan-count template : conf/zapi/cdot/9.8.0/node.yaml REST : endpoint : api/cluster/nodes metric : controller.failed_fan.count template : conf/rest/9.12.0/node.yaml - Harvest Metric : node_failed_power Description : Number of failed power supply units. ZAPI : endpoint : system-node-get-iter metric : node-details-info.env-failed-power-supply-count template : conf/zapi/cdot/9.8.0/node.yaml REST : endpoint : api/cluster/nodes metric : controller.failed_power_supply.count template : conf/rest/9.12.0/node.yaml - Harvest Metric : node_fcp_data_recv Description : Number of FCP kilobytes (KB) received per second ZAPI : endpoint : perf-object-get-instances system:node metric : fcp_data_recv template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : fcp_data_received template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_fcp_data_sent Description : Number of FCP kilobytes (KB) sent per second ZAPI : endpoint : perf-object-get-instances system:node metric : fcp_data_sent template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : fcp_data_sent template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_fcp_ops Description : Number of FCP operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : fcp_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : fcp_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_hdd_data_read Description : Number of HDD Disk kilobytes (KB) read per second ZAPI : endpoint : perf-object-get-instances system:node metric : hdd_data_read template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : hdd_data_read template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_hdd_data_written Description : Number of HDD kilobytes (KB) written per second ZAPI : endpoint : perf-object-get-instances system:node metric : hdd_data_written template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : hdd_data_written template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_iscsi_ops Description : Number of iSCSI operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : iscsi_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : iscsi_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_memory Description : Total memory in megabytes (MB) ZAPI : endpoint : perf-object-get-instances system:node metric : memory template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/system:node metric : memory template : conf/restperf/9.12.0/system_node.yaml unit : none type : raw - Harvest Metric : node_net_data_recv Description : Number of network kilobytes (KB) received per second ZAPI : endpoint : perf-object-get-instances system:node metric : net_data_recv template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : network_data_received template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_net_data_sent Description : Number of network kilobytes (KB) sent per second ZAPI : endpoint : perf-object-get-instances system:node metric : net_data_sent template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : network_data_sent template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_nfs_access_avg_latency Description : Average latency of ACCESS procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : access_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : access_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : access.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : access.total - Harvest Metric : node_nfs_access_total Description : Total number of ACCESS procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : access_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : access.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_backchannel_ctl_avg_latency Description : Average latency of NFSv4.2 BACKCHANNEL_CTL operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : backchannel_ctl_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : backchannel_ctl_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : backchannel_ctl.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : backchannel_ctl.total - Harvest Metric : node_nfs_backchannel_ctl_total Description : Total number of NFSv4.2 BACKCHANNEL_CTL operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : backchannel_ctl_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : backchannel_ctl.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_bind_conn_to_session_avg_latency Description : Average latency of NFSv4.2 BIND_CONN_TO_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : bind_conn_to_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : bind_conn_to_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : bind_conn_to_session.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : bind_conn_to_session.total - Harvest Metric : node_nfs_bind_conn_to_session_total Description : Total number of NFSv4.2 BIND_CONN_TO_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : bind_conn_to_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : bind_conn_to_session.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : delta - Harvest Metric : node_nfs_close_avg_latency Description : Average latency of CLOSE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : close_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : close_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : close.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : close.total - Harvest Metric : node_nfs_close_total Description : Total number of CLOSE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : close_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : close.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_commit_avg_latency Description : Average latency of COMMIT procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : commit_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : commit_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : commit.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : commit.total - Harvest Metric : node_nfs_commit_total Description : Total number of COMMIT procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : commit_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : commit.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_create_avg_latency Description : Average latency of CREATE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : create_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : create_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : create.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : create.total - Harvest Metric : node_nfs_create_session_avg_latency Description : Average latency of NFSv4.2 CREATE_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : create_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : create_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : create_session.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : create_session.total - Harvest Metric : node_nfs_create_session_total Description : Total number of NFSv4.2 CREATE_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : create_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : create_session.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_create_total Description : Total number of CREATE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : create_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : create.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_delegpurge_avg_latency Description : Average latency of DELEGPURGE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : delegpurge_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : delegpurge_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : delegpurge.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : delegpurge.total - Harvest Metric : node_nfs_delegpurge_total Description : Total number of DELEGPURGE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : delegpurge_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : delegpurge.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_delegreturn_avg_latency Description : Average latency of DELEGRETURN procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : delegreturn_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : delegreturn_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : delegreturn.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : delegreturn.total - Harvest Metric : node_nfs_delegreturn_total Description : Total number of DELEGRETURN procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : delegreturn_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : delegreturn.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_destroy_clientid_avg_latency Description : Average latency of NFSv4.2 DESTROY_CLIENTID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : destroy_clientid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : destroy_clientid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : destroy_clientid.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : destroy_clientid.total - Harvest Metric : node_nfs_destroy_clientid_total Description : Total number of NFSv4.2 DESTROY_CLIENTID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : destroy_clientid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : destroy_clientid.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_destroy_session_avg_latency Description : Average latency of NFSv4.2 DESTROY_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : destroy_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : destroy_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : destroy_session.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : destroy_session.total - Harvest Metric : node_nfs_destroy_session_total Description : Total number of NFSv4.2 DESTROY_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : destroy_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : destroy_session.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_exchange_id_avg_latency Description : Average latency of NFSv4.2 EXCHANGE_ID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : exchange_id_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : exchange_id_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : exchange_id.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : exchange_id.total - Harvest Metric : node_nfs_exchange_id_total Description : Total number of NFSv4.2 EXCHANGE_ID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : exchange_id_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : exchange_id.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_free_stateid_avg_latency Description : Average latency of NFSv4.2 FREE_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : free_stateid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : free_stateid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : free_stateid.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : free_stateid.total - Harvest Metric : node_nfs_free_stateid_total Description : Total number of NFSv4.2 FREE_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : free_stateid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : free_stateid.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_fsinfo_avg_latency Description : Average latency of FSInfo procedure requests. The counter keeps track of the average response time of FSInfo requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : fsinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : fsinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : fsinfo.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : fsinfo.total - Harvest Metric : node_nfs_fsinfo_total Description : Total number FSInfo of procedure requests. It is the total number of FSInfo success and FSInfo error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : fsinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : fsinfo.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_fsstat_avg_latency Description : Average latency of FSStat procedure requests. The counter keeps track of the average response time of FSStat requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : fsstat_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : fsstat_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : fsstat.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : fsstat.total - Harvest Metric : node_nfs_fsstat_total Description : Total number FSStat of procedure requests. It is the total number of FSStat success and FSStat error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : fsstat_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : fsstat.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_get_dir_delegation_avg_latency Description : Average latency of NFSv4.2 GET_DIR_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : get_dir_delegation_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : get_dir_delegation_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : get_dir_delegation.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : get_dir_delegation.total - Harvest Metric : node_nfs_get_dir_delegation_total Description : Total number of NFSv4.2 GET_DIR_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : get_dir_delegation_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : get_dir_delegation.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_getattr_avg_latency Description : Average latency of GETATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : getattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : getattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : getattr.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : getattr.total - Harvest Metric : node_nfs_getattr_total Description : Total number of GETATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : getattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : getattr.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_getdeviceinfo_avg_latency Description : Average latency of NFSv4.2 GETDEVICEINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : getdeviceinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : getdeviceinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : getdeviceinfo.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : getdeviceinfo.total - Harvest Metric : node_nfs_getdeviceinfo_total Description : Total number of NFSv4.2 GETDEVICEINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : getdeviceinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : getdeviceinfo.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_getdevicelist_avg_latency Description : Average latency of NFSv4.2 GETDEVICELIST operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : getdevicelist_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : getdevicelist_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : getdevicelist.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : getdevicelist.total - Harvest Metric : node_nfs_getdevicelist_total Description : Total number of NFSv4.2 GETDEVICELIST operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : getdevicelist_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : getdevicelist.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_getfh_avg_latency Description : Average latency of GETFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : getfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : getfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : getfh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : getfh.total - Harvest Metric : node_nfs_getfh_total Description : Total number of GETFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : getfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : getfh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_latency Description : Average latency of NFSv4 requests. This counter keeps track of the average response time of NFSv4 requests. ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : total_ops REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : total_ops - Harvest Metric : node_nfs_layoutcommit_avg_latency Description : Average latency of NFSv4.2 LAYOUTCOMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutcommit_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : layoutcommit_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutcommit.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : layoutcommit.total - Harvest Metric : node_nfs_layoutcommit_total Description : Total number of NFSv4.2 LAYOUTCOMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutcommit_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutcommit.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_layoutget_avg_latency Description : Average latency of NFSv4.2 LAYOUTGET operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutget_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : layoutget_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutget.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : layoutget.total - Harvest Metric : node_nfs_layoutget_total Description : Total number of NFSv4.2 LAYOUTGET operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutget_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutget.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_layoutreturn_avg_latency Description : Average latency of NFSv4.2 LAYOUTRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutreturn_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : layoutreturn_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutreturn.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : layoutreturn.total - Harvest Metric : node_nfs_layoutreturn_total Description : Total number of NFSv4.2 LAYOUTRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : layoutreturn_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : layoutreturn.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_link_avg_latency Description : Average latency of LINK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : link_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : link_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : link.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : link.total - Harvest Metric : node_nfs_link_total Description : Total number of LINK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : link_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : link.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_lock_avg_latency Description : Average latency of LOCK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lock_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : lock_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lock.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : lock.total - Harvest Metric : node_nfs_lock_total Description : Total number of LOCK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lock_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lock.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_lockt_avg_latency Description : Average latency of LOCKT procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lockt_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : lockt_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lockt.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : lockt.total - Harvest Metric : node_nfs_lockt_total Description : Total number of LOCKT procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lockt_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lockt.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_locku_avg_latency Description : Average latency of LOCKU procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : locku_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : locku_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : locku.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : locku.total - Harvest Metric : node_nfs_locku_total Description : Total number of LOCKU procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : locku_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : locku.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_lookup_avg_latency Description : Average latency of LOOKUP procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lookup_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : lookup_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lookup.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : lookup.total - Harvest Metric : node_nfs_lookup_total Description : Total number of LOOKUP procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lookup_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lookup.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_lookupp_avg_latency Description : Average latency of LOOKUPP procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lookupp_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : lookupp_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lookupp.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : lookupp.total - Harvest Metric : node_nfs_lookupp_total Description : Total number of LOOKUPP procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : lookupp_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : lookupp.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_mkdir_avg_latency Description : Average latency of MkDir procedure requests. The counter keeps track of the average response time of MkDir requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : mkdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : mkdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : mkdir.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : mkdir.total - Harvest Metric : node_nfs_mkdir_total Description : Total number MkDir of procedure requests. It is the total number of MkDir success and MkDir error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : mkdir_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : mkdir.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_mknod_avg_latency Description : Average latency of MkNod procedure requests. The counter keeps track of the average response time of MkNod requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : mknod_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : mknod_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : mknod.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : mknod.total - Harvest Metric : node_nfs_mknod_total Description : Total number MkNod of procedure requests. It is the total number of MkNod success and MkNod error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : mknod_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : mknod.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_null_avg_latency Description : Average Latency of NULL procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : null_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : null_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : null.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : null.total - Harvest Metric : node_nfs_null_total Description : Total number of NULL procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : null_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : null.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_nverify_avg_latency Description : Average latency of NVERIFY procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : nverify_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : nverify_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : nverify.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : nverify.total - Harvest Metric : node_nfs_nverify_total Description : Total number of NVERIFY procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : nverify_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : nverify.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_open_avg_latency Description : Average latency of OPEN procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : open_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : open.total - Harvest Metric : node_nfs_open_confirm_avg_latency Description : Average latency of OPEN_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_confirm_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : open_confirm_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open_confirm.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : open_confirm.total - Harvest Metric : node_nfs_open_confirm_total Description : Total number of OPEN_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_confirm_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open_confirm.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_open_downgrade_avg_latency Description : Average latency of OPEN_DOWNGRADE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_downgrade_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : open_downgrade_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open_downgrade.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : open_downgrade.total - Harvest Metric : node_nfs_open_downgrade_total Description : Total number of OPEN_DOWNGRADE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_downgrade_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open_downgrade.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_open_total Description : Total number of OPEN procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : open_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : open.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_openattr_avg_latency Description : Average latency of OPENATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : openattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : openattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : openattr.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : openattr.total - Harvest Metric : node_nfs_openattr_total Description : Total number of OPENATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : openattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : openattr.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_ops Description : Number of NFS operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : nfs_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : nfs_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_pathconf_avg_latency Description : Average latency of PathConf procedure requests. The counter keeps track of the average response time of PathConf requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : pathconf_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : pathconf_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : pathconf.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : pathconf.total - Harvest Metric : node_nfs_pathconf_total Description : Total number PathConf of procedure requests. It is the total number of PathConf success and PathConf error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : pathconf_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : pathconf.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_putfh_avg_latency Description : Average latency of PUTFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : putfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putfh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : putfh.total - Harvest Metric : node_nfs_putfh_total Description : Total number of PUTFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putfh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_putpubfh_avg_latency Description : Average latency of PUTPUBFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putpubfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : putpubfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putpubfh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : putpubfh.total - Harvest Metric : node_nfs_putpubfh_total Description : Total number of PUTPUBFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putpubfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putpubfh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_putrootfh_avg_latency Description : Average latency of PUTROOTFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putrootfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : putrootfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putrootfh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : putrootfh.total - Harvest Metric : node_nfs_putrootfh_total Description : Total number of PUTROOTFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : putrootfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : putrootfh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_read_avg_latency Description : Average latency of READ procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : read_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : read_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : read.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : read.total - Harvest Metric : node_nfs_read_ops Description : Total observed NFSv3 read operations per second. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : nfsv3_read_ops template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : read_ops template : conf/restperf/9.12.0/nfsv3_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_read_symlink_avg_latency Description : Average latency of ReadSymLink procedure requests. The counter keeps track of the average response time of ReadSymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : read_symlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : read_symlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : read_symlink.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : read_symlink.total - Harvest Metric : node_nfs_read_symlink_total Description : Total number of ReadSymLink procedure requests. It is the total number of read symlink success and read symlink error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : read_symlink_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : read_symlink.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : delta - Harvest Metric : node_nfs_read_throughput Description : NFSv4 read data transfers ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : nfs4_read_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : total.read_throughput template : conf/restperf/9.12.0/nfsv4_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_read_total Description : Total number of READ procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : read_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : read.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_readdir_avg_latency Description : Average latency of READDIR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : readdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : readdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : readdir.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : readdir.total - Harvest Metric : node_nfs_readdir_total Description : Total number of READDIR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : readdir_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : readdir.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_readdirplus_avg_latency Description : Average latency of ReadDirPlus procedure requests. The counter keeps track of the average response time of ReadDirPlus requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : readdirplus_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : readdirplus_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : readdirplus.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : readdirplus.total - Harvest Metric : node_nfs_readdirplus_total Description : Total number ReadDirPlus of procedure requests. It is the total number of ReadDirPlus success and ReadDirPlus error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : readdirplus_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : readdirplus.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_readlink_avg_latency Description : Average latency of READLINK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : readlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : readlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : readlink.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : readlink.total - Harvest Metric : node_nfs_readlink_total Description : Total number of READLINK procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : readlink_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : readlink.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_reclaim_complete_avg_latency Description : Average latency of NFSv4.2 RECLAIM_complete operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : reclaim_complete_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : reclaim_complete_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : reclaim_complete.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : reclaim_complete.total - Harvest Metric : node_nfs_reclaim_complete_total Description : Total number of NFSv4.2 RECLAIM_complete operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : reclaim_complete_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : reclaim_complete.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_release_lock_owner_avg_latency Description : Average Latency of RELEASE_LOCKOWNER procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : release_lock_owner_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : release_lock_owner_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : release_lock_owner.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : release_lock_owner.total - Harvest Metric : node_nfs_release_lock_owner_total Description : Total number of RELEASE_LOCKOWNER procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : release_lock_owner_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : release_lock_owner.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_remove_avg_latency Description : Average latency of REMOVE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : remove_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : remove_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : remove.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : remove.total - Harvest Metric : node_nfs_remove_total Description : Total number of REMOVE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : remove_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : remove.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_rename_avg_latency Description : Average latency of RENAME procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : rename_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : rename_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : rename.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : rename.total - Harvest Metric : node_nfs_rename_total Description : Total number of RENAME procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : rename_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : rename.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_renew_avg_latency Description : Average latency of RENEW procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : renew_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : renew_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : renew.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : renew.total - Harvest Metric : node_nfs_renew_total Description : Total number of RENEW procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : renew_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : renew.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_restorefh_avg_latency Description : Average latency of RESTOREFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : restorefh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : restorefh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : restorefh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : restorefh.total - Harvest Metric : node_nfs_restorefh_total Description : Total number of RESTOREFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : restorefh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : restorefh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_rmdir_avg_latency Description : Average latency of RmDir procedure requests. The counter keeps track of the average response time of RmDir requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : rmdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : rmdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : rmdir.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : rmdir.total - Harvest Metric : node_nfs_rmdir_total Description : Total number RmDir of procedure requests. It is the total number of RmDir success and RmDir error requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : rmdir_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : rmdir.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_savefh_avg_latency Description : Average latency of SAVEFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : savefh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : savefh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : savefh.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : savefh.total - Harvest Metric : node_nfs_savefh_total Description : Total number of SAVEFH procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : savefh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : savefh.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_secinfo_avg_latency Description : Average latency of SECINFO procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : secinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : secinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : secinfo.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : secinfo.total - Harvest Metric : node_nfs_secinfo_no_name_avg_latency Description : Average latency of NFSv4.2 SECINFO_NO_NAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : secinfo_no_name_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : secinfo_no_name_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : secinfo_no_name.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : secinfo_no_name.total - Harvest Metric : node_nfs_secinfo_no_name_total Description : Total number of NFSv4.2 SECINFO_NO_NAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : secinfo_no_name_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : secinfo_no_name.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_secinfo_total Description : Total number of SECINFO procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : secinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : secinfo.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_sequence_avg_latency Description : Average latency of NFSv4.2 SEQUENCE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : sequence_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : sequence_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : sequence.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : sequence.total - Harvest Metric : node_nfs_sequence_total Description : Total number of NFSv4.2 SEQUENCE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : sequence_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : sequence.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_set_ssv_avg_latency Description : Average latency of NFSv4.2 SET_SSV operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : set_ssv_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : set_ssv_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : set_ssv.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : set_ssv.total - Harvest Metric : node_nfs_set_ssv_total Description : Total number of NFSv4.2 SET_SSV operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : set_ssv_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : set_ssv.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_setattr_avg_latency Description : Average latency of SETATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : setattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setattr.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : setattr.total - Harvest Metric : node_nfs_setattr_total Description : Total number of SETATTR procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setattr.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_setclientid_avg_latency Description : Average latency of SETCLIENTID procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setclientid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : setclientid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setclientid.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : setclientid.total - Harvest Metric : node_nfs_setclientid_confirm_avg_latency Description : Average latency of SETCLIENTID_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setclientid_confirm_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : setclientid_confirm_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setclientid_confirm.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : setclientid_confirm.total - Harvest Metric : node_nfs_setclientid_confirm_total Description : Total number of SETCLIENTID_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setclientid_confirm_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setclientid_confirm.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_setclientid_total Description : Total number of SETCLIENTID procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : setclientid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : setclientid.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_symlink_avg_latency Description : Average latency of SymLink procedure requests. The counter keeps track of the average response time of SymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : symlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : microsec type : average,no-zero-values base : symlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : symlink.average_latency template : conf/restperf/9.12.0/nfsv3_node.yaml unit : microsec type : average base : symlink.total - Harvest Metric : node_nfs_symlink_total Description : Total number SymLink of procedure requests. It is the total number of SymLink success and create SymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : symlink_total template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : symlink.total template : conf/restperf/9.12.0/nfsv3_node.yaml unit : none type : rate - Harvest Metric : node_nfs_test_stateid_avg_latency Description : Average latency of NFSv4.2 TEST_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : test_stateid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : test_stateid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : test_stateid.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : test_stateid.total - Harvest Metric : node_nfs_test_stateid_total Description : Total number of NFSv4.2 TEST_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : test_stateid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : test_stateid.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_throughput Description : NFSv4 data transfers ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : nfs4_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : total.throughput template : conf/restperf/9.12.0/nfsv4_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_total_ops Description : Total number of NFSv4 requests per second. ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : total_ops template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : total_ops template : conf/restperf/9.12.0/nfsv4_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_verify_avg_latency Description : Average latency of VERIFY procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : verify_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : verify_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : verify.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : verify.total - Harvest Metric : node_nfs_verify_total Description : Total number of VERIFY procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : verify_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : verify.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nfs_want_delegation_avg_latency Description : Average latency of NFSv4.2 WANT_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : want_delegation_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : microsec type : average,no-zero-values base : want_delegation_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : want_delegation.average_latency template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : microsec type : average base : want_delegation.total - Harvest Metric : node_nfs_want_delegation_total Description : Total number of NFSv4.2 WANT_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1:node metric : want_delegation_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42:node metric : want_delegation.total template : conf/restperf/9.12.0/nfsv4_2_node.yaml unit : none type : rate - Harvest Metric : node_nfs_write_avg_latency Description : Average Latency of WRITE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : write_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : microsec type : average,no-zero-values base : write_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : write.average_latency template : conf/restperf/9.12.0/nfsv4_node.yaml unit : microsec type : average base : write.total - Harvest Metric : node_nfs_write_ops Description : Total observed NFSv3 write operations per second. ZAPI : endpoint : perf-object-get-instances nfsv3:node metric : nfsv3_write_ops template : conf/zapiperf/cdot/9.8.0/nfsv3_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v3:node metric : write_ops template : conf/restperf/9.12.0/nfsv3_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_write_throughput Description : NFSv4 write data transfers ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : nfs4_write_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : total.write_throughput template : conf/restperf/9.12.0/nfsv4_node.yaml unit : per_sec type : rate - Harvest Metric : node_nfs_write_total Description : Total number of WRITE procedures ZAPI : endpoint : perf-object-get-instances nfsv4:node metric : write_total template : conf/zapiperf/cdot/9.8.0/nfsv4_node.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4:node metric : write.total template : conf/restperf/9.12.0/nfsv4_node.yaml unit : none type : rate - Harvest Metric : node_nvmf_data_recv Description : NVMe/FC kilobytes (KB) received per second ZAPI : endpoint : perf-object-get-instances system:node metric : nvmf_data_recv template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : nvme_fc_data_received template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_nvmf_data_sent Description : NVMe/FC kilobytes (KB) sent per second ZAPI : endpoint : perf-object-get-instances system:node metric : nvmf_data_sent template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : nvme_fc_data_sent template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_nvmf_ops Description : NVMe/FC operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : nvmf_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : nvme_fc_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_ssd_data_read Description : Number of SSD Disk kilobytes (KB) read per second ZAPI : endpoint : perf-object-get-instances system:node metric : ssd_data_read template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : ssd_data_read template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_ssd_data_written Description : Number of SSD Disk kilobytes (KB) written per second ZAPI : endpoint : perf-object-get-instances system:node metric : ssd_data_written template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : ssd_data_written template : conf/restperf/9.12.0/system_node.yaml unit : kb_per_sec type : rate - Harvest Metric : node_total_data Description : Total throughput in bytes ZAPI : endpoint : perf-object-get-instances system:node metric : total_data template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : total_data template : conf/restperf/9.12.0/system_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_total_latency Description : Average latency for all operations in the system in microseconds ZAPI : endpoint : perf-object-get-instances system:node metric : total_latency template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : microsec type : average base : total_ops REST : endpoint : api/cluster/counter/tables/system:node metric : total_latency template : conf/restperf/9.12.0/system_node.yaml unit : microsec type : average base : total_ops - Harvest Metric : node_total_ops Description : Total number of operations per second ZAPI : endpoint : perf-object-get-instances system:node metric : total_ops template : conf/zapiperf/cdot/9.8.0/system_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/system:node metric : total_ops template : conf/restperf/9.12.0/system_node.yaml unit : per_sec type : rate - Harvest Metric : node_uptime Description : The total time, in seconds, that the node has been up. ZAPI : endpoint : system-node-get-iter metric : node-details-info.node-uptime template : conf/zapi/cdot/9.8.0/node.yaml REST : endpoint : api/cluster/nodes metric : uptime template : conf/rest/9.12.0/node.yaml - Harvest Metric : node_vol_cifs_other_latency Description : Average time for the WAFL filesystem to process other CIFS operations to the volume; not including CIFS protocol request processing or network communication time which will also be included in client observed CIFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_other_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : cifs_other_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.other_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : cifs.other_ops - Harvest Metric : node_vol_cifs_other_ops Description : Number of other CIFS operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_other_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.other_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_cifs_read_data Description : Bytes read per second via CIFS ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_read_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.read_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_cifs_read_latency Description : Average time for the WAFL filesystem to process CIFS read requests to the volume; not including CIFS protocol request processing or network communication time which will also be included in client observed CIFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_read_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : cifs_read_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.read_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : cifs.read_ops - Harvest Metric : node_vol_cifs_read_ops Description : Number of CIFS read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_read_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.read_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_cifs_write_data Description : Bytes written per second via CIFS ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_write_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.write_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_cifs_write_latency Description : Average time for the WAFL filesystem to process CIFS write requests to the volume; not including CIFS protocol request processing or network communication time which will also be included in client observed CIFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_write_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : cifs_write_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.write_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : cifs.write_ops - Harvest Metric : node_vol_cifs_write_ops Description : Number of CIFS write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : cifs_write_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : cifs.write_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_fcp_other_latency Description : Average time for the WAFL filesystem to process other FCP protocol operations to the volume; not including FCP protocol request processing or network communication time which will also be included in client observed FCP protocol request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_other_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : fcp_other_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.other_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : fcp.other_ops - Harvest Metric : node_vol_fcp_other_ops Description : Number of other block protocol operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_other_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.other_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_fcp_read_data Description : Bytes read per second via block protocol ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_read_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.read_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_fcp_read_latency Description : Average time for the WAFL filesystem to process FCP protocol read operations to the volume; not including FCP protocol request processing or network communication time which will also be included in client observed FCP protocol request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_read_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : fcp_read_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.read_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : fcp.read_ops - Harvest Metric : node_vol_fcp_read_ops Description : Number of block protocol read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_read_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.read_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_fcp_write_data Description : Bytes written per second via block protocol ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_write_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.write_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_fcp_write_latency Description : Average time for the WAFL filesystem to process FCP protocol write operations to the volume; not including FCP protocol request processing or network communication time which will also be included in client observed FCP protocol request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_write_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : fcp_write_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.write_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : fcp.write_ops - Harvest Metric : node_vol_fcp_write_ops Description : Number of block protocol write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : fcp_write_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : fcp.write_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_iscsi_other_latency Description : Average time for the WAFL filesystem to process other iSCSI protocol operations to the volume; not including iSCSI protocol request processing or network communication time which will also be included in client observed iSCSI protocol request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_other_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : iscsi_other_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.other_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : iscsi.other_ops - Harvest Metric : node_vol_iscsi_other_ops Description : Number of other block protocol operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_other_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.other_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_iscsi_read_data Description : Bytes read per second via block protocol ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_read_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.read_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_iscsi_read_latency Description : Average time for the WAFL filesystem to process iSCSI protocol read operations to the volume; not including iSCSI protocol request processing or network communication time which will also be included in client observed iSCSI protocol request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_read_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : iscsi_read_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.read_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : iscsi.read_ops - Harvest Metric : node_vol_iscsi_read_ops Description : Number of block protocol read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_read_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.read_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_iscsi_write_data Description : Bytes written per second via block protocol ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_write_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.write_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_iscsi_write_latency Description : Average time for the WAFL filesystem to process iSCSI protocol write operations to the volume; not including iSCSI protocol request processing or network communication time which will also be included in client observed iSCSI request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_write_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : iscsi_write_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.write_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : iscsi.write_ops - Harvest Metric : node_vol_iscsi_write_ops Description : Number of block protocol write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : iscsi_write_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : iscsi.write_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_nfs_other_latency Description : Average time for the WAFL filesystem to process other NFS operations to the volume; not including NFS protocol request processing or network communication time which will also be included in client observed NFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_other_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : nfs_other_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.other_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : nfs.other_ops - Harvest Metric : node_vol_nfs_other_ops Description : Number of other NFS operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_other_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.other_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_nfs_read_data Description : Bytes read per second via NFS ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_read_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.read_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_nfs_read_latency Description : Average time for the WAFL filesystem to process NFS protocol read requests to the volume; not including NFS protocol request processing or network communication time which will also be included in client observed NFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_read_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : nfs_read_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.read_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : nfs.read_ops - Harvest Metric : node_vol_nfs_read_ops Description : Number of NFS read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_read_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.read_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_nfs_write_data Description : Bytes written per second via NFS ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_write_data template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.write_data template : conf/restperf/9.12.0/volume_node.yaml unit : b_per_sec type : rate - Harvest Metric : node_vol_nfs_write_latency Description : Average time for the WAFL filesystem to process NFS protocol write requests to the volume; not including NFS protocol request processing or network communication time, which will also be included in client observed NFS request latency ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_write_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : nfs_write_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.write_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : nfs.write_ops - Harvest Metric : node_vol_nfs_write_ops Description : Number of NFS write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:node metric : nfs_write_ops template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:node metric : nfs.write_ops template : conf/restperf/9.12.0/volume_node.yaml unit : per_sec type : rate - Harvest Metric : node_vol_read_latency Description : Average latency in microseconds for the WAFL filesystem to process read request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:node metric : read_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : read_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : total_read_ops - Harvest Metric : node_vol_write_latency Description : Average latency in microseconds for the WAFL filesystem to process write request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:node metric : write_latency template : conf/zapiperf/cdot/9.8.0/volume_node.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/volume:node metric : write_latency template : conf/restperf/9.12.0/volume_node.yaml unit : microsec type : average base : total_write_ops - Harvest Metric : nvme_lif_avg_latency Description : Average latency for NVMF operations ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : avg_latency template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : microsec type : average base : total_ops REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : average_latency template : conf/restperf/9.12.0/nvmf_lif.yaml unit : microsec type : average base : total_ops - Harvest Metric : nvme_lif_avg_other_latency Description : Average latency for operations other than read, write, compare or compare-and-write. ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : avg_other_latency template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : average_other_latency template : conf/restperf/9.12.0/nvmf_lif.yaml unit : microsec type : average base : other_ops - Harvest Metric : nvme_lif_avg_read_latency Description : Average latency for read operations ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : avg_read_latency template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : average_read_latency template : conf/restperf/9.12.0/nvmf_lif.yaml unit : microsec type : average base : read_ops - Harvest Metric : nvme_lif_avg_write_latency Description : Average latency for write operations ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : avg_write_latency template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : average_write_latency template : conf/restperf/9.12.0/nvmf_lif.yaml unit : microsec type : average base : write_ops - Harvest Metric : nvme_lif_other_ops Description : Number of operations that are not read, write, compare or compare-and-write. ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : other_ops template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : other_ops template : conf/restperf/9.12.0/nvmf_lif.yaml unit : per_sec type : rate - Harvest Metric : nvme_lif_read_data Description : Amount of data read from the storage system ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : read_data template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : read_data template : conf/restperf/9.12.0/nvmf_lif.yaml unit : b_per_sec type : rate - Harvest Metric : nvme_lif_read_ops Description : Number of read operations ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : read_ops template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : read_ops template : conf/restperf/9.12.0/nvmf_lif.yaml unit : per_sec type : rate - Harvest Metric : nvme_lif_total_ops Description : Total number of operations. ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : total_ops template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : total_ops template : conf/restperf/9.12.0/nvmf_lif.yaml unit : per_sec type : rate - Harvest Metric : nvme_lif_write_data Description : Amount of data written to the storage system ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : write_data template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : write_data template : conf/restperf/9.12.0/nvmf_lif.yaml unit : b_per_sec type : rate - Harvest Metric : nvme_lif_write_ops Description : Number of write operations ZAPI : endpoint : perf-object-get-instances nvmf_fc_lif metric : write_ops template : conf/zapiperf/cdot/9.10.1/nvmf_lif.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/nvmf_lif metric : write_ops template : conf/restperf/9.12.0/nvmf_lif.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_abort_multipart_upload_failed Description : Number of failed Abort Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : abort_multipart_upload_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_abort_multipart_upload_failed_client_close Description : Number of times Abort Multipart Upload operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : abort_multipart_upload_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_abort_multipart_upload_latency Description : Average latency for Abort Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : abort_multipart_upload_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : abort_multipart_upload_latency_base - Harvest Metric : ontaps3_abort_multipart_upload_rate Description : Number of Abort Multipart Upload operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : abort_multipart_upload_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_abort_multipart_upload_total Description : Number of Abort Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : abort_multipart_upload_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_allow_access Description : Number of times access was allowed. ZAPI : endpoint : perf-object-get-instances object_store_server metric : allow_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_anonymous_access Description : Number of times anonymous access was allowed. ZAPI : endpoint : perf-object-get-instances object_store_server metric : anonymous_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_anonymous_deny_access Description : Number of times anonymous access was denied. ZAPI : endpoint : perf-object-get-instances object_store_server metric : anonymous_deny_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_authentication_failures Description : Number of authentication failures. ZAPI : endpoint : perf-object-get-instances object_store_server metric : authentication_failures template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_chunked_upload_reqs Description : Total number of object store server chunked object upload requests ZAPI : endpoint : perf-object-get-instances object_store_server metric : chunked_upload_reqs template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_complete_multipart_upload_failed Description : Number of failed Complete Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : complete_multipart_upload_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_complete_multipart_upload_failed_client_close Description : Number of times Complete Multipart Upload operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : complete_multipart_upload_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_complete_multipart_upload_latency Description : Average latency for Complete Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : complete_multipart_upload_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : complete_multipart_upload_latency_base - Harvest Metric : ontaps3_complete_multipart_upload_rate Description : Number of Complete Multipart Upload operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : complete_multipart_upload_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_complete_multipart_upload_total Description : Number of Complete Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : complete_multipart_upload_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_connected_connections Description : Number of object store server connections currently established ZAPI : endpoint : perf-object-get-instances object_store_server metric : connected_connections template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : raw - Harvest Metric : ontaps3_connections Description : Total number of object store server connections. ZAPI : endpoint : perf-object-get-instances object_store_server metric : connections template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_create_bucket_failed Description : Number of failed Create Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : create_bucket_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_create_bucket_failed_client_close Description : Number of times Create Bucket operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : create_bucket_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_create_bucket_latency Description : Average latency for Create Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : create_bucket_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average,no-zero-values base : create_bucket_latency_base - Harvest Metric : ontaps3_create_bucket_rate Description : Number of Create Bucket operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : create_bucket_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : ontaps3_create_bucket_total Description : Number of Create Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : create_bucket_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_default_deny_access Description : Number of times access was denied by default and not through any policy statement. ZAPI : endpoint : perf-object-get-instances object_store_server metric : default_deny_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_bucket_failed Description : Number of failed Delete Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_bucket_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_bucket_failed_client_close Description : Number of times Delete Bucket operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_bucket_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_bucket_latency Description : Average latency for Delete Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_bucket_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average,no-zero-values base : delete_bucket_latency_base - Harvest Metric : ontaps3_delete_bucket_rate Description : Number of Delete Bucket operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_bucket_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : ontaps3_delete_bucket_total Description : Number of Delete Bucket operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_bucket_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_object_failed Description : Number of failed DELETE object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_delete_object_failed_client_close Description : Number of times DELETE object operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_delete_object_latency Description : Average latency for DELETE object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : delete_object_latency_base - Harvest Metric : ontaps3_delete_object_rate Description : Number of DELETE object operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_delete_object_tagging_failed Description : Number of failed DELETE object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_tagging_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_object_tagging_failed_client_close Description : Number of times DELETE object tagging operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_tagging_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_object_tagging_latency Description : Average latency for DELETE object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_tagging_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average,no-zero-values base : delete_object_tagging_latency_base - Harvest Metric : ontaps3_delete_object_tagging_rate Description : Number of DELETE object tagging operations per sec. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_tagging_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : ontaps3_delete_object_tagging_total Description : Number of DELETE object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_tagging_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_delete_object_total Description : Number of DELETE object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : delete_object_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_explicit_deny_access Description : Number of times access was denied explicitly by a policy statement. ZAPI : endpoint : perf-object-get-instances object_store_server metric : explicit_deny_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_get_bucket_acl_failed Description : Number of failed GET Bucket ACL operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_bucket_acl_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_bucket_acl_total Description : Number of GET Bucket ACL operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_bucket_acl_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_bucket_versioning_failed Description : Number of failed Get Bucket Versioning operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_bucket_versioning_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_bucket_versioning_total Description : Number of Get Bucket Versioning operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_bucket_versioning_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_data Description : Rate of GET object data transfers per second ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_data template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : b_per_sec type : rate - Harvest Metric : ontaps3_get_object_acl_failed Description : Number of failed GET Object ACL operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_acl_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_acl_total Description : Number of GET Object ACL operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_acl_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_failed Description : Number of failed GET object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_failed_client_close Description : Number of times GET object operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_lastbyte_latency Description : Average last-byte latency for GET object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_lastbyte_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : get_object_lastbyte_latency_base - Harvest Metric : ontaps3_get_object_latency Description : Average first-byte latency for GET object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : get_object_latency_base - Harvest Metric : ontaps3_get_object_rate Description : Number of GET object operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_get_object_tagging_failed Description : Number of failed GET object tagging operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_tagging_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_tagging_failed_client_close Description : Number of times GET object tagging operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_tagging_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_tagging_latency Description : Average latency for GET object tagging operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_tagging_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : get_object_tagging_latency_base - Harvest Metric : ontaps3_get_object_tagging_rate Description : Number of GET object tagging operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_tagging_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_get_object_tagging_total Description : Number of GET object tagging operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_tagging_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_get_object_total Description : Number of GET object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : get_object_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_group_policy_evaluated Description : Number of times group policies were evaluated. ZAPI : endpoint : perf-object-get-instances object_store_server metric : group_policy_evaluated template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_head_bucket_failed Description : Number of failed HEAD bucket operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_bucket_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_head_bucket_failed_client_close Description : Number of times HEAD bucket operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_bucket_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_head_bucket_latency Description : Average latency for HEAD bucket operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_bucket_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : head_bucket_latency_base - Harvest Metric : ontaps3_head_bucket_rate Description : Number of HEAD bucket operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_bucket_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_head_bucket_total Description : Number of HEAD bucket operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_bucket_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_head_object_failed Description : Number of failed HEAD Object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_object_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_head_object_failed_client_close Description : Number of times HEAD object operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_object_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_head_object_latency Description : Average latency for HEAD object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_object_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : head_object_latency_base - Harvest Metric : ontaps3_head_object_rate Description : Number of HEAD Object operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_object_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_head_object_total Description : Number of HEAD Object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : head_object_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_initiate_multipart_upload_failed Description : Number of failed Initiate Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : initiate_multipart_upload_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_initiate_multipart_upload_failed_client_close Description : Number of times Initiate Multipart Upload operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : initiate_multipart_upload_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_initiate_multipart_upload_latency Description : Average latency for Initiate Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : initiate_multipart_upload_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : initiate_multipart_upload_latency_base - Harvest Metric : ontaps3_initiate_multipart_upload_rate Description : Number of Initiate Multipart Upload operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : initiate_multipart_upload_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_initiate_multipart_upload_total Description : Number of Initiate Multipart Upload operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : initiate_multipart_upload_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_input_flow_control_entry Description : Number of times input flow control was entered. ZAPI : endpoint : perf-object-get-instances object_store_server metric : input_flow_control_entry template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_input_flow_control_exit Description : Number of times input flow control was exited. ZAPI : endpoint : perf-object-get-instances object_store_server metric : input_flow_control_exit template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_buckets_failed Description : Number of failed LIST Buckets operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_buckets_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_buckets_failed_client_close Description : Number of times LIST Bucket operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_buckets_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_buckets_latency Description : Average latency for LIST Buckets operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_buckets_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : head_object_latency_base - Harvest Metric : ontaps3_list_buckets_rate Description : Number of LIST Buckets operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_buckets_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_list_buckets_total Description : Number of LIST Buckets operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_buckets_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_object_versions_failed Description : Number of failed LIST object versions operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_object_versions_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_object_versions_failed_client_close Description : Number of times LIST object versions operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_object_versions_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_list_object_versions_latency Description : Average latency for LIST Object versions operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_object_versions_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average,no-zero-values base : list_object_versions_latency_base - Harvest Metric : ontaps3_list_object_versions_rate Description : Number of LIST Object Versions operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_object_versions_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : ontaps3_list_object_versions_total Description : Number of LIST Object Versions operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_object_versions_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_objects_failed Description : Number of failed LIST objects operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_objects_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_objects_failed_client_close Description : Number of times LIST objects operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_objects_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_objects_latency Description : Average latency for LIST Objects operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_objects_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : list_objects_latency_base - Harvest Metric : ontaps3_list_objects_rate Description : Number of LIST Objects operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_objects_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_list_objects_total Description : Number of LIST Objects operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_objects_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_uploads_failed Description : Number of failed LIST Uploads operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_uploads_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_uploads_failed_client_close Description : Number of times LIST Uploads operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_uploads_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_list_uploads_latency Description : Average latency for LIST Uploads operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_uploads_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : list_uploads_latency_base - Harvest Metric : ontaps3_list_uploads_rate Description : Number of LIST Uploads operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_uploads_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_list_uploads_total Description : Number of LIST Uploads operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : list_uploads_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_logical_used_size REST : endpoint : api/protocols/s3/buckets metric : logical_used_size template : conf/rest/9.7.0/ontap_s3.yaml - Harvest Metric : ontaps3_max_cmds_per_connection Description : Maximum commands pipelined at any instance on a connection. ZAPI : endpoint : perf-object-get-instances object_store_server metric : max_cmds_per_connection template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_max_connected_connections Description : Maximum number of object store server connections established at one time ZAPI : endpoint : perf-object-get-instances object_store_server metric : max_connected_connections template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : raw - Harvest Metric : ontaps3_max_requests_outstanding Description : Maximum number of object store server requests in process at one time ZAPI : endpoint : perf-object-get-instances object_store_server metric : max_requests_outstanding template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : raw - Harvest Metric : ontaps3_multi_delete_reqs Description : Total number of object store server multiple object delete requests ZAPI : endpoint : perf-object-get-instances object_store_server metric : multi_delete_reqs template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_output_flow_control_entry Description : Number of output flow control was entered. ZAPI : endpoint : perf-object-get-instances object_store_server metric : output_flow_control_entry template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_output_flow_control_exit Description : Number of times output flow control was exited. ZAPI : endpoint : perf-object-get-instances object_store_server metric : output_flow_control_exit template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_presigned_url_reqs Description : Total number of presigned object store server URL requests. ZAPI : endpoint : perf-object-get-instances object_store_server metric : presigned_url_reqs template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_put_bucket_versioning_failed Description : Number of failed Put Bucket Versioning operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_bucket_versioning_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_put_bucket_versioning_total Description : Number of Put Bucket Versioning operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_bucket_versioning_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_put_data Description : Rate of PUT object data transfers per second ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_data template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : b_per_sec type : rate - Harvest Metric : ontaps3_put_object_failed Description : Number of failed PUT object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_put_object_failed_client_close Description : Number of times PUT object operation failed due to the case where client closed the connection while the operation was still pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_put_object_latency Description : Average latency for PUT object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : put_object_latency_base - Harvest Metric : ontaps3_put_object_rate Description : Number of PUT object operations per sec ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_put_object_tagging_failed Description : Number of failed PUT object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_tagging_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_put_object_tagging_failed_client_close Description : Number of times PUT object tagging operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_tagging_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_put_object_tagging_latency Description : Average latency for PUT object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_tagging_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average,no-zero-values base : put_object_tagging_latency_base - Harvest Metric : ontaps3_put_object_tagging_rate Description : Number of PUT object tagging operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_tagging_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate,no-zero-values - Harvest Metric : ontaps3_put_object_tagging_total Description : Number of PUT object tagging operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_tagging_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_put_object_total Description : Number of PUT object operations ZAPI : endpoint : perf-object-get-instances object_store_server metric : put_object_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_request_parse_errors Description : Number of request parser errors due to malformed requests. ZAPI : endpoint : perf-object-get-instances object_store_server metric : request_parse_errors template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_requests Description : Total number of object store server requests ZAPI : endpoint : perf-object-get-instances object_store_server metric : requests template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_requests_outstanding Description : Number of object store server requests in process ZAPI : endpoint : perf-object-get-instances object_store_server metric : requests_outstanding template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : raw - Harvest Metric : ontaps3_root_user_access Description : Number of times access was done by root user. ZAPI : endpoint : perf-object-get-instances object_store_server metric : root_user_access template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_server_connection_close Description : Number of connection closes triggered by server due to fatal errors. ZAPI : endpoint : perf-object-get-instances object_store_server metric : server_connection_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_signature_v2_reqs Description : Total number of object store server signature V2 requests ZAPI : endpoint : perf-object-get-instances object_store_server metric : signature_v2_reqs template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_signature_v4_reqs Description : Total number of object store server signature V4 requests ZAPI : endpoint : perf-object-get-instances object_store_server metric : signature_v4_reqs template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_size REST : endpoint : api/protocols/s3/buckets metric : size template : conf/rest/9.7.0/ontap_s3.yaml - Harvest Metric : ontaps3_tagging Description : Number of requests with tagging specified. ZAPI : endpoint : perf-object-get-instances object_store_server metric : tagging template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta,no-zero-values - Harvest Metric : ontaps3_upload_part_failed Description : Number of failed Upload Part operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : upload_part_failed template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_upload_part_failed_client_close Description : Number of times Upload Part operation failed because client terminated connection for operation pending on server. ZAPI : endpoint : perf-object-get-instances object_store_server metric : upload_part_failed_client_close template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : ontaps3_upload_part_latency Description : Average latency for Upload Part operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : upload_part_latency template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : microsec type : average base : upload_part_latency_base - Harvest Metric : ontaps3_upload_part_rate Description : Number of Upload Part operations per second. ZAPI : endpoint : perf-object-get-instances object_store_server metric : upload_part_rate template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : per_sec type : rate - Harvest Metric : ontaps3_upload_part_total Description : Number of Upload Part operations. ZAPI : endpoint : perf-object-get-instances object_store_server metric : upload_part_total template : conf/zapiperf/cdot/9.8.0/ontap_s3.yaml unit : none type : delta - Harvest Metric : path_read_data Description : The average read throughput in kilobytes per second read from the indicated target port by the controller. ZAPI : endpoint : perf-object-get-instances path metric : read_data template : conf/zapiperf/cdot/9.8.0/path.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : read_data template : conf/restperf/9.12.0/path.yaml unit : kb_per_sec type : rate - Harvest Metric : path_read_iops Description : The number of I/O read operations sent from the initiator port to the indicated target port. ZAPI : endpoint : perf-object-get-instances path metric : read_iops template : conf/zapiperf/cdot/9.8.0/path.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : read_iops template : conf/restperf/9.12.0/path.yaml unit : per_sec type : rate - Harvest Metric : path_read_latency Description : The average latency of I/O read operations sent from this controller to the indicated target port. ZAPI : endpoint : perf-object-get-instances path metric : read_latency template : conf/zapiperf/cdot/9.8.0/path.yaml unit : microsec type : average base : read_iops REST : endpoint : api/cluster/counter/tables/path metric : read_latency template : conf/restperf/9.12.0/path.yaml unit : microsec type : average base : read_iops - Harvest Metric : path_total_data Description : The average throughput in kilobytes per second read and written from/to the indicated target port by the controller. ZAPI : endpoint : perf-object-get-instances path metric : total_data template : conf/zapiperf/cdot/9.8.0/path.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : total_data template : conf/restperf/9.12.0/path.yaml unit : kb_per_sec type : rate - Harvest Metric : path_total_iops Description : The number of total read/write I/O operations sent from the initiator port to the indicated target port. ZAPI : endpoint : perf-object-get-instances path metric : total_iops template : conf/zapiperf/cdot/9.8.0/path.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : total_iops template : conf/restperf/9.12.0/path.yaml unit : per_sec type : rate - Harvest Metric : path_write_data Description : The average write throughput in kilobytes per second written to the indicated target port by the controller. ZAPI : endpoint : perf-object-get-instances path metric : write_data template : conf/zapiperf/cdot/9.8.0/path.yaml unit : kb_per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : write_data template : conf/restperf/9.12.0/path.yaml unit : kb_per_sec type : rate - Harvest Metric : path_write_iops Description : The number of I/O write operations sent from the initiator port to the indicated target port. ZAPI : endpoint : perf-object-get-instances path metric : write_iops template : conf/zapiperf/cdot/9.8.0/path.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/path metric : write_iops template : conf/restperf/9.12.0/path.yaml unit : per_sec type : rate - Harvest Metric : path_write_latency Description : The average latency of I/O write operations sent from this controller to the indicated target port. ZAPI : endpoint : perf-object-get-instances path metric : write_latency template : conf/zapiperf/cdot/9.8.0/path.yaml unit : microsec type : average base : write_iops REST : endpoint : api/cluster/counter/tables/path metric : write_latency template : conf/restperf/9.12.0/path.yaml unit : microsec type : average base : write_iops - Harvest Metric : qos_concurrency Description : This is the average number of concurrent requests for the workload. ZAPI : endpoint : perf-object-get-instances workload metric : concurrency template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : none type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : concurrency template : conf/restperf/9.12.0/workload.yaml unit : none type : rate - Harvest Metric : qos_detail_service_time Description : The workload's average service time per visit to the service center. ZAPI : endpoint : perf-object-get-instances workload_detail metric : service_time template : conf/zapiperf/cdot/9.8.0/workload_detail.yaml unit : microsec type : average,no-zero-values base : visits REST : endpoint : api/cluster/counter/tables/qos_detail metric : service_time template : conf/restperf/9.12.0/workload_detail.yaml unit : microsec type : average base : visits - Harvest Metric : qos_detail_visits Description : The number of visits that the workload made to the service center; measured in visits per second. ZAPI : endpoint : perf-object-get-instances workload_detail metric : visits template : conf/zapiperf/cdot/9.8.0/workload_detail.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_detail metric : visits template : conf/restperf/9.12.0/workload_detail.yaml unit : per_sec type : rate - Harvest Metric : qos_detail_volume_service_time Description : The workload's average service time per visit to the service center. ZAPI : endpoint : perf-object-get-instances workload_detail_volume metric : service_time template : conf/zapiperf/cdot/9.8.0/workload_detail_volume.yaml unit : microsec type : average,no-zero-values base : visits REST : endpoint : api/cluster/counter/tables/qos_detail_volume metric : service_time template : conf/restperf/9.12.0/workload_detail_volume.yaml unit : microsec type : average base : visits - Harvest Metric : qos_detail_volume_visits Description : The number of visits that the workload made to the service center; measured in visits per second. ZAPI : endpoint : perf-object-get-instances workload_detail_volume metric : visits template : conf/zapiperf/cdot/9.8.0/workload_detail_volume.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_detail_volume metric : visits template : conf/restperf/9.12.0/workload_detail_volume.yaml unit : per_sec type : rate - Harvest Metric : qos_detail_volume_wait_time Description : The workload's average wait time per visit to the service center. ZAPI : endpoint : perf-object-get-instances workload_detail_volume metric : wait_time template : conf/zapiperf/cdot/9.8.0/workload_detail_volume.yaml unit : microsec type : average,no-zero-values base : visits REST : endpoint : api/cluster/counter/tables/qos_detail_volume metric : wait_time template : conf/restperf/9.12.0/workload_detail_volume.yaml unit : microsec type : average base : visits - Harvest Metric : qos_detail_wait_time Description : The workload's average wait time per visit to the service center. ZAPI : endpoint : perf-object-get-instances workload_detail metric : wait_time template : conf/zapiperf/cdot/9.8.0/workload_detail.yaml unit : microsec type : average,no-zero-values base : visits REST : endpoint : api/cluster/counter/tables/qos_detail metric : wait_time template : conf/restperf/9.12.0/workload_detail.yaml unit : microsec type : average base : visits - Harvest Metric : qos_latency Description : This is the average response time for requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : latency template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : microsec type : average,no-zero-values base : ops REST : endpoint : api/cluster/counter/tables/qos metric : latency template : conf/restperf/9.12.0/workload.yaml unit : microsec type : average base : ops - Harvest Metric : qos_ops Description : Workload operations executed per second. ZAPI : endpoint : perf-object-get-instances workload metric : ops template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : ops template : conf/restperf/9.12.0/workload.yaml unit : per_sec type : rate - Harvest Metric : qos_read_data Description : This is the amount of data read per second from the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : read_data template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : read_data template : conf/restperf/9.12.0/workload.yaml unit : b_per_sec type : rate - Harvest Metric : qos_read_io_type Description : This is the percentage of read requests served from various components (such as buffer cache, ext_cache, disk, etc.). ZAPI : endpoint : perf-object-get-instances workload metric : read_io_type template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : percent type : percent base : read_io_type_base REST : endpoint : api/cluster/counter/tables/qos metric : read_io_type_percent template : conf/restperf/9.12.0/workload.yaml unit : percent type : percent base : read_io_type_base - Harvest Metric : qos_read_latency Description : This is the average response time for read requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : read_latency template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : microsec type : average,no-zero-values base : read_ops REST : endpoint : api/cluster/counter/tables/qos metric : read_latency template : conf/restperf/9.12.0/workload.yaml unit : microsec type : average base : read_ops - Harvest Metric : qos_read_ops Description : This is the rate of this workload's read operations that completed during the measurement interval. ZAPI : endpoint : perf-object-get-instances workload metric : read_ops template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : read_ops template : conf/restperf/9.12.0/workload.yaml unit : per_sec type : rate - Harvest Metric : qos_sequential_reads Description : This is the percentage of reads, performed on behalf of the workload, that were sequential. ZAPI : endpoint : perf-object-get-instances workload metric : sequential_reads template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : percent type : percent,no-zero-values base : sequential_reads_base REST : endpoint : api/cluster/counter/tables/qos metric : sequential_reads_percent template : conf/restperf/9.12.0/workload.yaml unit : percent type : percent base : sequential_reads_base - Harvest Metric : qos_sequential_writes Description : This is the percentage of writes, performed on behalf of the workload, that were sequential. This counter is only available on platforms with more than 4GB of NVRAM. ZAPI : endpoint : perf-object-get-instances workload metric : sequential_writes template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : percent type : percent,no-zero-values base : sequential_writes_base REST : endpoint : api/cluster/counter/tables/qos metric : sequential_writes_percent template : conf/restperf/9.12.0/workload.yaml unit : percent type : percent base : sequential_writes_base - Harvest Metric : qos_total_data Description : This is the total amount of data read/written per second from/to the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : total_data template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : total_data template : conf/restperf/9.12.0/workload.yaml unit : b_per_sec type : rate - Harvest Metric : qos_volume_latency Description : This is the average response time for requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : latency template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : microsec type : average,no-zero-values base : ops REST : endpoint : api/cluster/counter/tables/qos_volume metric : latency template : conf/restperf/9.12.0/workload_volume.yaml unit : microsec type : average base : ops - Harvest Metric : qos_volume_ops Description : This field is the workload's rate of operations that completed during the measurement interval; measured per second. ZAPI : endpoint : perf-object-get-instances workload_volume metric : ops template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : ops template : conf/restperf/9.12.0/workload_volume.yaml unit : per_sec type : rate - Harvest Metric : qos_volume_read_data Description : This is the amount of data read per second from the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : read_data template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : read_data template : conf/restperf/9.12.0/workload_volume.yaml unit : b_per_sec type : rate - Harvest Metric : qos_volume_read_io_type Description : This is the percentage of read requests served from various components (such as buffer cache, ext_cache, disk, etc.). ZAPI : endpoint : perf-object-get-instances workload_volume metric : read_io_type template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : percent type : percent base : read_io_type_base REST : endpoint : api/cluster/counter/tables/qos_volume metric : read_io_type_percent template : conf/restperf/9.12.0/workload_volume.yaml unit : percent type : percent base : read_io_type_base - Harvest Metric : qos_volume_read_latency Description : This is the average response time for read requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : read_latency template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : microsec type : average,no-zero-values base : read_ops REST : endpoint : api/cluster/counter/tables/qos_volume metric : read_latency template : conf/restperf/9.12.0/workload_volume.yaml unit : microsec type : average base : read_ops - Harvest Metric : qos_volume_read_ops Description : This is the rate of this workload's read operations that completed during the measurement interval. ZAPI : endpoint : perf-object-get-instances workload_volume metric : read_ops template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : read_ops template : conf/restperf/9.12.0/workload_volume.yaml unit : per_sec type : rate - Harvest Metric : qos_volume_sequential_reads Description : This is the percentage of reads, performed on behalf of the workload, that were sequential. ZAPI : endpoint : perf-object-get-instances workload_volume metric : sequential_reads template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : percent type : percent,no-zero-values base : sequential_reads_base REST : endpoint : api/cluster/counter/tables/qos_volume metric : sequential_reads_percent template : conf/restperf/9.12.0/workload_volume.yaml unit : percent type : percent base : sequential_reads_base - Harvest Metric : qos_volume_sequential_writes Description : This is the percentage of writes, performed on behalf of the workload, that were sequential. This counter is only available on platforms with more than 4GB of NVRAM. ZAPI : endpoint : perf-object-get-instances workload_volume metric : sequential_writes template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : percent type : percent,no-zero-values base : sequential_writes_base REST : endpoint : api/cluster/counter/tables/qos_volume metric : sequential_writes_percent template : conf/restperf/9.12.0/workload_volume.yaml unit : percent type : percent base : sequential_writes_base - Harvest Metric : qos_volume_total_data Description : This is the total amount of data read/written per second from/to the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : total_data template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : total_data template : conf/restperf/9.12.0/workload_volume.yaml unit : b_per_sec type : rate - Harvest Metric : qos_volume_write_data Description : This is the amount of data written per second to the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : write_data template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : write_data template : conf/restperf/9.12.0/workload_volume.yaml unit : b_per_sec type : rate - Harvest Metric : qos_volume_write_latency Description : This is the average response time for write requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload_volume metric : write_latency template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : microsec type : average,no-zero-values base : write_ops REST : endpoint : api/cluster/counter/tables/qos_volume metric : write_latency template : conf/restperf/9.12.0/workload_volume.yaml unit : microsec type : average base : write_ops - Harvest Metric : qos_volume_write_ops Description : This is the workload's write operations that completed during the measurement interval; measured per second. ZAPI : endpoint : perf-object-get-instances workload_volume metric : write_ops template : conf/zapiperf/cdot/9.8.0/workload_volume.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos_volume metric : write_ops template : conf/restperf/9.12.0/workload_volume.yaml unit : per_sec type : rate - Harvest Metric : qos_write_data Description : This is the amount of data written per second to the filer by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : write_data template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : b_per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : write_data template : conf/restperf/9.12.0/workload.yaml unit : b_per_sec type : rate - Harvest Metric : qos_write_latency Description : This is the average response time for write requests that were initiated by the workload. ZAPI : endpoint : perf-object-get-instances workload metric : write_latency template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : microsec type : average,no-zero-values base : write_ops REST : endpoint : api/cluster/counter/tables/qos metric : write_latency template : conf/restperf/9.12.0/workload.yaml unit : microsec type : average base : write_ops - Harvest Metric : qos_write_ops Description : This is the workload's write operations that completed during the measurement interval; measured per second. ZAPI : endpoint : perf-object-get-instances workload metric : write_ops template : conf/zapiperf/cdot/9.8.0/workload.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/qos metric : write_ops template : conf/restperf/9.12.0/workload.yaml unit : per_sec type : rate - Harvest Metric : qtree_cifs_ops Description : Number of CIFS operations per second to the qtree ZAPI : endpoint : perf-object-get-instances qtree metric : cifs_ops template : conf/zapiperf/cdot/9.8.0/qtree.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/qtree metric : cifs_ops template : conf/restperf/9.12.0/qtree.yaml unit : per_sec type : rate - Harvest Metric : qtree_id Description : The identifier for the qtree, unique within the qtree's volume. REST : endpoint : api/storage/qtrees metric : id template : conf/rest/9.12.0/qtree.yaml - Harvest Metric : qtree_internal_ops Description : Number of internal operations generated by activites such as snapmirror and backup per second to the qtree ZAPI : endpoint : perf-object-get-instances qtree metric : internal_ops template : conf/zapiperf/cdot/9.8.0/qtree.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/qtree metric : internal_ops template : conf/restperf/9.12.0/qtree.yaml unit : per_sec type : rate - Harvest Metric : qtree_nfs_ops Description : Number of NFS operations per second to the qtree ZAPI : endpoint : perf-object-get-instances qtree metric : nfs_ops template : conf/zapiperf/cdot/9.8.0/qtree.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/qtree metric : nfs_ops template : conf/restperf/9.12.0/qtree.yaml unit : per_sec type : rate - Harvest Metric : qtree_total_ops Description : Summation of NFS ops, CIFS ops, CSS ops and internal ops ZAPI : endpoint : perf-object-get-instances qtree metric : total_ops template : conf/zapiperf/cdot/9.8.0/qtree.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/qtree metric : total_ops template : conf/restperf/9.12.0/qtree.yaml unit : per_sec type : rate - Harvest Metric : security_audit_destination_port ZAPI : endpoint : cluster-log-forward-get-iter metric : cluster-log-forward-info.port template : conf/zapi/cdot/9.8.0/security_audit_dest.yaml - Harvest Metric : security_certificate_expiry_time Description : Certificate expiration time. Can be provided on POST if creating self-signed certificate. The expiration time range is between 1 day to 10 years. ZAPI : endpoint : security-certificate-get-iter metric : certificate-info.expiration-date template : conf/zapi/cdot/9.8.0/security_certificate.yaml REST : endpoint : api/security/certificates metric : expiry_time template : conf/rest/9.12.0/security_certificate.yaml - Harvest Metric : security_ssh_max_instances REST : endpoint : api/security/ssh metric : max_instances template : conf/rest/9.12.0/security_ssh.yaml - Harvest Metric : shelf_disk_count ZAPI : endpoint : storage-shelf-info-get-iter metric : storage-shelf-info.disk-count template : conf/zapi/cdot/9.8.0/shelf.yaml REST : endpoint : api/storage/shelves metric : disk_count template : conf/rest/9.12.0/shelf.yaml - Harvest Metric : snapmirror_break_failed_count Description : The number of failed SnapMirror break operations for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.break-failed-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : break_failed_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_break_successful_count Description : The number of successful SnapMirror break operations for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.break-successful-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : break_successful_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_lag_time Description : Amount of time since the last snapmirror transfer in seconds ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.lag-time template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : lag_time template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_last_transfer_duration Description : Duration of the last SnapMirror transfer in seconds ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.last-transfer-duration template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : last_transfer_duration template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_last_transfer_end_timestamp Description : The Timestamp of the end of the last transfer ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.last-transfer-end-timestamp template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : last_transfer_end_timestamp template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_last_transfer_size Description : Size in kilobytes (1024 bytes) of the last transfer ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.last-transfer-size template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : last_transfer_size template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_newest_snapshot_timestamp Description : The timestamp of the newest Snapshot copy on the destination volume ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.newest-snapshot-timestamp template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : newest_snapshot_timestamp template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_resync_failed_count Description : The number of failed SnapMirror resync operations for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.resync-failed-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : resync_failed_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_resync_successful_count Description : The number of successful SnapMirror resync operations for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.resync-successful-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : resync_successful_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_total_transfer_bytes Description : Cumulative bytes transferred for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.total-transfer-bytes template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : total_transfer_bytes template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_total_transfer_time_secs Description : Cumulative total transfer time in seconds for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.total-transfer-time-secs template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : total_transfer_time_secs template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_update_failed_count Description : The number of successful SnapMirror update operations for the relationship ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.update-failed-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : update_failed_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapmirror_update_successful_count Description : Number of Successful Updates ZAPI : endpoint : snapmirror-get-iter metric : snapmirror-info.update-successful-count template : conf/zapi/cdot/9.8.0/snapmirror.yaml REST : endpoint : api/private/cli/snapmirror metric : update_successful_count template : conf/rest/9.12.0/snapmirror.yaml - Harvest Metric : snapshot_policy_total_schedules Description : Total Number of Schedules in this Policy ZAPI : endpoint : snapshot-policy-get-iter metric : snapshot-policy-info.total-schedules template : conf/zapi/cdot/9.8.0/snapshotPolicy.yaml REST : endpoint : api/private/cli/snapshot/policy metric : total_schedules template : conf/rest/9.12.0/snapshotPolicy.yaml - Harvest Metric : svm_cifs_connections Description : Number of connections ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : connections template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs metric : connections template : conf/restperf/9.12.0/cifs_svm.yaml unit : none type : raw - Harvest Metric : svm_cifs_established_sessions Description : Number of established SMB and SMB2 sessions ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : established_sessions template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs metric : established_sessions template : conf/restperf/9.12.0/cifs_svm.yaml unit : none type : raw - Harvest Metric : svm_cifs_latency Description : Average latency for CIFS operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_latency template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : microsec type : average base : cifs_latency_base REST : endpoint : api/cluster/counter/tables/svm_cifs metric : latency template : conf/restperf/9.12.0/cifs_svm.yaml unit : microsec type : average base : latency_base - Harvest Metric : svm_cifs_op_count Description : Array of select CIFS operation counts ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_op_count template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs metric : op_count template : conf/restperf/9.12.0/cifs_svm.yaml unit : none type : rate - Harvest Metric : svm_cifs_open_files Description : Number of open files over SMB and SMB2 ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : open_files template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs metric : open_files template : conf/restperf/9.12.0/cifs_svm.yaml unit : none type : raw - Harvest Metric : svm_cifs_ops Description : Total number of CIFS operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_ops template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs metric : total_ops template : conf/restperf/9.12.0/cifs_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_cifs_read_latency Description : Average latency for CIFS read operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_read_latency template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : microsec type : average base : cifs_read_ops REST : endpoint : api/cluster/counter/tables/svm_cifs metric : average_read_latency template : conf/restperf/9.12.0/cifs_svm.yaml unit : microsec type : average base : total_read_ops - Harvest Metric : svm_cifs_read_ops Description : Total number of CIFS read operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_read_ops template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs metric : total_read_ops template : conf/restperf/9.12.0/cifs_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_cifs_signed_sessions Description : Number of signed SMB and SMB2 sessions. ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : signed_sessions template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : none type : raw REST : endpoint : api/cluster/counter/tables/svm_cifs metric : signed_sessions template : conf/restperf/9.12.0/cifs_svm.yaml unit : none type : raw - Harvest Metric : svm_cifs_write_latency Description : Average latency for CIFS write operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_write_latency template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : microsec type : average base : cifs_write_ops REST : endpoint : api/cluster/counter/tables/svm_cifs metric : average_write_latency template : conf/restperf/9.12.0/cifs_svm.yaml unit : microsec type : average base : total_write_ops - Harvest Metric : svm_cifs_write_ops Description : Total number of CIFS write operations ZAPI : endpoint : perf-object-get-instances cifs:vserver metric : cifs_write_ops template : conf/zapiperf/cdot/9.8.0/cifs_vserver.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/svm_cifs metric : total_write_ops template : conf/restperf/9.12.0/cifs_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_access_avg_latency Description : Average latency of NFSv4.2 ACCESS operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : access_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : access_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : access.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : access.total - Harvest Metric : svm_nfs_access_total Description : Total number of NFSv4.2 ACCESS operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : access_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : access.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_backchannel_ctl_avg_latency Description : Average latency of NFSv4.2 BACKCHANNEL_CTL operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : backchannel_ctl_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : backchannel_ctl_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : backchannel_ctl.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : backchannel_ctl.total - Harvest Metric : svm_nfs_backchannel_ctl_total Description : Total number of NFSv4.2 BACKCHANNEL_CTL operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : backchannel_ctl_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : backchannel_ctl.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_bind_conn_to_session_avg_latency Description : Average latency of NFSv4.2 BIND_CONN_TO_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : bind_conn_to_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : bind_conn_to_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : bind_conn_to_session.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : bind_conn_to_session.total - Harvest Metric : svm_nfs_bind_conn_to_session_total Description : Total number of NFSv4.2 BIND_CONN_TO_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : bind_conn_to_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : bind_conn_to_session.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : delta - Harvest Metric : svm_nfs_close_avg_latency Description : Average latency of NFSv4.2 CLOSE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : close_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : close_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : close.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : close.total - Harvest Metric : svm_nfs_close_total Description : Total number of NFSv4.2 CLOSE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : close_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : close.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_commit_avg_latency Description : Average latency of NFSv4.2 COMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : commit_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : commit_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : commit.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : commit.total - Harvest Metric : svm_nfs_commit_total Description : Total number of NFSv4.2 COMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : commit_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : commit.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_create_avg_latency Description : Average latency of NFSv4.2 CREATE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : create_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : create_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : create.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : create.total - Harvest Metric : svm_nfs_create_session_avg_latency Description : Average latency of NFSv4.2 CREATE_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : create_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : create_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : create_session.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : create_session.total - Harvest Metric : svm_nfs_create_session_total Description : Total number of NFSv4.2 CREATE_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : create_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : create_session.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_create_total Description : Total number of NFSv4.2 CREATE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : create_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : create.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_delegpurge_avg_latency Description : Average latency of NFSv4.2 DELEGPURGE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : delegpurge_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : delegpurge_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : delegpurge.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : delegpurge.total - Harvest Metric : svm_nfs_delegpurge_total Description : Total number of NFSv4.2 DELEGPURGE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : delegpurge_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : delegpurge.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_delegreturn_avg_latency Description : Average latency of NFSv4.2 DELEGRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : delegreturn_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : delegreturn_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : delegreturn.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : delegreturn.total - Harvest Metric : svm_nfs_delegreturn_total Description : Total number of NFSv4.2 DELEGRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : delegreturn_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : delegreturn.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_destroy_clientid_avg_latency Description : Average latency of NFSv4.2 DESTROY_CLIENTID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : destroy_clientid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : destroy_clientid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : destroy_clientid.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : destroy_clientid.total - Harvest Metric : svm_nfs_destroy_clientid_total Description : Total number of NFSv4.2 DESTROY_CLIENTID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : destroy_clientid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : destroy_clientid.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_destroy_session_avg_latency Description : Average latency of NFSv4.2 DESTROY_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : destroy_session_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : destroy_session_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : destroy_session.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : destroy_session.total - Harvest Metric : svm_nfs_destroy_session_total Description : Total number of NFSv4.2 DESTROY_SESSION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : destroy_session_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : destroy_session.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_exchange_id_avg_latency Description : Average latency of NFSv4.2 EXCHANGE_ID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : exchange_id_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : exchange_id_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : exchange_id.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : exchange_id.total - Harvest Metric : svm_nfs_exchange_id_total Description : Total number of NFSv4.2 EXCHANGE_ID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : exchange_id_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : exchange_id.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_free_stateid_avg_latency Description : Average latency of NFSv4.2 FREE_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : free_stateid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : free_stateid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : free_stateid.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : free_stateid.total - Harvest Metric : svm_nfs_free_stateid_total Description : Total number of NFSv4.2 FREE_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : free_stateid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : free_stateid.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_fsinfo_avg_latency Description : Average latency of FSInfo procedure requests. The counter keeps track of the average response time of FSInfo requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : fsinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : fsinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : fsinfo.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : fsinfo.total - Harvest Metric : svm_nfs_fsinfo_total Description : Total number FSInfo of procedure requests. It is the total number of FSInfo success and FSInfo error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : fsinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : fsinfo.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_fsstat_avg_latency Description : Average latency of FSStat procedure requests. The counter keeps track of the average response time of FSStat requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : fsstat_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : fsstat_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : fsstat.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : fsstat.total - Harvest Metric : svm_nfs_fsstat_total Description : Total number FSStat of procedure requests. It is the total number of FSStat success and FSStat error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : fsstat_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : fsstat.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_get_dir_delegation_avg_latency Description : Average latency of NFSv4.2 GET_DIR_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : get_dir_delegation_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : get_dir_delegation_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : get_dir_delegation.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : get_dir_delegation.total - Harvest Metric : svm_nfs_get_dir_delegation_total Description : Total number of NFSv4.2 GET_DIR_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : get_dir_delegation_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : get_dir_delegation.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_getattr_avg_latency Description : Average latency of NFSv4.2 GETATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : getattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getattr.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : getattr.total - Harvest Metric : svm_nfs_getattr_total Description : Total number of NFSv4.2 GETATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getattr.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_getdeviceinfo_avg_latency Description : Average latency of NFSv4.2 GETDEVICEINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getdeviceinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : getdeviceinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getdeviceinfo.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : getdeviceinfo.total - Harvest Metric : svm_nfs_getdeviceinfo_total Description : Total number of NFSv4.2 GETDEVICEINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getdeviceinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getdeviceinfo.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_getdevicelist_avg_latency Description : Average latency of NFSv4.2 GETDEVICELIST operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getdevicelist_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : getdevicelist_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getdevicelist.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : getdevicelist.total - Harvest Metric : svm_nfs_getdevicelist_total Description : Total number of NFSv4.2 GETDEVICELIST operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getdevicelist_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getdevicelist.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_getfh_avg_latency Description : Average latency of NFSv4.2 GETFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : getfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getfh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : getfh.total - Harvest Metric : svm_nfs_getfh_total Description : Total number of NFSv4.2 GETFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : getfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : getfh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_latency Description : Average latency of nfsv42 requests. This counter keeps track of the average response time of nfsv42 requests. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : total_ops REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : total_ops - Harvest Metric : svm_nfs_layoutcommit_avg_latency Description : Average latency of NFSv4.2 LAYOUTCOMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutcommit_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : layoutcommit_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutcommit.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : layoutcommit.total - Harvest Metric : svm_nfs_layoutcommit_total Description : Total number of NFSv4.2 LAYOUTCOMMIT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutcommit_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutcommit.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_layoutget_avg_latency Description : Average latency of NFSv4.2 LAYOUTGET operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutget_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : layoutget_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutget.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : layoutget.total - Harvest Metric : svm_nfs_layoutget_total Description : Total number of NFSv4.2 LAYOUTGET operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutget_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutget.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_layoutreturn_avg_latency Description : Average latency of NFSv4.2 LAYOUTRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutreturn_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : layoutreturn_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutreturn.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : layoutreturn.total - Harvest Metric : svm_nfs_layoutreturn_total Description : Total number of NFSv4.2 LAYOUTRETURN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : layoutreturn_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : layoutreturn.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_link_avg_latency Description : Average latency of NFSv4.2 LINK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : link_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : link_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : link.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : link.total - Harvest Metric : svm_nfs_link_total Description : Total number of NFSv4.2 LINK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : link_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : link.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_lock_avg_latency Description : Average latency of NFSv4.2 LOCK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lock_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : lock_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lock.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : lock.total - Harvest Metric : svm_nfs_lock_total Description : Total number of NFSv4.2 LOCK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lock_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lock.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_lockt_avg_latency Description : Average latency of NFSv4.2 LOCKT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lockt_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : lockt_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lockt.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : lockt.total - Harvest Metric : svm_nfs_lockt_total Description : Total number of NFSv4.2 LOCKT operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lockt_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lockt.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_locku_avg_latency Description : Average latency of NFSv4.2 LOCKU operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : locku_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : locku_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : locku.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : locku.total - Harvest Metric : svm_nfs_locku_total Description : Total number of NFSv4.2 LOCKU operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : locku_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : locku.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_lookup_avg_latency Description : Average latency of NFSv4.2 LOOKUP operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lookup_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : lookup_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lookup.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : lookup.total - Harvest Metric : svm_nfs_lookup_total Description : Total number of NFSv4.2 LOOKUP operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lookup_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lookup.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_lookupp_avg_latency Description : Average latency of NFSv4.2 LOOKUPP operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lookupp_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : lookupp_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lookupp.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : lookupp.total - Harvest Metric : svm_nfs_lookupp_total Description : Total number of NFSv4.2 LOOKUPP operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : lookupp_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : lookupp.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_mkdir_avg_latency Description : Average latency of MkDir procedure requests. The counter keeps track of the average response time of MkDir requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : mkdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : mkdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : mkdir.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : mkdir.total - Harvest Metric : svm_nfs_mkdir_total Description : Total number MkDir of procedure requests. It is the total number of MkDir success and MkDir error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : mkdir_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : mkdir.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_mknod_avg_latency Description : Average latency of MkNod procedure requests. The counter keeps track of the average response time of MkNod requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : mknod_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : mknod_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : mknod.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : mknod.total - Harvest Metric : svm_nfs_mknod_total Description : Total number MkNod of procedure requests. It is the total number of MkNod success and MkNod error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : mknod_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : mknod.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_null_avg_latency Description : Average latency of NFSv4.2 NULL procedures. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : null_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : null_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : null.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : null.total - Harvest Metric : svm_nfs_null_total Description : Total number of NFSv4.2 NULL procedures. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : null_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : null.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_nverify_avg_latency Description : Average latency of NFSv4.2 NVERIFY operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : nverify_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : nverify_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : nverify.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : nverify.total - Harvest Metric : svm_nfs_nverify_total Description : Total number of NFSv4.2 NVERIFY operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : nverify_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : nverify.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_open_avg_latency Description : Average latency of NFSv4.2 OPEN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : open_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : open_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : open.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : open.total - Harvest Metric : svm_nfs_open_confirm_avg_latency Description : Average latency of OPEN_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : open_confirm_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : microsec type : average,no-zero-values base : open_confirm_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : open_confirm.average_latency template : conf/restperf/9.12.0/nfsv4.yaml unit : microsec type : average base : open_confirm.total - Harvest Metric : svm_nfs_open_confirm_total Description : Total number of OPEN_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : open_confirm_total template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : open_confirm.total template : conf/restperf/9.12.0/nfsv4.yaml unit : none type : rate - Harvest Metric : svm_nfs_open_downgrade_avg_latency Description : Average latency of NFSv4.2 OPEN_DOWNGRADE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : open_downgrade_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : open_downgrade_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : open_downgrade.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : open_downgrade.total - Harvest Metric : svm_nfs_open_downgrade_total Description : Total number of NFSv4.2 OPEN_DOWNGRADE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : open_downgrade_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : open_downgrade.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_open_total Description : Total number of NFSv4.2 OPEN operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : open_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : open.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_openattr_avg_latency Description : Average latency of NFSv4.2 OPENATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : openattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : openattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : openattr.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : openattr.total - Harvest Metric : svm_nfs_openattr_total Description : Total number of NFSv4.2 OPENATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : openattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : openattr.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_ops Description : Total number of nfsv42 requests per sec. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : total_ops template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : total_ops template : conf/restperf/9.12.0/nfsv4_2.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_pathconf_avg_latency Description : Average latency of PathConf procedure requests. The counter keeps track of the average response time of PathConf requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : pathconf_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : pathconf_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : pathconf.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : pathconf.total - Harvest Metric : svm_nfs_pathconf_total Description : Total number PathConf of procedure requests. It is the total number of PathConf success and PathConf error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : pathconf_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : pathconf.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_putfh_avg_latency Description : Average latency of NFSv4.2 PUTFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : putfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putfh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : putfh.total - Harvest Metric : svm_nfs_putfh_total Description : Total number of NFSv4.2 PUTFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putfh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_putpubfh_avg_latency Description : Average latency of NFSv4.2 PUTPUBFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putpubfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : putpubfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putpubfh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : putpubfh.total - Harvest Metric : svm_nfs_putpubfh_total Description : Total number of NFSv4.2 PUTPUBFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putpubfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putpubfh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_putrootfh_avg_latency Description : Average latency of NFSv4.2 PUTROOTFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putrootfh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : putrootfh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putrootfh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : putrootfh.total - Harvest Metric : svm_nfs_putrootfh_total Description : Total number of NFSv4.2 PUTROOTFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : putrootfh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : putrootfh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_read_avg_latency Description : Average latency of NFSv4.2 READ operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : read_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : read_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : read.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : read.total - Harvest Metric : svm_nfs_read_ops Description : Total observed NFSv3 read operations per second. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : nfsv3_read_ops template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : read_ops template : conf/restperf/9.12.0/nfsv3.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_read_symlink_avg_latency Description : Average latency of ReadSymLink procedure requests. The counter keeps track of the average response time of ReadSymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : read_symlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : read_symlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : read_symlink.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : read_symlink.total - Harvest Metric : svm_nfs_read_symlink_total Description : Total number of ReadSymLink procedure requests. It is the total number of read symlink success and read symlink error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : read_symlink_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : read_symlink.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : delta - Harvest Metric : svm_nfs_read_throughput Description : NFSv4.2 read data transfers. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : nfs41_read_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : total.read_throughput template : conf/restperf/9.12.0/nfsv4_2.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_read_total Description : Total number of NFSv4.2 READ operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : read_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : read.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_readdir_avg_latency Description : Average latency of NFSv4.2 READDIR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : readdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : readdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : readdir.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : readdir.total - Harvest Metric : svm_nfs_readdir_total Description : Total number of NFSv4.2 READDIR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : readdir_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : readdir.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_readdirplus_avg_latency Description : Average latency of ReadDirPlus procedure requests. The counter keeps track of the average response time of ReadDirPlus requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : readdirplus_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : readdirplus_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : readdirplus.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : readdirplus.total - Harvest Metric : svm_nfs_readdirplus_total Description : Total number ReadDirPlus of procedure requests. It is the total number of ReadDirPlus success and ReadDirPlus error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : readdirplus_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : readdirplus.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_readlink_avg_latency Description : Average latency of NFSv4.2 READLINK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : readlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : readlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : readlink.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : readlink.total - Harvest Metric : svm_nfs_readlink_total Description : Total number of NFSv4.2 READLINK operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : readlink_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : readlink.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_reclaim_complete_avg_latency Description : Average latency of NFSv4.2 RECLAIM_complete operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : reclaim_complete_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : reclaim_complete_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : reclaim_complete.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : reclaim_complete.total - Harvest Metric : svm_nfs_reclaim_complete_total Description : Total number of NFSv4.2 RECLAIM_complete operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : reclaim_complete_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : reclaim_complete.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_release_lock_owner_avg_latency Description : Average Latency of RELEASE_LOCKOWNER procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : release_lock_owner_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : microsec type : average,no-zero-values base : release_lock_owner_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : release_lock_owner.average_latency template : conf/restperf/9.12.0/nfsv4.yaml unit : microsec type : average base : release_lock_owner.total - Harvest Metric : svm_nfs_release_lock_owner_total Description : Total number of RELEASE_LOCKOWNER procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : release_lock_owner_total template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : release_lock_owner.total template : conf/restperf/9.12.0/nfsv4.yaml unit : none type : rate - Harvest Metric : svm_nfs_remove_avg_latency Description : Average latency of NFSv4.2 REMOVE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : remove_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : remove_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : remove.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : remove.total - Harvest Metric : svm_nfs_remove_total Description : Total number of NFSv4.2 REMOVE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : remove_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : remove.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_rename_avg_latency Description : Average latency of NFSv4.2 RENAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : rename_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : rename_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : rename.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : rename.total - Harvest Metric : svm_nfs_rename_total Description : Total number of NFSv4.2 RENAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : rename_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : rename.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_renew_avg_latency Description : Average latency of RENEW procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : renew_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : microsec type : average,no-zero-values base : renew_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : renew.average_latency template : conf/restperf/9.12.0/nfsv4.yaml unit : microsec type : average base : renew.total - Harvest Metric : svm_nfs_renew_total Description : Total number of RENEW procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : renew_total template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : renew.total template : conf/restperf/9.12.0/nfsv4.yaml unit : none type : rate - Harvest Metric : svm_nfs_restorefh_avg_latency Description : Average latency of NFSv4.2 RESTOREFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : restorefh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : restorefh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : restorefh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : restorefh.total - Harvest Metric : svm_nfs_restorefh_total Description : Total number of NFSv4.2 RESTOREFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : restorefh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : restorefh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_rmdir_avg_latency Description : Average latency of RmDir procedure requests. The counter keeps track of the average response time of RmDir requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : rmdir_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : rmdir_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : rmdir.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : rmdir.total - Harvest Metric : svm_nfs_rmdir_total Description : Total number RmDir of procedure requests. It is the total number of RmDir success and RmDir error requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : rmdir_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : rmdir.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_savefh_avg_latency Description : Average latency of NFSv4.2 SAVEFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : savefh_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : savefh_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : savefh.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : savefh.total - Harvest Metric : svm_nfs_savefh_total Description : Total number of NFSv4.2 SAVEFH operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : savefh_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : savefh.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_secinfo_avg_latency Description : Average latency of NFSv4.2 SECINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : secinfo_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : secinfo_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : secinfo.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : secinfo.total - Harvest Metric : svm_nfs_secinfo_no_name_avg_latency Description : Average latency of NFSv4.2 SECINFO_NO_NAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : secinfo_no_name_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : secinfo_no_name_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : secinfo_no_name.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : secinfo_no_name.total - Harvest Metric : svm_nfs_secinfo_no_name_total Description : Total number of NFSv4.2 SECINFO_NO_NAME operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : secinfo_no_name_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : secinfo_no_name.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_secinfo_total Description : Total number of NFSv4.2 SECINFO operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : secinfo_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : secinfo.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_sequence_avg_latency Description : Average latency of NFSv4.2 SEQUENCE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : sequence_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : sequence_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : sequence.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : sequence.total - Harvest Metric : svm_nfs_sequence_total Description : Total number of NFSv4.2 SEQUENCE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : sequence_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : sequence.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_set_ssv_avg_latency Description : Average latency of NFSv4.2 SET_SSV operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : set_ssv_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : set_ssv_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : set_ssv.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : set_ssv.total - Harvest Metric : svm_nfs_set_ssv_total Description : Total number of NFSv4.2 SET_SSV operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : set_ssv_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : set_ssv.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_setattr_avg_latency Description : Average latency of NFSv4.2 SETATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : setattr_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : setattr_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : setattr.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : setattr.total - Harvest Metric : svm_nfs_setattr_total Description : Total number of NFSv4.2 SETATTR operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : setattr_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : setattr.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_setclientid_avg_latency Description : Average latency of SETCLIENTID procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : setclientid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : microsec type : average,no-zero-values base : setclientid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : setclientid.average_latency template : conf/restperf/9.12.0/nfsv4.yaml unit : microsec type : average base : setclientid.total - Harvest Metric : svm_nfs_setclientid_confirm_avg_latency Description : Average latency of SETCLIENTID_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : setclientid_confirm_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : microsec type : average,no-zero-values base : setclientid_confirm_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : setclientid_confirm.average_latency template : conf/restperf/9.12.0/nfsv4.yaml unit : microsec type : average base : setclientid_confirm.total - Harvest Metric : svm_nfs_setclientid_confirm_total Description : Total number of SETCLIENTID_CONFIRM procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : setclientid_confirm_total template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : setclientid_confirm.total template : conf/restperf/9.12.0/nfsv4.yaml unit : none type : rate - Harvest Metric : svm_nfs_setclientid_total Description : Total number of SETCLIENTID procedures ZAPI : endpoint : perf-object-get-instances nfsv4 metric : setclientid_total template : conf/zapiperf/cdot/9.8.0/nfsv4.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v4 metric : setclientid.total template : conf/restperf/9.12.0/nfsv4.yaml unit : none type : rate - Harvest Metric : svm_nfs_symlink_avg_latency Description : Average latency of SymLink procedure requests. The counter keeps track of the average response time of SymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : symlink_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : microsec type : average,no-zero-values base : symlink_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : symlink.average_latency template : conf/restperf/9.12.0/nfsv3.yaml unit : microsec type : average base : symlink.total - Harvest Metric : svm_nfs_symlink_total Description : Total number SymLink of procedure requests. It is the total number of SymLink success and create SymLink requests. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : symlink_total template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : symlink.total template : conf/restperf/9.12.0/nfsv3.yaml unit : none type : rate - Harvest Metric : svm_nfs_test_stateid_avg_latency Description : Average latency of NFSv4.2 TEST_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : test_stateid_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : test_stateid_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : test_stateid.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : test_stateid.total - Harvest Metric : svm_nfs_test_stateid_total Description : Total number of NFSv4.2 TEST_STATEID operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : test_stateid_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : test_stateid.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_throughput Description : NFSv4.2 write data transfers. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : nfs41_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : total.write_throughput template : conf/restperf/9.12.0/nfsv4_2.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_verify_avg_latency Description : Average latency of NFSv4.2 VERIFY operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : verify_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : verify_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : verify.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : verify.total - Harvest Metric : svm_nfs_verify_total Description : Total number of NFSv4.2 VERIFY operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : verify_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : verify.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_want_delegation_avg_latency Description : Average latency of NFSv4.2 WANT_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : want_delegation_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : want_delegation_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : want_delegation.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : want_delegation.total - Harvest Metric : svm_nfs_want_delegation_total Description : Total number of NFSv4.2 WANT_DELEGATION operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : want_delegation_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : want_delegation.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_nfs_write_avg_latency Description : Average latency of NFSv4.2 WRITE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : write_avg_latency template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : microsec type : average,no-zero-values base : write_total REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : write.average_latency template : conf/restperf/9.12.0/nfsv4_2.yaml unit : microsec type : average base : write.total - Harvest Metric : svm_nfs_write_ops Description : Total observed NFSv3 write operations per second. ZAPI : endpoint : perf-object-get-instances nfsv3 metric : nfsv3_write_ops template : conf/zapiperf/cdot/9.8.0/nfsv3.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v3 metric : write_ops template : conf/restperf/9.12.0/nfsv3.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_write_throughput Description : NFSv4.2 data transfers. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : nfs41_write_throughput template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : per_sec type : rate,no-zero-values REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : total.throughput template : conf/restperf/9.12.0/nfsv4_2.yaml unit : per_sec type : rate - Harvest Metric : svm_nfs_write_total Description : Total number of NFSv4.2 WRITE operations. ZAPI : endpoint : perf-object-get-instances nfsv4_1 metric : write_total template : conf/zapiperf/cdot/9.8.0/nfsv4_1.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/svm_nfs_v42 metric : write.total template : conf/restperf/9.12.0/nfsv4_2.yaml unit : none type : rate - Harvest Metric : svm_vol_avg_latency Description : Average latency in microseconds for the WAFL filesystem to process all the operations on the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:vserver metric : avg_latency template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : microsec type : average base : total_ops REST : endpoint : api/cluster/counter/tables/volume:svm metric : average_latency template : conf/restperf/9.12.0/volume_svm.yaml unit : microsec type : average base : total_ops - Harvest Metric : svm_vol_other_latency Description : Average latency in microseconds for the WAFL filesystem to process other operations to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:vserver metric : other_latency template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/volume:svm metric : other_latency template : conf/restperf/9.12.0/volume_svm.yaml unit : microsec type : average base : total_other_ops - Harvest Metric : svm_vol_other_ops Description : Number of other operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:vserver metric : other_ops template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : total_other_ops template : conf/restperf/9.12.0/volume_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_vol_read_data Description : Bytes read per second ZAPI : endpoint : perf-object-get-instances volume:vserver metric : read_data template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : bytes_read template : conf/restperf/9.12.0/volume_svm.yaml unit : b_per_sec type : rate - Harvest Metric : svm_vol_read_latency Description : Average latency in microseconds for the WAFL filesystem to process read request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:vserver metric : read_latency template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/volume:svm metric : read_latency template : conf/restperf/9.12.0/volume_svm.yaml unit : microsec type : average base : total_read_ops - Harvest Metric : svm_vol_read_ops Description : Number of read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume:vserver metric : read_ops template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : total_read_ops template : conf/restperf/9.12.0/volume_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_vol_total_ops Description : Number of operations per second serviced by the volume ZAPI : endpoint : perf-object-get-instances volume:vserver metric : total_ops template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : total_ops template : conf/restperf/9.12.0/volume_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_vol_write_data Description : Bytes written per second ZAPI : endpoint : perf-object-get-instances volume:vserver metric : write_data template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : bytes_written template : conf/restperf/9.12.0/volume_svm.yaml unit : b_per_sec type : rate - Harvest Metric : svm_vol_write_latency Description : Average latency in microseconds for the WAFL filesystem to process write request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume:vserver metric : write_latency template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/volume:svm metric : write_latency template : conf/restperf/9.12.0/volume_svm.yaml unit : microsec type : average base : total_write_ops - Harvest Metric : svm_vol_write_ops Description : Number of write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume:vserver metric : write_ops template : conf/zapiperf/cdot/9.8.0/volume_svm.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume:svm metric : total_write_ops template : conf/restperf/9.12.0/volume_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_vscan_connections_active Description : Total number of current active connections ZAPI : endpoint : perf-object-get-instances offbox_vscan metric : connections_active template : conf/zapiperf/cdot/9.8.0/vscan_svm.yaml unit : none type : raw - Harvest Metric : svm_vscan_dispatch_latency Description : Average dispatch latency ZAPI : endpoint : perf-object-get-instances offbox_vscan metric : dispatch_latency template : conf/zapiperf/cdot/9.8.0/vscan_svm.yaml unit : microsec type : average base : dispatch_latency_base - Harvest Metric : svm_vscan_scan_latency Description : Average scan latency ZAPI : endpoint : perf-object-get-instances offbox_vscan metric : scan_latency template : conf/zapiperf/cdot/9.8.0/vscan_svm.yaml unit : microsec type : average base : scan_latency_base - Harvest Metric : svm_vscan_scan_noti_received_rate Description : Total number of scan notifications received by the dispatcher per second ZAPI : endpoint : perf-object-get-instances offbox_vscan metric : scan_noti_received_rate template : conf/zapiperf/cdot/9.8.0/vscan_svm.yaml unit : per_sec type : rate - Harvest Metric : svm_vscan_scan_request_dispatched_rate Description : Total number of scan requests sent to the Vscanner per second ZAPI : endpoint : perf-object-get-instances offbox_vscan metric : scan_request_dispatched_rate template : conf/zapiperf/cdot/9.8.0/vscan_svm.yaml unit : per_sec type : rate - Harvest Metric : token_copy_bytes Description : Total number of bytes copied. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_copy_bytes template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/token_manager metric : token_copy.bytes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : rate - Harvest Metric : token_copy_failure Description : Number of failed token copy requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_copy_failure template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_copy.failures template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : token_copy_success Description : Number of successful token copy requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_copy_success template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_copy.successes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : token_create_bytes Description : Total number of bytes for which tokens are created. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_create_bytes template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/token_manager metric : token_create.bytes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : rate - Harvest Metric : token_create_failure Description : Number of failed token create requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_create_failure template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_create.failures template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : token_create_success Description : Number of successful token create requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_create_success template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_create.successes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : token_zero_bytes Description : Total number of bytes zeroed. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_zero_bytes template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : rate REST : endpoint : api/cluster/counter/tables/token_manager metric : token_zero.bytes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : rate - Harvest Metric : token_zero_failure Description : Number of failed token zero requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_zero_failure template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_zero.failures template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : token_zero_success Description : Number of successful token zero requests. ZAPI : endpoint : perf-object-get-instances token_manager metric : token_zero_success template : conf/zapiperf/cdot/9.8.0/token_manager.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/token_manager metric : token_zero.successes template : conf/restperf/9.12.0/token_manager.yaml unit : none type : delta - Harvest Metric : volume_autosize_grow_threshold_percent Description : Used space threshold size, in percentage, for the automatic growth of the volume. When the amount of used space in the volume becomes greater than this threhold, the volume automatically grows unless it has reached the maximum size. The volume grows when 'space.used' is greater than this percent of 'space.size'. The 'grow_threshold' size cannot be less than or equal to the 'shrink_threshold' size.. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-autosize-attributes.grow-threshold-percent template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : autosize.grow_threshold template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_autosize_maximum_size Description : Maximum size in bytes up to which a volume grows automatically. This size cannot be less than the current volume size, or less than or equal to the minimum size of volume. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-autosize-attributes.maximum-size template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : autosize.maximum template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_avg_latency Description : Average latency in microseconds for the WAFL filesystem to process all the operations on the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume metric : avg_latency template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : microsec type : average base : total_ops REST : endpoint : api/cluster/counter/tables/volume metric : average_latency template : conf/restperf/9.12.0/volume.yaml unit : microsec type : average base : total_ops - Harvest Metric : volume_filesystem_size Description : Total usable size of the volume, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.filesystem-size template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.filesystem_size template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_inode_files_total Description : Total user-visible file (inode) count, i.e., current maximum number of user-visible files (inodes) that this volume can currently hold. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-inode-attributes.files-total template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : files template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_inode_files_used Description : Number of user-visible files (inodes) used. This field is valid only when the volume is online. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-inode-attributes.files-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : files_used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_other_latency Description : Average latency in microseconds for the WAFL filesystem to process other operations to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume metric : other_latency template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : microsec type : average base : other_ops REST : endpoint : api/cluster/counter/tables/volume metric : other_latency template : conf/restperf/9.12.0/volume.yaml unit : microsec type : average base : total_other_ops - Harvest Metric : volume_other_ops Description : Number of other operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume metric : other_ops template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : total_other_ops template : conf/restperf/9.12.0/volume.yaml unit : per_sec type : rate - Harvest Metric : volume_read_data Description : Bytes read per second ZAPI : endpoint : perf-object-get-instances volume metric : read_data template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : bytes_read template : conf/restperf/9.12.0/volume.yaml unit : b_per_sec type : rate - Harvest Metric : volume_read_latency Description : Average latency in microseconds for the WAFL filesystem to process read request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume metric : read_latency template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : microsec type : average base : read_ops REST : endpoint : api/cluster/counter/tables/volume metric : read_latency template : conf/restperf/9.12.0/volume.yaml unit : microsec type : average base : total_read_ops - Harvest Metric : volume_read_ops Description : Number of read operations per second from the volume ZAPI : endpoint : perf-object-get-instances volume metric : read_ops template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : total_read_ops template : conf/restperf/9.12.0/volume.yaml unit : per_sec type : rate - Harvest Metric : volume_sis_compress_saved Description : The total disk space (in bytes) that is saved by compressing blocks on the referenced file system. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.compression-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : compression_space_saved template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_sis_compress_saved_percent Description : Percentage of the total disk space that is saved by compressing blocks on the referenced file system ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.percentage-compression-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : compression_space_saved_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_sis_dedup_saved Description : The total disk space (in bytes) that is saved by deduplication and file cloning. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.deduplication-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : dedupe_space_saved template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_sis_dedup_saved_percent Description : Percentage of the total disk space that is saved by deduplication and file cloning. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.percentage-deduplication-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : dedupe_space_saved_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_sis_total_saved Description : Total space saved (in bytes) in the volume due to deduplication, compression, and file cloning. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.total-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : sis_space_saved template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_sis_total_saved_percent Description : Percentage of total disk space that is saved by compressing blocks, deduplication and file cloning. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-sis-attributes.percentage-total-space-saved template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/private/cli/volume metric : sis_space_saved_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_size Description : Total provisioned size. The default size is equal to the minimum size of 20MB, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.size template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_size_available Description : The available space, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size-available template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.available template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_size_total Description : Total size of AFS, excluding snap-reserve, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size-total template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.afs_total template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_size_used Description : The virtual space used (includes volume reserves) before storage efficiency, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_size_used_percent Description : Percentage of the volume size that is used. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.percentage-size-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.percent_used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshot_count Description : Number of Snapshot copies in the volume. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-snapshot-attributes.snapshot-count template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : snapshot_count template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshot_reserve_available Description : Size available for Snapshot copies within the Snapshot copy reserve, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.snapshot-reserve-available template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.snapshot.reserve_available template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshot_reserve_percent Description : The space that has been set aside as a reserve for Snapshot copy usage, in percent. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.percentage-snapshot-reserve template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.snapshot.reserve_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshot_reserve_size Description : Size in the volume that has been set aside as a reserve for Snapshot copy usage, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.snapshot-reserve-size template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.snapshot.reserve_size template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshot_reserve_used_percent Description : Percentage of snapshot reserve size that has been used. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.percentage-snapshot-reserve-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.snapshot.space_used_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshots_size_available Description : Available space for Snapshot copies from snap-reserve, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size-available-for-snapshots template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.size_available_for_snapshots template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_snapshots_size_used Description : The total space used by Snapshot copies in the volume, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.size-used-by-snapshots template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.snapshot.used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_expected_available Description : Size that should be available for the volume, irrespective of available size in the aggregate, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.expected-available template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.expected_available template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_logical_available Description : The amount of space available in this volume with storage efficiency space considered used, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.logical-available template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.logical_space.available template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_logical_used Description : SUM of (physical-used, shared_refs, compression_saved_in_plane0, vbn_zero, future_blk_cnt), in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.logical-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.logical_space.used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_logical_used_by_afs Description : The virtual space used by AFS alone (includes volume reserves) and along with storage efficiency, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.logical-used-by-afs template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.logical_space.used_by_afs template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_logical_used_by_snapshots Description : Size that is logically used across all Snapshot copies in the volume, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.logical-used-by-snapshots template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.logical_space.used_by_snapshots template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_logical_used_percent Description : SUM of (physical-used, shared_refs, compression_saved_in_plane0, vbn_zero, future_blk_cnt), as a percentage. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.logical-used-percent template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.logical_space.used_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_physical_used Description : Size that is physically used in the volume, in bytes. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.physical-used template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.physical_used template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_space_physical_used_percent Description : Size that is physically used in the volume, as a percentage. ZAPI : endpoint : volume-get-iter metric : volume-attributes.volume-space-attributes.physical-used-percent template : conf/zapi/cdot/9.8.0/volume.yaml REST : endpoint : api/storage/volumes metric : space.physical_used_percent template : conf/rest/9.12.0/volume.yaml - Harvest Metric : volume_total_ops Description : Number of operations per second serviced by the volume ZAPI : endpoint : perf-object-get-instances volume metric : total_ops template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : total_ops template : conf/restperf/9.12.0/volume.yaml unit : per_sec type : rate - Harvest Metric : volume_write_data Description : Bytes written per second ZAPI : endpoint : perf-object-get-instances volume metric : write_data template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : b_per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : bytes_written template : conf/restperf/9.12.0/volume.yaml unit : b_per_sec type : rate - Harvest Metric : volume_write_latency Description : Average latency in microseconds for the WAFL filesystem to process write request to the volume; not including request processing or network communication time ZAPI : endpoint : perf-object-get-instances volume metric : write_latency template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : microsec type : average base : write_ops REST : endpoint : api/cluster/counter/tables/volume metric : write_latency template : conf/restperf/9.12.0/volume.yaml unit : microsec type : average base : total_write_ops - Harvest Metric : volume_write_ops Description : Number of write operations per second to the volume ZAPI : endpoint : perf-object-get-instances volume metric : write_ops template : conf/zapiperf/cdot/9.8.0/volume.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/volume metric : total_write_ops template : conf/restperf/9.12.0/volume.yaml unit : per_sec type : rate - Harvest Metric : vscan_scan_latency Description : Average scan latency ZAPI : endpoint : perf-object-get-instances offbox_vscan_server metric : scan_latency template : conf/zapiperf/cdot/9.8.0/vscan.yaml unit : microsec type : average base : scan_latency_base - Harvest Metric : vscan_scan_request_dispatched_rate Description : Total number of scan requests sent to the Vscanner per second ZAPI : endpoint : perf-object-get-instances offbox_vscan_server metric : scan_request_dispatched_rate template : conf/zapiperf/cdot/9.8.0/vscan.yaml unit : per_sec type : rate - Harvest Metric : vscan_scanner_stats_pct_cpu_used Description : Percentage CPU utilization on scanner ZAPI : endpoint : perf-object-get-instances offbox_vscan_server metric : scanner_stats_pct_cpu_used template : conf/zapiperf/cdot/9.8.0/vscan.yaml unit : none type : raw - Harvest Metric : vscan_scanner_stats_pct_mem_used Description : Percentage RAM utilization on scanner ZAPI : endpoint : perf-object-get-instances offbox_vscan_server metric : scanner_stats_pct_mem_used template : conf/zapiperf/cdot/9.8.0/vscan.yaml unit : none type : raw - Harvest Metric : vscan_scanner_stats_pct_network_used Description : Percentage network utilization on scanner ZAPI : endpoint : perf-object-get-instances offbox_vscan_server metric : scanner_stats_pct_network_used template : conf/zapiperf/cdot/9.8.0/vscan.yaml unit : none type : raw - Harvest Metric : wafl_avg_msg_latency Description : Average turnaround time for WAFL messages in milliseconds. ZAPI : endpoint : perf-object-get-instances wafl metric : avg_wafl_msg_latency template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : millisec type : average base : wafl_msg_total REST : endpoint : api/cluster/counter/tables/wafl metric : average_msg_latency template : conf/restperf/9.12.0/wafl.yaml unit : millisec type : average base : msg_total - Harvest Metric : wafl_avg_non_wafl_msg_latency Description : Average turnaround time for non-WAFL messages in milliseconds. ZAPI : endpoint : perf-object-get-instances wafl metric : avg_non_wafl_msg_latency template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : millisec type : average base : non_wafl_msg_total REST : endpoint : api/cluster/counter/tables/wafl metric : average_non_wafl_msg_latency template : conf/restperf/9.12.0/wafl.yaml unit : millisec type : average base : non_wafl_msg_total - Harvest Metric : wafl_avg_repl_msg_latency Description : Average turnaround time for replication WAFL messages in milliseconds. ZAPI : endpoint : perf-object-get-instances wafl metric : avg_wafl_repl_msg_latency template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : millisec type : average base : wafl_repl_msg_total REST : endpoint : api/cluster/counter/tables/wafl metric : average_replication_msg_latency template : conf/restperf/9.12.0/wafl.yaml unit : millisec type : average base : replication_msg_total - Harvest Metric : wafl_cp_count Description : Array of counts of different types of Consistency Points (CP). ZAPI : endpoint : perf-object-get-instances wafl metric : cp_count template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : cp_count template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_cp_phase_times Description : Array of percentage time spent in different phases of Consistency Point (CP). ZAPI : endpoint : perf-object-get-instances wafl metric : cp_phase_times template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : percent type : percent base : total_cp_msecs REST : endpoint : api/cluster/counter/tables/wafl metric : cp_phase_times template : conf/restperf/9.12.0/wafl.yaml unit : percent type : percent base : total_cp_msecs - Harvest Metric : wafl_memory_free Description : The current WAFL memory available in the system. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_memory_free template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : mb type : raw REST : endpoint : api/cluster/counter/tables/wafl metric : memory_free template : conf/restperf/9.12.0/wafl.yaml unit : mb type : raw - Harvest Metric : wafl_memory_used Description : The current WAFL memory used in the system. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_memory_used template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : mb type : raw REST : endpoint : api/cluster/counter/tables/wafl metric : memory_used template : conf/restperf/9.12.0/wafl.yaml unit : mb type : raw - Harvest Metric : wafl_msg_total Description : Total number of WAFL messages per second. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_msg_total template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl metric : msg_total template : conf/restperf/9.12.0/wafl.yaml unit : per_sec type : rate - Harvest Metric : wafl_non_wafl_msg_total Description : Total number of non-WAFL messages per second. ZAPI : endpoint : perf-object-get-instances wafl metric : non_wafl_msg_total template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl metric : non_wafl_msg_total template : conf/restperf/9.12.0/wafl.yaml unit : per_sec type : rate - Harvest Metric : wafl_read_io_type Description : Percentage of reads served from buffer cache, external cache, or disk. ZAPI : endpoint : perf-object-get-instances wafl metric : read_io_type template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : percent type : percent base : read_io_type_base REST : endpoint : api/cluster/counter/tables/wafl metric : read_io_type template : conf/restperf/9.12.0/wafl.yaml unit : percent type : percent base : read_io_type_base - Harvest Metric : wafl_reads_from_cache Description : WAFL reads from cache. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_cache template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_cache template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_cloud Description : WAFL reads from cloud storage. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_cloud template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_cloud template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_cloud_s2c_bin Description : WAFL reads from cloud storage via s2c bin. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_cloud_s2c_bin template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_cloud_s2c_bin template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_disk Description : WAFL reads from disk. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_disk template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_disk template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_ext_cache Description : WAFL reads from external cache. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_ext_cache template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_external_cache template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_fc_miss Description : WAFL reads from remote volume for fc_miss. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_fc_miss template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_fc_miss template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_pmem Description : Wafl reads from persistent mmeory. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_pmem template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_reads_from_ssd Description : WAFL reads from SSD. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_reads_from_ssd template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : none type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : reads_from_ssd template : conf/restperf/9.12.0/wafl.yaml unit : none type : delta - Harvest Metric : wafl_repl_msg_total Description : Total number of replication WAFL messages per second. ZAPI : endpoint : perf-object-get-instances wafl metric : wafl_repl_msg_total template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : per_sec type : rate REST : endpoint : api/cluster/counter/tables/wafl metric : replication_msg_total template : conf/restperf/9.12.0/wafl.yaml unit : per_sec type : rate - Harvest Metric : wafl_total_cp_msecs Description : Milliseconds spent in Consistency Point (CP). ZAPI : endpoint : perf-object-get-instances wafl metric : total_cp_msecs template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : millisec type : delta REST : endpoint : api/cluster/counter/tables/wafl metric : total_cp_msecs template : conf/restperf/9.12.0/wafl.yaml unit : millisec type : delta - Harvest Metric : wafl_total_cp_util Description : Percentage of time spent in a Consistency Point (CP). ZAPI : endpoint : perf-object-get-instances wafl metric : total_cp_util template : conf/zapiperf/cdot/9.8.0/wafl.yaml unit : percent type : percent base : cpu_elapsed_time REST : endpoint : api/cluster/counter/tables/wafl metric : total_cp_util template : conf/restperf/9.12.0/wafl.yaml unit : percent type : percent base : cpu_elapsed_time","title":"Metrics"},{"location":"plugins/","text":"Built-in Plugins \u00b6 The plugin feature allows users to manipulate and customize data collected by collectors without changing the collectors. Plugins have the same capabilities as collectors and therefore can collect data on their own as well. Furthermore, multiple plugins can be put in a pipeline to perform more complex operations. Harvest architecture defines three types of plugins: built-in generic - Statically compiled, generic plugins. \"Generic\" means the plugin is collector-agnostic. These plugins are provided in this package. dynamic-generic - These are generic plugins as well, but they are compiled as shared objects and dynamically loaded. These plugins are living in the directory src/plugins. dynamic-custom - These plugins are collector-specific. Their source code should reside inside the plugins/ subdirectory of the collector package. Custom plugins have access to all the parameters of their parent collector and should be therefore treated with great care. This documentation gives an overview of builtin plugins. For other plugins, see their respective documentation. For writing your own plugin, see Developer's documentation. Note: the rules are executed in the same order as you've added them. Aggregator \u00b6 Aggregator creates a new collection of metrics (Matrix) by summarizing and/or averaging metric values from an existing Matrix for a given label. For example, if the collected metrics are for volumes, you can create an aggregation for nodes or svms. Rule syntax \u00b6 simplest case: plugins : Aggregator : - LABEL # will aggregate a new Matrix based on target label LABEL If you want to specify which labels should be included in the new instances, you can add those space-seperated after LABEL : - LABEL LABEL1,LABEL2 # same, but LABEL1 and LABEL2 will be copied into the new instances # (default is to only copy LABEL and any global labels (such as cluster and datacenter) Or include all labels: - LABEL ... # copy all labels of the original instance By default, aggregated metrics will be prefixed with LABEL . For example if the object of the original Matrix is volume (meaning metrics are prefixed with volume_ ) and LABEL is aggr , then the metric volume_read_ops will become aggr_volume_read_ops , etc. You can override this by providing the <>OBJ using the following syntax: - LABEL<>OBJ # use OBJ as the object of the new matrix, e.g. if the original object is \"volume\" and you # want to leave metric names unchanged, use \"volume\" Finally, sometimes you only want to aggregate instances with a specific label value. You can use <VALUE> for that ( optionally follow by OBJ ): - LABEL<VALUE> # aggregate all instances if LABEL has value VALUE - LABEL<`VALUE`> # same, but VALUE is regular expression - LABEL<LABELX=`VALUE`> # same, but check against \"LABELX\" (instead of \"LABEL\") Examples: plugins : Aggregator : # will aggregate metrics of the aggregate. The labels \"node\" and \"type\" are included in the new instances - aggr node type # aggregate instances if label \"type\" has value \"flexgroup\" # include all original labels - type<flexgroup> ... # aggregate all instances if value of \"volume\" ends with underscore and 4 digits - volume<`_\\d{4}$`> Aggregation rules \u00b6 The plugin tries to intelligently aggregate metrics based on a few rules: Sum - the default rule, if no other rules apply Average - if any of the following is true: metric name has suffix _percent or _percentage metric name has prefix average_ or avg_ metric has property ( metric.GetProperty() ) percent or average Weighted Average - applied if metric has property average and suffix _latency and if there is a matching _ops metric. (This is currently only matching to ZapiPerf metrics, which use the Property field of metrics.) Ignore - metrics created by some plugins, such as value_to_num by LabelAgent Max \u00b6 Max creates a new collection of metrics (Matrix) by calculating max of metric values from an existing Matrix for a given label. For example, if the collected metrics are for disks, you can create max at the node or aggregate level. Refer Max Examples for more details. Max Rule syntax \u00b6 simplest case: plugins : Max : - LABEL # create a new Matrix of max values on target label LABEL If you want to specify which labels should be included in the new instances, you can add those space-seperated after LABEL : - LABEL LABEL1,LABEL2 # similar to the above example, but LABEL1 and LABEL2 will be copied into the new instances # (default is to only copy LABEL and all global labels (such as cluster and datacenter) Or include all labels: - LABEL ... # copy all labels of the original instance By default, metrics will be prefixed with LABEL . For example if the object of the original Matrix is volume (meaning metrics are prefixed with volume_ ) and LABEL is aggr , then the metric volume_read_ops will become aggr_volume_read_ops . You can override this using the <>OBJ pattern shown below: - LABEL<>OBJ # use OBJ as the object of the new matrix, e.g. if the original object is \"volume\" and you # want to leave metric names unchanged, use \"volume\" Finally, sometimes you only want to generate instances with a specific label value. You can use <VALUE> for that ( optionally followed by OBJ ): - LABEL<VALUE> # aggregate all instances if LABEL has value VALUE - LABEL<`VALUE`> # same, but VALUE is regular expression - LABEL<LABELX=`VALUE`> # same, but check against \"LABELX\" (instead of \"LABEL\") Max Examples \u00b6 plugins : Max : # will create max of each aggregate metric. All metrics will be prefixed with aggr_disk_max. All labels are included in the new instances - aggr<>aggr_disk_max ... # calculate max instances if label \"disk\" has value \"1.1.0\". Prefix with disk_max # include all original labels - disk<1.1.0>disk_max ... # max of all instances if value of \"volume\" ends with underscore and 4 digits - volume<`_\\d{4}$`> LabelAgent \u00b6 LabelAgent are used to manipulate instance labels based on rules. You can define multiple rules, here is an example of what you could add to the yaml file of a collector: plugins : LabelAgent : # our rules: split : node `/` ,aggr,plex,disk replace_regex : node node `^(node)_(\\d+)_.*$` `Node-$2` Note: Labels for creating new label should use name defined in right side of =>. If not present then left side of => is used. split \u00b6 Rule syntax: split : - LABEL `SEP` LABEL1,LABEL2,LABEL3 # source label - separator - comma-seperated target labels Splits the value of a given label by separator SEP and creates new labels if their number matches to the number of target labels defined in rule. To discard a subvalue, just add a redundant , in the names of the target labels. Example: split : - node `/` ,aggr,plex,disk # will split the value of \"node\" using separator \"/\" # will expect 4 values: first will be discarded, remaining # three will be stored as labels \"aggr\", \"plex\" and \"disk\" split_regex \u00b6 Does the same as split but uses a regular expression instead of a separator. Rule syntax: split_regex : - LABEL `REGEX` LABEL1,LABEL2,LABEL3 Example: split_regex : - node `.*_(ag\\d+)_(p\\d+)_(d\\d+)` aggr,plex,disk # will look for \"_ag\", \"_p\", \"_d\", each followed by one # or more numbers, if there is a match, the submatches # will be stored as \"aggr\", \"plex\" and \"disk\" split_pairs \u00b6 Rule syntax: split_pairs : - LABEL `SEP1` `SEP2` # source label - pair separator - key-value separator Extracts key-value pairs from the value of source label LABEL . Note that you need to add these keys in the export options, otherwise they will not be exported. Example: split_pairs : - comment ` ` `:` # will split pairs using a single space and split key-values using colon # e.g. if comment=\"owner:jack contact:some@email\", the result wll be # two new labels: owner=\"jack\" and contact=\"some@email\" join \u00b6 Join multiple label values using separator SEP and create a new label. Rule syntax: join : - LABEL `SEP` LABEL1,LABEL2,LABEL3 # target label - separator - comma-seperated source labels Example: join : - plex_long `_` aggr,plex # will look for the values of labels \"aggr\" and \"plex\", # if they are set, a new \"plex_long\" label will be added # by joining their values with \"_\" replace \u00b6 Substitute substring OLD with NEW in label SOURCE and store in TARGET . Note that target and source labels can be the same. Rule syntax: replace : - SOURCE TARGET `OLD` `NEW` # source label - target label - substring to replace - replace with Example: replace : - node node_short `node_` `` # this rule will just remove \"node_\" from all values of label # \"node\". E.g. if label is \"node_jamaica1\", it will rewrite it # as \"jamaica1\" replace_regex \u00b6 Same as replace , but will use a regular expression instead of OLD . Note you can use $n to specify n th submatch in NEW . Rule syntax: replace_regex : - SOURCE TARGET `REGEX` `NEW` # source label - target label - substring to replace - replace with Example: replace_regex : - node node `^(node)_(\\d+)_.*$` `Node-$2` # if there is a match, will capitalize \"Node\" and remove suffixes. # E.g. if label is \"node_10_dc2\", it will rewrite it as # will rewrite it as \"Node-10\" exclude_equals \u00b6 Exclude each instance, if the value of LABEL is exactly VALUE . Exclude means that metrics for this instance will not be exported. Rule syntax: exclude_equals : - LABEL `VALUE` # label name - label value Example: exclude_equals : - vol_type `flexgroup_constituent` # all instances, which have label \"vol_type\" with value # \"flexgroup_constituent\" will not be exported exclude_contains \u00b6 Same as exclude_equals , but all labels that contain VALUE will be excluded Rule syntax: exclude_contains : - LABEL `VALUE` # label name - label value Example: exclude_contains : - vol_type `flexgroup_` # all instances, which have label \"vol_type\" which contain # \"flexgroup_\" will not be exported exclude_regex \u00b6 Same as exclude_equals , but will use a regular expression and all matching instances will be excluded. Rule syntax: exclude_regex : - LABEL `REGEX` # label name - regular expression Example: exclude_regex : - vol_type `^flex` # all instances, which have label \"vol_type\" which starts with # \"flex\" will not be exported include_equals \u00b6 Include each instance, if the value of LABEL is exactly VALUE . Include means that metrics for this instance will be exported and instances that do not match will not be exported. Rule syntax: include_equals : - LABEL `VALUE` # label name - label value Example: include_equals : - vol_type `flexgroup_constituent` # all instances, which have label \"vol_type\" with value # \"flexgroup_constituent\" will be exported include_contains \u00b6 Same as include_equals , but all labels that contain VALUE will be included Rule syntax: include_contains : - LABEL `VALUE` # label name - label value Example: include_contains : - vol_type `flexgroup_` # all instances, which have label \"vol_type\" which contain # \"flexgroup_\" will be exported include_regex \u00b6 Same as include_equals , but a regular expression will be used for inclusion. Similar to the other includes, all matching instances will be included and all non-matching will not be exported. Rule syntax: include_regex : - LABEL `REGEX` # label name - regular expression Example: include_regex : - vol_type `^flex` # all instances, which have label \"vol_type\" which starts with # \"flex\" will be exported value_mapping \u00b6 value_mapping was deprecated in 21.11 and removed in 22.02. Use value_to_num mapping instead. value_to_num \u00b6 Map values of a given label to a numeric metric (of type uint8 ). This rule maps values of a given label to a numeric metric (of type unit8 ). Healthy is mapped to 1 and all non-healthy values are mapped to 0. This is handy to manipulate the data in the DB or Grafana (e.g. change color based on status or create alert). Note that you don't define the numeric values yourself, instead, you only provide the possible (expected) values, the plugin will map each value to its index in the rule. Rule syntax: value_to_num : - METRIC LABEL ZAPI_VALUE REST_VALUE `N` # map values of LABEL to 1 if it is ZAPI_VALUE or REST_VALUE # otherwise, value of METRIC is set to N The default value N is optional, if no default value is given and the label value does not match any of the given values, the metric value will not be set. Examples: value_to_num : - status state up online `0` # a new metric will be created with the name \"status\" # if an instance has label \"state\" with value \"up\", the metric value will be 1, # if it's \"online\", the value will be set to 1, # if it's any other value, it will be set to the specified default, 0 value_to_num : - status state up online `4` # metric value will be set to 1 if \"state\" is \"up\", otherwise to **4** value_to_num : - status outage - - `0` #ok_value is empty value. # metric value will be set to 1 if \"outage\" is empty, if it's any other value, it will be set to the default, 0 # '-' is a special symbol in this mapping, and it will be converted to blank while processing. value_to_num_regex \u00b6 Same as value_to_num, but will use a regular expression. All matches are mapped to 1 and non-matches are mapped to 0. This is handy to manipulate the data in the DB or Grafana (e.g. change color based on status or create alert). Note that you don't define the numeric values, instead, you provide the expected values and the plugin will map each value to its index in the rule. Rule syntax: value_to_num_regex : - METRIC LABEL ZAPI_REGEX REST_REGEX `N` # map values of LABEL to 1 if it matches ZAPI_REGEX or REST_REGEX # otherwise, value of METRIC is set to N The default value N is optional, if no default value is given and the label value does not match any of the given values, the metric value will not be set. Examples: value_to_num_regex : - certificateuser methods .*cert.*$ .*certificate.*$ `0` # a new metric will be created with the name \"certificateuser\" # if an instance has label \"methods\" with value contains \"cert\", the metric value will be 1, # if value contains \"certificate\", the value will be set to 1, # if value doesn't contain \"cert\" and \"certificate\", it will be set to the specified default, 0 value_to_num_regex : - status state ^up$ ^ok$ `4` # metric value will be set to 1 if label \"state\" matches regex, otherwise set to **4** MetricAgent \u00b6 MetricAgent are used to manipulate metrics based on rules. You can define multiple rules, here is an example of what you could add to the yaml file of a collector: plugins : MetricAgent : compute_metric : - snapshot_maxfiles_possible ADD snapshot.max_files_available snapshot.max_files_used - raid_disk_count ADD block_storage.primary.disk_count block_storage.hybrid_cache.disk_count Note: Metric names used to create new metrics can come from the left or right side of the rename operator ( => ) Note: The metric agent currently does not work for histogram or array metrics. compute_metric \u00b6 This rule creates a new metric (of type float64) using the provided scalar or an existing metric value combined with a mathematical operation. You can provide a numeric value or a metric name with an operation. The plugin will use the provided number or fetch the value of a given metric, perform the requested mathematical operation, and store the result in new custom metric. Currently, we support these operations: ADD SUBTRACT MULTIPLY DIVIDE PERCENT Rule syntax: compute_metric : - METRIC OPERATION METRIC1 METRIC2 METRIC3 # target new metric - mathematical operation - input metric names # apply OPERATION on metric values of METRIC1, METRIC2 and METRIC3 and set result in METRIC # METRIC1, METRIC2, METRIC3 can be a scalar or an existing metric name. Examples: compute_metric : - space_total ADD space_available space_used # a new metric will be created with the name \"space_total\" # if an instance has metric \"space_available\" with value \"1000\", and \"space_used\" with value \"400\", # the result value will be \"1400\" and set to metric \"space_total\". compute_metric : - disk_count ADD primary.disk_count secondary.disk_count hybrid.disk_count # value of metric \"disk_count\" would be addition of all the given disk_counts metric values. # disk_count = primary.disk_count + secondary.disk_count + hybrid.disk_count compute_metric : - files_available SUBTRACT files files_used # value of metric \"files_available\" would be subtraction of the metric value of files_used from metric value of files. # files_available = files - files_used compute_metric : - total_bytes MULTIPLY bytes_per_sector sector_count # value of metric \"total_bytes\" would be multiplication of metric value of bytes_per_sector and metric value of sector_count. # total_bytes = bytes_per_sector * sector_count compute_metric : - uptime MULTIPLY stats.power_on_hours 60 60 # value of metric \"uptime\" would be multiplication of metric value of stats.power_on_hours and scalar value of 60 * 60. # total_bytes = bytes_per_sector * sector_count compute_metric : - transmission_rate DIVIDE transfer.bytes_transferred transfer.total_duration # value of metric \"transmission_rate\" would be division of metric value of transfer.bytes_transferred by metric value of transfer.total_duration. # transmission_rate = transfer.bytes_transferred / transfer.total_duration compute_metric : - inode_used_percent PERCENT inode_files_used inode_files_total # a new metric named \"inode_used_percent\" will be created by dividing the metric \"inode_files_used\" by # \"inode_files_total\" and multiplying the result by 100. # inode_used_percent = inode_files_used / inode_files_total * 100","title":"Plugins"},{"location":"plugins/#built-in-plugins","text":"The plugin feature allows users to manipulate and customize data collected by collectors without changing the collectors. Plugins have the same capabilities as collectors and therefore can collect data on their own as well. Furthermore, multiple plugins can be put in a pipeline to perform more complex operations. Harvest architecture defines three types of plugins: built-in generic - Statically compiled, generic plugins. \"Generic\" means the plugin is collector-agnostic. These plugins are provided in this package. dynamic-generic - These are generic plugins as well, but they are compiled as shared objects and dynamically loaded. These plugins are living in the directory src/plugins. dynamic-custom - These plugins are collector-specific. Their source code should reside inside the plugins/ subdirectory of the collector package. Custom plugins have access to all the parameters of their parent collector and should be therefore treated with great care. This documentation gives an overview of builtin plugins. For other plugins, see their respective documentation. For writing your own plugin, see Developer's documentation. Note: the rules are executed in the same order as you've added them.","title":"Built-in Plugins"},{"location":"plugins/#aggregator","text":"Aggregator creates a new collection of metrics (Matrix) by summarizing and/or averaging metric values from an existing Matrix for a given label. For example, if the collected metrics are for volumes, you can create an aggregation for nodes or svms.","title":"Aggregator"},{"location":"plugins/#rule-syntax","text":"simplest case: plugins : Aggregator : - LABEL # will aggregate a new Matrix based on target label LABEL If you want to specify which labels should be included in the new instances, you can add those space-seperated after LABEL : - LABEL LABEL1,LABEL2 # same, but LABEL1 and LABEL2 will be copied into the new instances # (default is to only copy LABEL and any global labels (such as cluster and datacenter) Or include all labels: - LABEL ... # copy all labels of the original instance By default, aggregated metrics will be prefixed with LABEL . For example if the object of the original Matrix is volume (meaning metrics are prefixed with volume_ ) and LABEL is aggr , then the metric volume_read_ops will become aggr_volume_read_ops , etc. You can override this by providing the <>OBJ using the following syntax: - LABEL<>OBJ # use OBJ as the object of the new matrix, e.g. if the original object is \"volume\" and you # want to leave metric names unchanged, use \"volume\" Finally, sometimes you only want to aggregate instances with a specific label value. You can use <VALUE> for that ( optionally follow by OBJ ): - LABEL<VALUE> # aggregate all instances if LABEL has value VALUE - LABEL<`VALUE`> # same, but VALUE is regular expression - LABEL<LABELX=`VALUE`> # same, but check against \"LABELX\" (instead of \"LABEL\") Examples: plugins : Aggregator : # will aggregate metrics of the aggregate. The labels \"node\" and \"type\" are included in the new instances - aggr node type # aggregate instances if label \"type\" has value \"flexgroup\" # include all original labels - type<flexgroup> ... # aggregate all instances if value of \"volume\" ends with underscore and 4 digits - volume<`_\\d{4}$`>","title":"Rule syntax"},{"location":"plugins/#aggregation-rules","text":"The plugin tries to intelligently aggregate metrics based on a few rules: Sum - the default rule, if no other rules apply Average - if any of the following is true: metric name has suffix _percent or _percentage metric name has prefix average_ or avg_ metric has property ( metric.GetProperty() ) percent or average Weighted Average - applied if metric has property average and suffix _latency and if there is a matching _ops metric. (This is currently only matching to ZapiPerf metrics, which use the Property field of metrics.) Ignore - metrics created by some plugins, such as value_to_num by LabelAgent","title":"Aggregation rules"},{"location":"plugins/#max","text":"Max creates a new collection of metrics (Matrix) by calculating max of metric values from an existing Matrix for a given label. For example, if the collected metrics are for disks, you can create max at the node or aggregate level. Refer Max Examples for more details.","title":"Max"},{"location":"plugins/#max-rule-syntax","text":"simplest case: plugins : Max : - LABEL # create a new Matrix of max values on target label LABEL If you want to specify which labels should be included in the new instances, you can add those space-seperated after LABEL : - LABEL LABEL1,LABEL2 # similar to the above example, but LABEL1 and LABEL2 will be copied into the new instances # (default is to only copy LABEL and all global labels (such as cluster and datacenter) Or include all labels: - LABEL ... # copy all labels of the original instance By default, metrics will be prefixed with LABEL . For example if the object of the original Matrix is volume (meaning metrics are prefixed with volume_ ) and LABEL is aggr , then the metric volume_read_ops will become aggr_volume_read_ops . You can override this using the <>OBJ pattern shown below: - LABEL<>OBJ # use OBJ as the object of the new matrix, e.g. if the original object is \"volume\" and you # want to leave metric names unchanged, use \"volume\" Finally, sometimes you only want to generate instances with a specific label value. You can use <VALUE> for that ( optionally followed by OBJ ): - LABEL<VALUE> # aggregate all instances if LABEL has value VALUE - LABEL<`VALUE`> # same, but VALUE is regular expression - LABEL<LABELX=`VALUE`> # same, but check against \"LABELX\" (instead of \"LABEL\")","title":"Max Rule syntax"},{"location":"plugins/#max-examples","text":"plugins : Max : # will create max of each aggregate metric. All metrics will be prefixed with aggr_disk_max. All labels are included in the new instances - aggr<>aggr_disk_max ... # calculate max instances if label \"disk\" has value \"1.1.0\". Prefix with disk_max # include all original labels - disk<1.1.0>disk_max ... # max of all instances if value of \"volume\" ends with underscore and 4 digits - volume<`_\\d{4}$`>","title":"Max Examples"},{"location":"plugins/#labelagent","text":"LabelAgent are used to manipulate instance labels based on rules. You can define multiple rules, here is an example of what you could add to the yaml file of a collector: plugins : LabelAgent : # our rules: split : node `/` ,aggr,plex,disk replace_regex : node node `^(node)_(\\d+)_.*$` `Node-$2` Note: Labels for creating new label should use name defined in right side of =>. If not present then left side of => is used.","title":"LabelAgent"},{"location":"plugins/#split","text":"Rule syntax: split : - LABEL `SEP` LABEL1,LABEL2,LABEL3 # source label - separator - comma-seperated target labels Splits the value of a given label by separator SEP and creates new labels if their number matches to the number of target labels defined in rule. To discard a subvalue, just add a redundant , in the names of the target labels. Example: split : - node `/` ,aggr,plex,disk # will split the value of \"node\" using separator \"/\" # will expect 4 values: first will be discarded, remaining # three will be stored as labels \"aggr\", \"plex\" and \"disk\"","title":"split"},{"location":"plugins/#split_regex","text":"Does the same as split but uses a regular expression instead of a separator. Rule syntax: split_regex : - LABEL `REGEX` LABEL1,LABEL2,LABEL3 Example: split_regex : - node `.*_(ag\\d+)_(p\\d+)_(d\\d+)` aggr,plex,disk # will look for \"_ag\", \"_p\", \"_d\", each followed by one # or more numbers, if there is a match, the submatches # will be stored as \"aggr\", \"plex\" and \"disk\"","title":"split_regex"},{"location":"plugins/#split_pairs","text":"Rule syntax: split_pairs : - LABEL `SEP1` `SEP2` # source label - pair separator - key-value separator Extracts key-value pairs from the value of source label LABEL . Note that you need to add these keys in the export options, otherwise they will not be exported. Example: split_pairs : - comment ` ` `:` # will split pairs using a single space and split key-values using colon # e.g. if comment=\"owner:jack contact:some@email\", the result wll be # two new labels: owner=\"jack\" and contact=\"some@email\"","title":"split_pairs"},{"location":"plugins/#join","text":"Join multiple label values using separator SEP and create a new label. Rule syntax: join : - LABEL `SEP` LABEL1,LABEL2,LABEL3 # target label - separator - comma-seperated source labels Example: join : - plex_long `_` aggr,plex # will look for the values of labels \"aggr\" and \"plex\", # if they are set, a new \"plex_long\" label will be added # by joining their values with \"_\"","title":"join"},{"location":"plugins/#replace","text":"Substitute substring OLD with NEW in label SOURCE and store in TARGET . Note that target and source labels can be the same. Rule syntax: replace : - SOURCE TARGET `OLD` `NEW` # source label - target label - substring to replace - replace with Example: replace : - node node_short `node_` `` # this rule will just remove \"node_\" from all values of label # \"node\". E.g. if label is \"node_jamaica1\", it will rewrite it # as \"jamaica1\"","title":"replace"},{"location":"plugins/#replace_regex","text":"Same as replace , but will use a regular expression instead of OLD . Note you can use $n to specify n th submatch in NEW . Rule syntax: replace_regex : - SOURCE TARGET `REGEX` `NEW` # source label - target label - substring to replace - replace with Example: replace_regex : - node node `^(node)_(\\d+)_.*$` `Node-$2` # if there is a match, will capitalize \"Node\" and remove suffixes. # E.g. if label is \"node_10_dc2\", it will rewrite it as # will rewrite it as \"Node-10\"","title":"replace_regex"},{"location":"plugins/#exclude_equals","text":"Exclude each instance, if the value of LABEL is exactly VALUE . Exclude means that metrics for this instance will not be exported. Rule syntax: exclude_equals : - LABEL `VALUE` # label name - label value Example: exclude_equals : - vol_type `flexgroup_constituent` # all instances, which have label \"vol_type\" with value # \"flexgroup_constituent\" will not be exported","title":"exclude_equals"},{"location":"plugins/#exclude_contains","text":"Same as exclude_equals , but all labels that contain VALUE will be excluded Rule syntax: exclude_contains : - LABEL `VALUE` # label name - label value Example: exclude_contains : - vol_type `flexgroup_` # all instances, which have label \"vol_type\" which contain # \"flexgroup_\" will not be exported","title":"exclude_contains"},{"location":"plugins/#exclude_regex","text":"Same as exclude_equals , but will use a regular expression and all matching instances will be excluded. Rule syntax: exclude_regex : - LABEL `REGEX` # label name - regular expression Example: exclude_regex : - vol_type `^flex` # all instances, which have label \"vol_type\" which starts with # \"flex\" will not be exported","title":"exclude_regex"},{"location":"plugins/#include_equals","text":"Include each instance, if the value of LABEL is exactly VALUE . Include means that metrics for this instance will be exported and instances that do not match will not be exported. Rule syntax: include_equals : - LABEL `VALUE` # label name - label value Example: include_equals : - vol_type `flexgroup_constituent` # all instances, which have label \"vol_type\" with value # \"flexgroup_constituent\" will be exported","title":"include_equals"},{"location":"plugins/#include_contains","text":"Same as include_equals , but all labels that contain VALUE will be included Rule syntax: include_contains : - LABEL `VALUE` # label name - label value Example: include_contains : - vol_type `flexgroup_` # all instances, which have label \"vol_type\" which contain # \"flexgroup_\" will be exported","title":"include_contains"},{"location":"plugins/#include_regex","text":"Same as include_equals , but a regular expression will be used for inclusion. Similar to the other includes, all matching instances will be included and all non-matching will not be exported. Rule syntax: include_regex : - LABEL `REGEX` # label name - regular expression Example: include_regex : - vol_type `^flex` # all instances, which have label \"vol_type\" which starts with # \"flex\" will be exported","title":"include_regex"},{"location":"plugins/#value_mapping","text":"value_mapping was deprecated in 21.11 and removed in 22.02. Use value_to_num mapping instead.","title":"value_mapping"},{"location":"plugins/#value_to_num","text":"Map values of a given label to a numeric metric (of type uint8 ). This rule maps values of a given label to a numeric metric (of type unit8 ). Healthy is mapped to 1 and all non-healthy values are mapped to 0. This is handy to manipulate the data in the DB or Grafana (e.g. change color based on status or create alert). Note that you don't define the numeric values yourself, instead, you only provide the possible (expected) values, the plugin will map each value to its index in the rule. Rule syntax: value_to_num : - METRIC LABEL ZAPI_VALUE REST_VALUE `N` # map values of LABEL to 1 if it is ZAPI_VALUE or REST_VALUE # otherwise, value of METRIC is set to N The default value N is optional, if no default value is given and the label value does not match any of the given values, the metric value will not be set. Examples: value_to_num : - status state up online `0` # a new metric will be created with the name \"status\" # if an instance has label \"state\" with value \"up\", the metric value will be 1, # if it's \"online\", the value will be set to 1, # if it's any other value, it will be set to the specified default, 0 value_to_num : - status state up online `4` # metric value will be set to 1 if \"state\" is \"up\", otherwise to **4** value_to_num : - status outage - - `0` #ok_value is empty value. # metric value will be set to 1 if \"outage\" is empty, if it's any other value, it will be set to the default, 0 # '-' is a special symbol in this mapping, and it will be converted to blank while processing.","title":"value_to_num"},{"location":"plugins/#value_to_num_regex","text":"Same as value_to_num, but will use a regular expression. All matches are mapped to 1 and non-matches are mapped to 0. This is handy to manipulate the data in the DB or Grafana (e.g. change color based on status or create alert). Note that you don't define the numeric values, instead, you provide the expected values and the plugin will map each value to its index in the rule. Rule syntax: value_to_num_regex : - METRIC LABEL ZAPI_REGEX REST_REGEX `N` # map values of LABEL to 1 if it matches ZAPI_REGEX or REST_REGEX # otherwise, value of METRIC is set to N The default value N is optional, if no default value is given and the label value does not match any of the given values, the metric value will not be set. Examples: value_to_num_regex : - certificateuser methods .*cert.*$ .*certificate.*$ `0` # a new metric will be created with the name \"certificateuser\" # if an instance has label \"methods\" with value contains \"cert\", the metric value will be 1, # if value contains \"certificate\", the value will be set to 1, # if value doesn't contain \"cert\" and \"certificate\", it will be set to the specified default, 0 value_to_num_regex : - status state ^up$ ^ok$ `4` # metric value will be set to 1 if label \"state\" matches regex, otherwise set to **4**","title":"value_to_num_regex"},{"location":"plugins/#metricagent","text":"MetricAgent are used to manipulate metrics based on rules. You can define multiple rules, here is an example of what you could add to the yaml file of a collector: plugins : MetricAgent : compute_metric : - snapshot_maxfiles_possible ADD snapshot.max_files_available snapshot.max_files_used - raid_disk_count ADD block_storage.primary.disk_count block_storage.hybrid_cache.disk_count Note: Metric names used to create new metrics can come from the left or right side of the rename operator ( => ) Note: The metric agent currently does not work for histogram or array metrics.","title":"MetricAgent"},{"location":"plugins/#compute_metric","text":"This rule creates a new metric (of type float64) using the provided scalar or an existing metric value combined with a mathematical operation. You can provide a numeric value or a metric name with an operation. The plugin will use the provided number or fetch the value of a given metric, perform the requested mathematical operation, and store the result in new custom metric. Currently, we support these operations: ADD SUBTRACT MULTIPLY DIVIDE PERCENT Rule syntax: compute_metric : - METRIC OPERATION METRIC1 METRIC2 METRIC3 # target new metric - mathematical operation - input metric names # apply OPERATION on metric values of METRIC1, METRIC2 and METRIC3 and set result in METRIC # METRIC1, METRIC2, METRIC3 can be a scalar or an existing metric name. Examples: compute_metric : - space_total ADD space_available space_used # a new metric will be created with the name \"space_total\" # if an instance has metric \"space_available\" with value \"1000\", and \"space_used\" with value \"400\", # the result value will be \"1400\" and set to metric \"space_total\". compute_metric : - disk_count ADD primary.disk_count secondary.disk_count hybrid.disk_count # value of metric \"disk_count\" would be addition of all the given disk_counts metric values. # disk_count = primary.disk_count + secondary.disk_count + hybrid.disk_count compute_metric : - files_available SUBTRACT files files_used # value of metric \"files_available\" would be subtraction of the metric value of files_used from metric value of files. # files_available = files - files_used compute_metric : - total_bytes MULTIPLY bytes_per_sector sector_count # value of metric \"total_bytes\" would be multiplication of metric value of bytes_per_sector and metric value of sector_count. # total_bytes = bytes_per_sector * sector_count compute_metric : - uptime MULTIPLY stats.power_on_hours 60 60 # value of metric \"uptime\" would be multiplication of metric value of stats.power_on_hours and scalar value of 60 * 60. # total_bytes = bytes_per_sector * sector_count compute_metric : - transmission_rate DIVIDE transfer.bytes_transferred transfer.total_duration # value of metric \"transmission_rate\" would be division of metric value of transfer.bytes_transferred by metric value of transfer.total_duration. # transmission_rate = transfer.bytes_transferred / transfer.total_duration compute_metric : - inode_used_percent PERCENT inode_files_used inode_files_total # a new metric named \"inode_used_percent\" will be created by dividing the metric \"inode_files_used\" by # \"inode_files_total\" and multiplying the result by 100. # inode_used_percent = inode_files_used / inode_files_total * 100","title":"compute_metric"},{"location":"prepare-7mode-clusters/","text":"NetApp Harvest requires login credentials to access monitored hosts. Although, a generic admin account can be used, it is best practice to create a dedicated monitoring account with the least privilege access. ONTAP 7-mode supports only username / password based authentication with NetApp Harvest. Harvest communicates with monitored systems exclusively via HTTPS, which is not enabled by default in Data ONTAP 7-mode. Login as a user with full administrative privileges and execute the following steps. Enabling HTTPS and TLS (ONTAP 7-mode only) \u00b6 Verify SSL is configured secureadmin status ssl If ssl is \u2018active\u2019 continue. If not, setup SSL and be sure to choose a Key length (bits) of 2048: secureadmin setup ssl SSL Setup has already been done before. Do you want to proceed? [no] yes Country Name (2 letter code) [US]: NL State or Province Name (full name) [California]: Noord-Holland Locality Name (city, town, etc.) [Santa Clara]: Schiphol Organization Name (company) [Your Company]: NetApp Organization Unit Name (division): SalesEngineering Common Name (fully qualified domain name) [sdt-7dot1a.nltestlab.hq.netapp.com]: Administrator email: noreply@netapp.com Days until expires [5475] :5475 Key length (bits) [512] :2048 Enable management via SSL and enable TLS options httpd.admin.ssl.enable on options tls.enable on Creating ONTAP user \u00b6 Create the role with required capabilities \u00b6 role add netapp-harvest-role -c \"Role for performance monitoring by NetApp Harvest\" -a login-http-admin,api-system-get-version,api-system-get-info,api-perf-object-*,api-emsautosupport-log Create a group for this role \u00b6 useradmin group add netapp-harvest-group -c \"Group for performance monitoring by NetApp Harvest\" -r netapp-harvest-role Create a user for the role and enter the password when prompted \u00b6 useradmin user add netapp-harvest -c \"User account for performance monitoring by NetApp Harvest\" -n \"NetApp Harvest\" -g netapp-harvest-group The user is now created and can be configured for use by NetApp Harvest.","title":"ONTAP 7mode"},{"location":"prepare-7mode-clusters/#enabling-https-and-tls-ontap-7-mode-only","text":"Verify SSL is configured secureadmin status ssl If ssl is \u2018active\u2019 continue. If not, setup SSL and be sure to choose a Key length (bits) of 2048: secureadmin setup ssl SSL Setup has already been done before. Do you want to proceed? [no] yes Country Name (2 letter code) [US]: NL State or Province Name (full name) [California]: Noord-Holland Locality Name (city, town, etc.) [Santa Clara]: Schiphol Organization Name (company) [Your Company]: NetApp Organization Unit Name (division): SalesEngineering Common Name (fully qualified domain name) [sdt-7dot1a.nltestlab.hq.netapp.com]: Administrator email: noreply@netapp.com Days until expires [5475] :5475 Key length (bits) [512] :2048 Enable management via SSL and enable TLS options httpd.admin.ssl.enable on options tls.enable on","title":"Enabling HTTPS and TLS (ONTAP 7-mode only)"},{"location":"prepare-7mode-clusters/#creating-ontap-user","text":"","title":"Creating ONTAP user"},{"location":"prepare-7mode-clusters/#create-the-role-with-required-capabilities","text":"role add netapp-harvest-role -c \"Role for performance monitoring by NetApp Harvest\" -a login-http-admin,api-system-get-version,api-system-get-info,api-perf-object-*,api-emsautosupport-log","title":"Create the role with required capabilities"},{"location":"prepare-7mode-clusters/#create-a-group-for-this-role","text":"useradmin group add netapp-harvest-group -c \"Group for performance monitoring by NetApp Harvest\" -r netapp-harvest-role","title":"Create a group for this role"},{"location":"prepare-7mode-clusters/#create-a-user-for-the-role-and-enter-the-password-when-prompted","text":"useradmin user add netapp-harvest -c \"User account for performance monitoring by NetApp Harvest\" -n \"NetApp Harvest\" -g netapp-harvest-group The user is now created and can be configured for use by NetApp Harvest.","title":"Create a user for the role and enter the password when prompted"},{"location":"prepare-cdot-clusters/","text":"Prepare ONTAP cDOT cluster \u00b6 NetApp Harvest requires login credentials to access monitored hosts. Although, a generic admin account can be used, it is best practice to create a dedicated monitoring account with the least privilege access. In the examples below, the user, group, roles, etc., use a naming convention of \u2018netapp-harvest\u2019. These can be modified as needed to match your environment standards. There are few steps required to prepare each monitored system for collection. Harvest supports two authentication styles ( auth_style ) to connect to ONTAP clusters. They are basic_auth or certificate_auth . Both work well, but if you're starting fresh, the recommendation is to create a read-only harvest user on your ONTAP server and use certificate-based TLS authentication. Here's a summary of what we're going to do Create an ONTAP role with the necessary capabilities that Harvest will use to auth and collect data Create a user account using the role created in step #1. Creating ONTAP user \u00b6 There are two ways to create a read-only user: Create a user with read-only access to all API objects Create a user with read-only access to only the APIs Harvest collects today The second option has a smaller attack surface, but each time you want to collect counters for a new object, you will need to update the user's privileges. Below we explain how to create an ONTAP user and role for Harvest using ONTAP System Manager (Classic interface & New interface) and CLI. System Manager: New interface \u00b6 Note: in this section we add a user with read-only access to all API objects. For limited access, use either the classic interface or the CLI Open System Manager. Click on CLUSTER in the left menu bar, Settings and Users and Roles . In the right column, under Roles , click on Add to add a new role. Choose a role name (e.g. harvest2-role ). In the REST API PATH field, type /api and select Read-Only for ACCESS . Click on Save . In the left column, under Users , click on Add to create a new user. Choose a username. Under Role , select the role that we just created. Under User Login Methods select ONTAPI , and select one of the two authentication methods. Type in a password if you chose Password . Click on Save If you chose Password , you can add the username and password to the Harvest configuration file and start Harvest. If you chose Certificate jump to Using Certificate Authentication to generate certificates files. System Manager: Classic interface \u00b6 Open System Manager. Click on the Settings icon in the top-right corner of the window. Click on Roles in the left menu bar and click Add . Choose a role name (e.g. harvest2-role ). If you want to give Harvest read-only access to all API objects, then under Role Attributes click on Add , under Command type DEFAULT , leave Query empty, select readonly under Access Level , click on OK and Add . If you want to limit the API objects, then under Role Attributes , add each of the following lines as an entry. All of those should be entered under the Command column, Query should be left blank, and Access Level should be selected readonly . cluster lun snapmirror statistics storage aggregate storage disk storage shelf system node version volume After you click on Add , this is what you should see: Now we need to create a user. Click on Users in the left menu bar and Add . Choose a username and password. Under User Login Methods , click on Add , select ontapi as Application and select the role that we just created as Role . Click on Add in the pop-up window to save. Now add the username and password to harvest.yml and start Harvest. ONTAP CLI \u00b6 We are going to: create a Harvest role with read-only access to the API objects create a Harvest user and assign it to that role You should decide if you want to limit the Harvest role to only the subset of API objects Harvest requires or give Harvest access to all API objects. In both cases, Harvest's access will be read-only. Either approach is fine, following the principle of least-privilege, we recommend the limited approach. Login to the CLI of your c-DOT ONTAP system using SSH. Least-privilege approach \u00b6 Verify there are no errors when you copy/paste these. Warnings are fine. security login role create -role harvest2-role -access readonly -cmddirname \"cluster\" security login role create -role harvest2-role -access readonly -cmddirname \"lun\" security login role create -role harvest2-role -access readonly -cmddirname \"qos workload show\" security login role create -role harvest2-role -access readonly -cmddirname \"snapmirror\" security login role create -role harvest2-role -access readonly -cmddirname \"statistics\" security login role create -role harvest2-role -access readonly -cmddirname \"storage aggregate\" security login role create -role harvest2-role -access readonly -cmddirname \"storage disk\" security login role create -role harvest2-role -access readonly -cmddirname \"storage shelf\" security login role create -role harvest2-role -access readonly -cmddirname \"system health status show\" security login role create -role harvest2-role -access readonly -cmddirname \"system health subsystem show\" security login role create -role harvest2-role -access readonly -cmddirname \"system node\" security login role create -role harvest2-role -access readonly -cmddirname \"version\" security login role create -role harvest2-role -access readonly -cmddirname \"volume\" # Permissions required for Harvest 22.05+ security dashboard security login role create -role harvest2-role -access readonly -cmddirname \"network interface\" security login role create -role harvest2-role -access readonly -cmddirname \"security\" security login role create -role harvest2-role -access readonly -cmddirname \"storage encryption disk\" security login role create -role harvest2-role -access readonly -cmddirname \"vserver\" All APIs read-only approach \u00b6 security login role create -role harvest2-role -access readonly -cmddirname \"DEFAULT\" Create harvest user and associate to role \u00b6 Use this for password authentication # ZAPI based access security login create -user-or-group-name harvest2 -application ontapi -role harvest2-role -authentication-method password # REST based access security login create -user-or-group-name harvest2 -application http -role harvest2-role -authentication-method password Or this for certificate authentication # ZAPI based access security login create -user-or-group-name harvest2 -application ontapi -role harvest2-role -authentication-method cert # REST based access security login create -user-or-group-name harvest2 -application http -role harvest2-role -authentication-method cert Verify that an entry is present by running the following commands # ZAPI based access vserver services web access show -role harvest2-role -name ontapi # REST based access vserver services web access show -role harvest2-role -name rest If the entry is missing, enable access by running the following # ZAPI based access vserver services web access create -vserver $ADMIN_VSERVER -name ontapi -role harvest2-role # REST based access vserver services web access create -vserver $ADMIN_VSERVER -name rest -role harvest2-role 7-Mode CLI \u00b6 Login to the CLI of your 7-Mode ONTAP system (e.g. using SSH). First, we create a user role. If you want to give the user readonly access to all API objects, type in the following command: useradmin role modify harvest2-role -a login-http-admin,api-system-get-version, \\ api-system-get-info,api-perf-object-*,api-ems-autosupport-log,api-diagnosis-status-get, \\ api-lun-list-info,api-diagnosis-subsystem-config-get-iter,api-disk-list-info, \\ api-diagnosis-config-get-iter,api-aggr-list-info,api-volume-list-info, \\ api-storage-shelf-environment-list-info,api-qtree-list,api-quota-report Using Certificate Authentication \u00b6 See comments here for troubleshooting client certificate authentication. Client certificate authentication allows you to authenticate with your ONTAP cluster without including username/passwords in your harvest.yml file. The process to setup client certificates is straightforward, although self-signed certificates introduce more work as does Go's strict treatment of common names. Unless you've installed production certificates on your ONTAP cluster, you'll need to replace your cluster's common-name-based self-signed certificates with a subject alternative name based certificate. After that step is completed, we'll create client certificates and add those for passwordless login. If you can't or don't want to replace your ONTAP cluster certificates, there are some workarounds. You can Use use_insecure_tls: true in your harvest.yml to disable certificate verification Change your harvest.yml to connect via hostname instead of IP address Create Self-Signed Subject Alternate Name Certificates for ONTAP \u00b6 Subject alternate name (SAN) certificates allow multiple hostnames in a single certificate. Starting with Go 1.3, when connecting to a cluster via its IP address, the CN field in the server certificate is ignored. This often causes errors like this: x509: cannot validate certificate for 127.0.0.1 because it doesn't contain any IP SANs Overview of steps to create a self-signed SAN certificate and make ONTAP use it \u00b6 Create a root key Create a root certificate authority certificate Create a SAN certificate for your ONTAP cluster, using #2 to create it Install root ca certificate created in step #2 on cluster Install SAN certificate created in step #3 on your cluster Modify you cluster/SVM to use the new certificate installed at step #5 Setup \u00b6 # create a place to store the certificate authority files, adjust as needed mkdir -p ca/{private,certs} Create a root key \u00b6 cd ca # generate a private key that we will use to create our self-signed certificate authority openssl genrsa -out private/ca.key.pem 4096 chmod 400 private/ca.key.pem Create a root certificate authority certificate \u00b6 Download the sample [samples/openssl.cnf] file and put it in the directory we created in setup . Edit line 9, changing dir to point to your ca directory created in setup . openssl req -config openssl.cnf -key private/ca.key.pem -new -x509 -days 7300 -sha256 -extensions v3_ca -out certs/ca.cert.pem # Verify openssl x509 -noout -text -in certs/ca.cert.pem # Make sure these are present Signature Algorithm: sha256WithRSAEncryption <======== Signature Algorithm can not be sha-1 X509v3 extensions: X509v3 Subject Key Identifier: --removed X509v3 Authority Key Identifier: --removed X509v3 Basic Constraints: critical CA:TRUE <======== CA must be true X509v3 Key Usage: critical Digital Signature, Certificate Sign, CRL Sign <======== Digital and certificate signature Create a SAN certificate for your ONTAP cluster \u00b6 First, we'll create the certificate signing request and then the certificate. In this example, the ONTAP cluster is named umeng-aff300-05-06 , update accordingly. Download the sample [samples/server_cert.cnf] file and put it in the directory we created in setup . Edit lines 18-21 to include your ONTAP cluster hostnames and IP addresses. Edit lines 6-11 with new names as needed. openssl req -new -newkey rsa:4096 -nodes -sha256 -subj \"/\" -config server_cert.cnf -outform pem -out umeng-aff300-05-06.csr -keyout umeng-aff300-05-06.key # Verify openssl req -text -noout -in umeng-aff300-05-06.csr # Make sure these are present Attributes: Requested Extensions: X509v3 Subject Alternative Name: <======== Section that lists alternate DNS and IP names DNS:umeng-aff300-05-06-cm.rtp.openenglab.netapp.com, DNS:umeng-aff300-05-06, IP Address:10.193.48.11, IP Address:10.193.48.11 Signature Algorithm: sha256WithRSAEncryption <======== Signature Algorithm can not be sha-1 We'll now use the certificate signing request and the recently create certificate authority to create a new SAN certificate for our cluster. openssl x509 -req -sha256 -days 30 -in umeng-aff300-05-06.csr -CA certs/ca.cert.pem -CAkey private/ca.key.pem -CAcreateserial -out umeng-aff300-05-06.crt -extensions req_ext -extfile server_cert.cnf # Verify openssl x509 -text -noout -in umeng-aff300-05-06.crt # Make sure these are present X509v3 extensions: X509v3 Subject Alternative Name: <======== Section that lists alternate DNS and IP names DNS:umeng-aff300-05-06-cm.rtp.openenglab.netapp.com, DNS:umeng-aff300-05-06, IP Address:10.193.48.11, IP Address:10.193.48.11 Signature Algorithm: sha256WithRSAEncryption <======== Signature Algorithm can not be sha-1 Install Root CA Certificate On Cluster \u00b6 Login to your cluster with admin credentials and install the server certificate authority. Copy from ca/certs/ca.cert.pem ssh admin@IP umeng-aff300-05-06::*> security certificate install -type server-ca Please enter Certificate: Press <Enter> when done -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- You should keep a copy of the CA-signed digital certificate for future reference. The installed certificate's CA and serial number for reference: CA: ntap Serial: 46AFFC7A3A9999999E8FB2FEB0 The certificate's generated name for reference: ntap Now install the server certificate we created above with SAN. Copy certificate from ca/umeng-aff300-05-06.crt and private key from ca/umeng-aff300-05-06.key umeng-aff300-05-06::*> security certificate install -type server Please enter Certificate: Press <Enter> when done -----BEGIN CERTIFICATE----- .. -----END CERTIFICATE----- Please enter Private Key: Press <Enter> when done -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- Please enter certificates of Certification Authorities (CA) which form the certificate chain of the server certificate. This starts with the issuing CA certificate of the server certificate and can range up to the root CA certificate. Do you want to continue entering root and/or intermediate certificates {y|n}: n If ONTAP tells you the provided certificate does not have a common name in the subject field, type the hostname of the cluster like this: The provided certificate does not have a common name in the subject field. Enter a valid common name to continue installation of the certificate: Enter a valid common name to continue installation of the certificate: umeng-aff300-05-06-cm.rtp.openenglab.netapp.com You should keep a copy of the private key and the CA-signed digital certificate for future reference. The installed certificate's CA and serial number for reference: CA: ntap Serial: 67A94AA25B229A68AC5BABACA8939A835AA998A58 The certificate's generated name for reference: umeng-aff300-05-06-cm.rtp.openenglab.netapp.com Modify the admin SVM to use the new certificate \u00b6 We'll modify the cluster's admin SVM to use the just installed server certificate and certificate authority. vserver show -type admin -fields vserver,type vserver type ------------------ ----- umeng-aff300-05-06 admin umeng-aff300-05-06::*> ssl modify -vserver umeng-aff300-05-06 -server-enabled true -serial 67A94AA25B229A68AC5BABACA8939A835AA998A58 -ca ntap (security ssl modify) You can verify the certificate(s) are installed and working by using openssl like so: openssl s_client -CAfile certs/ca.cert.pem -showcerts -servername server -connect umeng-aff300-05-06-cm.rtp.openenglab.netapp.com:443 CONNECTED(00000005) depth=1 C = US, ST = NC, L = RTP, O = ntap, OU = ntap verify return:1 depth=0 verify return:1 ... without the -CAfile , openssl will report CONNECTED(00000005) depth=0 verify error:num=20:unable to get local issuer certificate verify return:1 depth=0 verify error:num=21:unable to verify the first certificate verify return:1 --- Create Client Certificates for Password-less Login \u00b6 Copy the server certificate we created above into the Harvest install directory. cp ca/umeng-aff300-05-06.crt /opt/harvest cd /opt/harvest Create a self-signed client key and certificate with the same name as the hostname where Harvest is running. It's not required to name the key/cert pair after the hostname, but if you do, Harvest will load them automatically when you specify auth_style: certificate_auth otherwise you can point to them directly. See Pollers for details. Change the common name to the ONTAP user you setup with the harvest role above. e.g harvest2 cd /opt/harvest mkdir cert openssl req -x509 -nodes -days 1095 -newkey rsa:2048 -keyout cert/$(hostname).key -out cert/$(hostname).pem -subj \"/CN=harvest2\" Install Client Certificates on Cluster \u00b6 Login to your cluster with admin credentials and install the client certificate. Copy from cert/$(hostname).pem ssh admin@IP umeng-aff300-05-06::*> security certificate install -type client-ca -vserver umeng-aff300-05-06 Please enter Certificate: Press <Enter> when done -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- You should keep a copy of the CA-signed digital certificate for future reference. The installed certificate's CA and serial number for reference: CA: cbg Serial: B77B59444444CCCC The certificate's generated name for reference: cbg_B77B59444444CCCC Now that the client certificate is installed, let's enable it. umeng-aff300-05-06::*> ssl modify -vserver umeng-aff300-05-06 -client-enabled true (security ssl modify) Verify with a recent version of curl . If you are running on a Mac see below . curl --cacert umeng-aff300-05-06.crt --key cert/$(hostname).key --cert cert/$(hostname).pem https://umeng-aff300-05-06-cm.rtp.openenglab.netapp.com/api/storage/disks Update Harvest.yml to use client certificates \u00b6 Update the poller section with auth_style: certificate_auth like this: u2-cert: auth_style: certificate_auth addr: umeng-aff300-05-06-cm.rtp.openenglab.netapp.com Restart your poller and enjoy your password-less life-style. macOS \u00b6 The version of curl installed on macOS up through Monterey is not recent enough to work with self-signed SAN certs. You will need to install a newer version of curl via Homebrew, MacPorts, source, etc. Example of failure when running with older version of curl - you will see this in client auth test step above. curl --version curl 7.64.1 (x86_64-apple-darwin20.0) libcurl/7.64.1 (SecureTransport) LibreSSL/2.8.3 zlib/1.2.11 nghttp2/1.41.0 curl --cacert umeng-aff300-05-06.crt --key cert/cgrindst-mac-0.key --cert cert/cgrindst-mac-0.pem https://umeng-aff300-05-06-cm.rtp.openenglab.netapp.com/api/storage/disks curl: (60) SSL certificate problem: unable to get local issuer certificate Let's install curl via Homebrew. Make sure you don't miss the message that Homebrew prints about your path. If you need to have curl first in your PATH, run: echo 'export PATH=\"/usr/local/opt/curl/bin:$PATH\"' >> /Users/cgrindst/.bash_profile Now when we make a client auth request with our self-signed certificate it works! \\o/ brew install curl curl --version curl 7.80.0 (x86_64-apple-darwin20.6.0) libcurl/7.80.0 (SecureTransport) OpenSSL/1.1.1l zlib/1.2.11 brotli/1.0.9 zstd/1.5.0 libidn2/2.3.2 libssh2/1.10.0 nghttp2/1.46.0 librtmp/2.3 OpenLDAP/2.6.0 Release-Date: 2021-11-10 Protocols: dict file ftp ftps gopher gophers http https imap imaps ldap ldaps mqtt pop3 pop3s rtmp rtsp scp sftp smb smbs smtp smtps telnet tftp Features: alt-svc AsynchDNS brotli GSS-API HSTS HTTP2 HTTPS-proxy IDN IPv6 Kerberos Largefile libz MultiSSL NTLM NTLM_WB SPNEGO SSL TLS-SRP UnixSockets zstd curl --cacert umeng-aff300-05-06.crt --key cert/cgrindst-mac-0.key --cert cert/cgrindst-mac-0.pem https://umeng-aff300-05-06-cm.rtp.openenglab.netapp.com/api/storage/disks { \"records\": [ { \"name\": \"1.1.22\", \"_links\": { \"self\": { \"href\": \"/api/storage/disks/1.1.22\" } } } } Change directory to your Harvest home directory (replace /opt/harvest/ if this is not the default): $ cd /opt/harvest/ Generate an SSL cert and key pair with the following command. Note that it's preferred to generate these files using the hostname of the local machine. The command below assumes debian8 as our hostname name and harvest2 as the user we created in the previous step: openssl req -x509 -nodes -days 1095 -newkey rsa:2048 -keyout cert/debian8.key \\ -out cert/debian8.pem -subj \"/CN=harvest2\" Next, open the public key ( debian8.pem in our example) and copy all of its content. Login into your ONTAP CLI and run this command by replacing CLUSTER with the name of your cluster. security certificate install -type client-ca -vserver CLUSTER Paste the public key content and hit enter. Output should be similar to this: jamaica::> security certificate install -type client-ca -vserver jamaica Please enter Certificate: Press <Enter> when done -----BEGIN CERTIFICATE----- MIIDETCCAfmgAwIBAgIUP9EUXyl2BDSUOkNEcDU0yqbJ29IwDQYJKoZIhvcNAQEL BQAwGDEWMBQGA1UEAwwNaGFydmVzdDItY2xpMzAeFw0yMDEwMDkxMjA0MDhaFw0y MzEwMDktcGFueSBMdGQxFzAVBgNVBAMlc3QyLWNsaTMwggEiMA0tcGFueSBGCSqG SIb3DQEBAQUAA4IBDwAwggEKAoIBAQCVVy25BeCRoGCJWFOlyUL7Ddkze4Hl2/6u qye/3mk5vBNsGuXUrtad5XfBB70Ez9hWl5sraLiY68ro6MyX1icjiUTeaYDvS/76 Iw7HeXJ5Pyb/fWth1nePunytoLyG/vaTCySINkIV5nlxC+k0X3wWFJdfJzhloPtt 1Vdm7aCF2q6a2oZRnUEBGQb6t5KyF0/Xh65mvfgB0pl/AS2HY5Gz+~L54Xyvs+BY V7UmTop7WBYl0L3QXLieERpHXnyOXmtwlm1vG5g4n/0DVBNTBXjEdvc6oRh8sxBN ZlQWRApE7pa/I1bLD7G2AiS4UcPmR4cEpPRVEsOFOaAN3Z3YskvnAgMBAAGjUzBR MB0GA1UdDgQWBBQr4syV6TCcgO/5EcU/F8L2YYF15jAfBgNVHSMEGDAWgBQr4syV 6TCcgO/5EcU/F8L2YYF15jAPBgNVHRMdfdfwerH/MA0GCSqGSIb^ECd3DQEBCwUA A4IBAQBjP1BVhClRKkO/M3zlWa2L9Ztce6SuGwSnm6Ebmbs+iMc7o2N9p3RmV6Xl h6NcdXRzzPAVrUoK8ewhnBzdghgIPoCI6inAf1CUhcCX2xcnE/osO+CfvKuFnPYE WQ7UNLsdfka0a9kTK13r3GMs09z/VsDs0gD8UhPjoeO7LQhdU9tJ/qOaSP3s48pv sYzZurHUgKmVOaOE4t9DAdevSECEWCETRETA $Vbn %@@@%%rcdrctru65ryFaByb+ hTtGhDnoHwzt/cAGvLGV/RyWdGFAbu7Fb1rV94ceggE7nh1FqbdLH9siot6LlnQN MhEWp5PYgndOW49dDYUxoauCCkiA -----END CERTIFICATE----- You should keep a copy of the CA-signed digital certificate for future reference. The installed certificate 's CA and serial number for reference: CA: harvest2 Serial: 3FD1145F2976043012213d3009095534CCRDBD2 The certificate' s generated name for reference: harvest2 Finally, we need to enable SSL authentication with the following command (replace CLUSTER with the name of your cluster): security ssl modify -client-enabled true -vserver CLUSTER Reference \u00b6 https://github.com/jcbsmpsn/golang-https-example","title":"ONTAP cDOT"},{"location":"prepare-cdot-clusters/#prepare-ontap-cdot-cluster","text":"NetApp Harvest requires login credentials to access monitored hosts. Although, a generic admin account can be used, it is best practice to create a dedicated monitoring account with the least privilege access. In the examples below, the user, group, roles, etc., use a naming convention of \u2018netapp-harvest\u2019. These can be modified as needed to match your environment standards. There are few steps required to prepare each monitored system for collection. Harvest supports two authentication styles ( auth_style ) to connect to ONTAP clusters. They are basic_auth or certificate_auth . Both work well, but if you're starting fresh, the recommendation is to create a read-only harvest user on your ONTAP server and use certificate-based TLS authentication. Here's a summary of what we're going to do Create an ONTAP role with the necessary capabilities that Harvest will use to auth and collect data Create a user account using the role created in step #1.","title":"Prepare ONTAP cDOT cluster"},{"location":"prepare-cdot-clusters/#creating-ontap-user","text":"There are two ways to create a read-only user: Create a user with read-only access to all API objects Create a user with read-only access to only the APIs Harvest collects today The second option has a smaller attack surface, but each time you want to collect counters for a new object, you will need to update the user's privileges. Below we explain how to create an ONTAP user and role for Harvest using ONTAP System Manager (Classic interface & New interface) and CLI.","title":"Creating ONTAP user"},{"location":"prepare-cdot-clusters/#system-manager-new-interface","text":"Note: in this section we add a user with read-only access to all API objects. For limited access, use either the classic interface or the CLI Open System Manager. Click on CLUSTER in the left menu bar, Settings and Users and Roles . In the right column, under Roles , click on Add to add a new role. Choose a role name (e.g. harvest2-role ). In the REST API PATH field, type /api and select Read-Only for ACCESS . Click on Save . In the left column, under Users , click on Add to create a new user. Choose a username. Under Role , select the role that we just created. Under User Login Methods select ONTAPI , and select one of the two authentication methods. Type in a password if you chose Password . Click on Save If you chose Password , you can add the username and password to the Harvest configuration file and start Harvest. If you chose Certificate jump to Using Certificate Authentication to generate certificates files.","title":"System Manager: New interface"},{"location":"prepare-cdot-clusters/#system-manager-classic-interface","text":"Open System Manager. Click on the Settings icon in the top-right corner of the window. Click on Roles in the left menu bar and click Add . Choose a role name (e.g. harvest2-role ). If you want to give Harvest read-only access to all API objects, then under Role Attributes click on Add , under Command type DEFAULT , leave Query empty, select readonly under Access Level , click on OK and Add . If you want to limit the API objects, then under Role Attributes , add each of the following lines as an entry. All of those should be entered under the Command column, Query should be left blank, and Access Level should be selected readonly . cluster lun snapmirror statistics storage aggregate storage disk storage shelf system node version volume After you click on Add , this is what you should see: Now we need to create a user. Click on Users in the left menu bar and Add . Choose a username and password. Under User Login Methods , click on Add , select ontapi as Application and select the role that we just created as Role . Click on Add in the pop-up window to save. Now add the username and password to harvest.yml and start Harvest.","title":"System Manager: Classic interface"},{"location":"prepare-cdot-clusters/#ontap-cli","text":"We are going to: create a Harvest role with read-only access to the API objects create a Harvest user and assign it to that role You should decide if you want to limit the Harvest role to only the subset of API objects Harvest requires or give Harvest access to all API objects. In both cases, Harvest's access will be read-only. Either approach is fine, following the principle of least-privilege, we recommend the limited approach. Login to the CLI of your c-DOT ONTAP system using SSH.","title":"ONTAP CLI"},{"location":"prepare-cdot-clusters/#least-privilege-approach","text":"Verify there are no errors when you copy/paste these. Warnings are fine. security login role create -role harvest2-role -access readonly -cmddirname \"cluster\" security login role create -role harvest2-role -access readonly -cmddirname \"lun\" security login role create -role harvest2-role -access readonly -cmddirname \"qos workload show\" security login role create -role harvest2-role -access readonly -cmddirname \"snapmirror\" security login role create -role harvest2-role -access readonly -cmddirname \"statistics\" security login role create -role harvest2-role -access readonly -cmddirname \"storage aggregate\" security login role create -role harvest2-role -access readonly -cmddirname \"storage disk\" security login role create -role harvest2-role -access readonly -cmddirname \"storage shelf\" security login role create -role harvest2-role -access readonly -cmddirname \"system health status show\" security login role create -role harvest2-role -access readonly -cmddirname \"system health subsystem show\" security login role create -role harvest2-role -access readonly -cmddirname \"system node\" security login role create -role harvest2-role -access readonly -cmddirname \"version\" security login role create -role harvest2-role -access readonly -cmddirname \"volume\" # Permissions required for Harvest 22.05+ security dashboard security login role create -role harvest2-role -access readonly -cmddirname \"network interface\" security login role create -role harvest2-role -access readonly -cmddirname \"security\" security login role create -role harvest2-role -access readonly -cmddirname \"storage encryption disk\" security login role create -role harvest2-role -access readonly -cmddirname \"vserver\"","title":"Least-privilege approach"},{"location":"prepare-cdot-clusters/#all-apis-read-only-approach","text":"security login role create -role harvest2-role -access readonly -cmddirname \"DEFAULT\"","title":"All APIs read-only approach"},{"location":"prepare-cdot-clusters/#create-harvest-user-and-associate-to-role","text":"Use this for password authentication # ZAPI based access security login create -user-or-group-name harvest2 -application ontapi -role harvest2-role -authentication-method password # REST based access security login create -user-or-group-name harvest2 -application http -role harvest2-role -authentication-method password Or this for certificate authentication # ZAPI based access security login create -user-or-group-name harvest2 -application ontapi -role harvest2-role -authentication-method cert # REST based access security login create -user-or-group-name harvest2 -application http -role harvest2-role -authentication-method cert Verify that an entry is present by running the following commands # ZAPI based access vserver services web access show -role harvest2-role -name ontapi # REST based access vserver services web access show -role harvest2-role -name rest If the entry is missing, enable access by running the following # ZAPI based access vserver services web access create -vserver $ADMIN_VSERVER -name ontapi -role harvest2-role # REST based access vserver services web access create -vserver $ADMIN_VSERVER -name rest -role harvest2-role","title":"Create harvest user and associate to role"},{"location":"prepare-cdot-clusters/#7-mode-cli","text":"Login to the CLI of your 7-Mode ONTAP system (e.g. using SSH). First, we create a user role. If you want to give the user readonly access to all API objects, type in the following command: useradmin role modify harvest2-role -a login-http-admin,api-system-get-version, \\ api-system-get-info,api-perf-object-*,api-ems-autosupport-log,api-diagnosis-status-get, \\ api-lun-list-info,api-diagnosis-subsystem-config-get-iter,api-disk-list-info, \\ api-diagnosis-config-get-iter,api-aggr-list-info,api-volume-list-info, \\ api-storage-shelf-environment-list-info,api-qtree-list,api-quota-report","title":"7-Mode CLI"},{"location":"prepare-cdot-clusters/#using-certificate-authentication","text":"See comments here for troubleshooting client certificate authentication. Client certificate authentication allows you to authenticate with your ONTAP cluster without including username/passwords in your harvest.yml file. The process to setup client certificates is straightforward, although self-signed certificates introduce more work as does Go's strict treatment of common names. Unless you've installed production certificates on your ONTAP cluster, you'll need to replace your cluster's common-name-based self-signed certificates with a subject alternative name based certificate. After that step is completed, we'll create client certificates and add those for passwordless login. If you can't or don't want to replace your ONTAP cluster certificates, there are some workarounds. You can Use use_insecure_tls: true in your harvest.yml to disable certificate verification Change your harvest.yml to connect via hostname instead of IP address","title":"Using Certificate Authentication"},{"location":"prepare-cdot-clusters/#create-self-signed-subject-alternate-name-certificates-for-ontap","text":"Subject alternate name (SAN) certificates allow multiple hostnames in a single certificate. Starting with Go 1.3, when connecting to a cluster via its IP address, the CN field in the server certificate is ignored. This often causes errors like this: x509: cannot validate certificate for 127.0.0.1 because it doesn't contain any IP SANs","title":"Create Self-Signed Subject Alternate Name Certificates for ONTAP"},{"location":"prepare-cdot-clusters/#overview-of-steps-to-create-a-self-signed-san-certificate-and-make-ontap-use-it","text":"Create a root key Create a root certificate authority certificate Create a SAN certificate for your ONTAP cluster, using #2 to create it Install root ca certificate created in step #2 on cluster Install SAN certificate created in step #3 on your cluster Modify you cluster/SVM to use the new certificate installed at step #5","title":"Overview of steps to create a self-signed SAN certificate and make ONTAP use it"},{"location":"prepare-cdot-clusters/#setup","text":"# create a place to store the certificate authority files, adjust as needed mkdir -p ca/{private,certs}","title":"Setup"},{"location":"prepare-cdot-clusters/#create-a-root-key","text":"cd ca # generate a private key that we will use to create our self-signed certificate authority openssl genrsa -out private/ca.key.pem 4096 chmod 400 private/ca.key.pem","title":"Create a root key"},{"location":"prepare-cdot-clusters/#create-a-root-certificate-authority-certificate","text":"Download the sample [samples/openssl.cnf] file and put it in the directory we created in setup . Edit line 9, changing dir to point to your ca directory created in setup . openssl req -config openssl.cnf -key private/ca.key.pem -new -x509 -days 7300 -sha256 -extensions v3_ca -out certs/ca.cert.pem # Verify openssl x509 -noout -text -in certs/ca.cert.pem # Make sure these are present Signature Algorithm: sha256WithRSAEncryption <======== Signature Algorithm can not be sha-1 X509v3 extensions: X509v3 Subject Key Identifier: --removed X509v3 Authority Key Identifier: --removed X509v3 Basic Constraints: critical CA:TRUE <======== CA must be true X509v3 Key Usage: critical Digital Signature, Certificate Sign, CRL Sign <======== Digital and certificate signature","title":"Create a root certificate authority certificate"},{"location":"prepare-cdot-clusters/#create-a-san-certificate-for-your-ontap-cluster","text":"First, we'll create the certificate signing request and then the certificate. In this example, the ONTAP cluster is named umeng-aff300-05-06 , update accordingly. Download the sample [samples/server_cert.cnf] file and put it in the directory we created in setup . Edit lines 18-21 to include your ONTAP cluster hostnames and IP addresses. Edit lines 6-11 with new names as needed. openssl req -new -newkey rsa:4096 -nodes -sha256 -subj \"/\" -config server_cert.cnf -outform pem -out umeng-aff300-05-06.csr -keyout umeng-aff300-05-06.key # Verify openssl req -text -noout -in umeng-aff300-05-06.csr # Make sure these are present Attributes: Requested Extensions: X509v3 Subject Alternative Name: <======== Section that lists alternate DNS and IP names DNS:umeng-aff300-05-06-cm.rtp.openenglab.netapp.com, DNS:umeng-aff300-05-06, IP Address:10.193.48.11, IP Address:10.193.48.11 Signature Algorithm: sha256WithRSAEncryption <======== Signature Algorithm can not be sha-1 We'll now use the certificate signing request and the recently create certificate authority to create a new SAN certificate for our cluster. openssl x509 -req -sha256 -days 30 -in umeng-aff300-05-06.csr -CA certs/ca.cert.pem -CAkey private/ca.key.pem -CAcreateserial -out umeng-aff300-05-06.crt -extensions req_ext -extfile server_cert.cnf # Verify openssl x509 -text -noout -in umeng-aff300-05-06.crt # Make sure these are present X509v3 extensions: X509v3 Subject Alternative Name: <======== Section that lists alternate DNS and IP names DNS:umeng-aff300-05-06-cm.rtp.openenglab.netapp.com, DNS:umeng-aff300-05-06, IP Address:10.193.48.11, IP Address:10.193.48.11 Signature Algorithm: sha256WithRSAEncryption <======== Signature Algorithm can not be sha-1","title":"Create a SAN certificate for your ONTAP cluster"},{"location":"prepare-cdot-clusters/#install-root-ca-certificate-on-cluster","text":"Login to your cluster with admin credentials and install the server certificate authority. Copy from ca/certs/ca.cert.pem ssh admin@IP umeng-aff300-05-06::*> security certificate install -type server-ca Please enter Certificate: Press <Enter> when done -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- You should keep a copy of the CA-signed digital certificate for future reference. The installed certificate's CA and serial number for reference: CA: ntap Serial: 46AFFC7A3A9999999E8FB2FEB0 The certificate's generated name for reference: ntap Now install the server certificate we created above with SAN. Copy certificate from ca/umeng-aff300-05-06.crt and private key from ca/umeng-aff300-05-06.key umeng-aff300-05-06::*> security certificate install -type server Please enter Certificate: Press <Enter> when done -----BEGIN CERTIFICATE----- .. -----END CERTIFICATE----- Please enter Private Key: Press <Enter> when done -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- Please enter certificates of Certification Authorities (CA) which form the certificate chain of the server certificate. This starts with the issuing CA certificate of the server certificate and can range up to the root CA certificate. Do you want to continue entering root and/or intermediate certificates {y|n}: n If ONTAP tells you the provided certificate does not have a common name in the subject field, type the hostname of the cluster like this: The provided certificate does not have a common name in the subject field. Enter a valid common name to continue installation of the certificate: Enter a valid common name to continue installation of the certificate: umeng-aff300-05-06-cm.rtp.openenglab.netapp.com You should keep a copy of the private key and the CA-signed digital certificate for future reference. The installed certificate's CA and serial number for reference: CA: ntap Serial: 67A94AA25B229A68AC5BABACA8939A835AA998A58 The certificate's generated name for reference: umeng-aff300-05-06-cm.rtp.openenglab.netapp.com","title":"Install Root CA Certificate On Cluster"},{"location":"prepare-cdot-clusters/#modify-the-admin-svm-to-use-the-new-certificate","text":"We'll modify the cluster's admin SVM to use the just installed server certificate and certificate authority. vserver show -type admin -fields vserver,type vserver type ------------------ ----- umeng-aff300-05-06 admin umeng-aff300-05-06::*> ssl modify -vserver umeng-aff300-05-06 -server-enabled true -serial 67A94AA25B229A68AC5BABACA8939A835AA998A58 -ca ntap (security ssl modify) You can verify the certificate(s) are installed and working by using openssl like so: openssl s_client -CAfile certs/ca.cert.pem -showcerts -servername server -connect umeng-aff300-05-06-cm.rtp.openenglab.netapp.com:443 CONNECTED(00000005) depth=1 C = US, ST = NC, L = RTP, O = ntap, OU = ntap verify return:1 depth=0 verify return:1 ... without the -CAfile , openssl will report CONNECTED(00000005) depth=0 verify error:num=20:unable to get local issuer certificate verify return:1 depth=0 verify error:num=21:unable to verify the first certificate verify return:1 ---","title":"Modify the admin SVM to use the new certificate"},{"location":"prepare-cdot-clusters/#create-client-certificates-for-password-less-login","text":"Copy the server certificate we created above into the Harvest install directory. cp ca/umeng-aff300-05-06.crt /opt/harvest cd /opt/harvest Create a self-signed client key and certificate with the same name as the hostname where Harvest is running. It's not required to name the key/cert pair after the hostname, but if you do, Harvest will load them automatically when you specify auth_style: certificate_auth otherwise you can point to them directly. See Pollers for details. Change the common name to the ONTAP user you setup with the harvest role above. e.g harvest2 cd /opt/harvest mkdir cert openssl req -x509 -nodes -days 1095 -newkey rsa:2048 -keyout cert/$(hostname).key -out cert/$(hostname).pem -subj \"/CN=harvest2\"","title":"Create Client Certificates for Password-less Login"},{"location":"prepare-cdot-clusters/#install-client-certificates-on-cluster","text":"Login to your cluster with admin credentials and install the client certificate. Copy from cert/$(hostname).pem ssh admin@IP umeng-aff300-05-06::*> security certificate install -type client-ca -vserver umeng-aff300-05-06 Please enter Certificate: Press <Enter> when done -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- You should keep a copy of the CA-signed digital certificate for future reference. The installed certificate's CA and serial number for reference: CA: cbg Serial: B77B59444444CCCC The certificate's generated name for reference: cbg_B77B59444444CCCC Now that the client certificate is installed, let's enable it. umeng-aff300-05-06::*> ssl modify -vserver umeng-aff300-05-06 -client-enabled true (security ssl modify) Verify with a recent version of curl . If you are running on a Mac see below . curl --cacert umeng-aff300-05-06.crt --key cert/$(hostname).key --cert cert/$(hostname).pem https://umeng-aff300-05-06-cm.rtp.openenglab.netapp.com/api/storage/disks","title":"Install Client Certificates on Cluster"},{"location":"prepare-cdot-clusters/#update-harvestyml-to-use-client-certificates","text":"Update the poller section with auth_style: certificate_auth like this: u2-cert: auth_style: certificate_auth addr: umeng-aff300-05-06-cm.rtp.openenglab.netapp.com Restart your poller and enjoy your password-less life-style.","title":"Update Harvest.yml to use client certificates"},{"location":"prepare-cdot-clusters/#macos","text":"The version of curl installed on macOS up through Monterey is not recent enough to work with self-signed SAN certs. You will need to install a newer version of curl via Homebrew, MacPorts, source, etc. Example of failure when running with older version of curl - you will see this in client auth test step above. curl --version curl 7.64.1 (x86_64-apple-darwin20.0) libcurl/7.64.1 (SecureTransport) LibreSSL/2.8.3 zlib/1.2.11 nghttp2/1.41.0 curl --cacert umeng-aff300-05-06.crt --key cert/cgrindst-mac-0.key --cert cert/cgrindst-mac-0.pem https://umeng-aff300-05-06-cm.rtp.openenglab.netapp.com/api/storage/disks curl: (60) SSL certificate problem: unable to get local issuer certificate Let's install curl via Homebrew. Make sure you don't miss the message that Homebrew prints about your path. If you need to have curl first in your PATH, run: echo 'export PATH=\"/usr/local/opt/curl/bin:$PATH\"' >> /Users/cgrindst/.bash_profile Now when we make a client auth request with our self-signed certificate it works! \\o/ brew install curl curl --version curl 7.80.0 (x86_64-apple-darwin20.6.0) libcurl/7.80.0 (SecureTransport) OpenSSL/1.1.1l zlib/1.2.11 brotli/1.0.9 zstd/1.5.0 libidn2/2.3.2 libssh2/1.10.0 nghttp2/1.46.0 librtmp/2.3 OpenLDAP/2.6.0 Release-Date: 2021-11-10 Protocols: dict file ftp ftps gopher gophers http https imap imaps ldap ldaps mqtt pop3 pop3s rtmp rtsp scp sftp smb smbs smtp smtps telnet tftp Features: alt-svc AsynchDNS brotli GSS-API HSTS HTTP2 HTTPS-proxy IDN IPv6 Kerberos Largefile libz MultiSSL NTLM NTLM_WB SPNEGO SSL TLS-SRP UnixSockets zstd curl --cacert umeng-aff300-05-06.crt --key cert/cgrindst-mac-0.key --cert cert/cgrindst-mac-0.pem https://umeng-aff300-05-06-cm.rtp.openenglab.netapp.com/api/storage/disks { \"records\": [ { \"name\": \"1.1.22\", \"_links\": { \"self\": { \"href\": \"/api/storage/disks/1.1.22\" } } } } Change directory to your Harvest home directory (replace /opt/harvest/ if this is not the default): $ cd /opt/harvest/ Generate an SSL cert and key pair with the following command. Note that it's preferred to generate these files using the hostname of the local machine. The command below assumes debian8 as our hostname name and harvest2 as the user we created in the previous step: openssl req -x509 -nodes -days 1095 -newkey rsa:2048 -keyout cert/debian8.key \\ -out cert/debian8.pem -subj \"/CN=harvest2\" Next, open the public key ( debian8.pem in our example) and copy all of its content. Login into your ONTAP CLI and run this command by replacing CLUSTER with the name of your cluster. security certificate install -type client-ca -vserver CLUSTER Paste the public key content and hit enter. Output should be similar to this: jamaica::> security certificate install -type client-ca -vserver jamaica Please enter Certificate: Press <Enter> when done -----BEGIN CERTIFICATE----- MIIDETCCAfmgAwIBAgIUP9EUXyl2BDSUOkNEcDU0yqbJ29IwDQYJKoZIhvcNAQEL BQAwGDEWMBQGA1UEAwwNaGFydmVzdDItY2xpMzAeFw0yMDEwMDkxMjA0MDhaFw0y MzEwMDktcGFueSBMdGQxFzAVBgNVBAMlc3QyLWNsaTMwggEiMA0tcGFueSBGCSqG SIb3DQEBAQUAA4IBDwAwggEKAoIBAQCVVy25BeCRoGCJWFOlyUL7Ddkze4Hl2/6u qye/3mk5vBNsGuXUrtad5XfBB70Ez9hWl5sraLiY68ro6MyX1icjiUTeaYDvS/76 Iw7HeXJ5Pyb/fWth1nePunytoLyG/vaTCySINkIV5nlxC+k0X3wWFJdfJzhloPtt 1Vdm7aCF2q6a2oZRnUEBGQb6t5KyF0/Xh65mvfgB0pl/AS2HY5Gz+~L54Xyvs+BY V7UmTop7WBYl0L3QXLieERpHXnyOXmtwlm1vG5g4n/0DVBNTBXjEdvc6oRh8sxBN ZlQWRApE7pa/I1bLD7G2AiS4UcPmR4cEpPRVEsOFOaAN3Z3YskvnAgMBAAGjUzBR MB0GA1UdDgQWBBQr4syV6TCcgO/5EcU/F8L2YYF15jAfBgNVHSMEGDAWgBQr4syV 6TCcgO/5EcU/F8L2YYF15jAPBgNVHRMdfdfwerH/MA0GCSqGSIb^ECd3DQEBCwUA A4IBAQBjP1BVhClRKkO/M3zlWa2L9Ztce6SuGwSnm6Ebmbs+iMc7o2N9p3RmV6Xl h6NcdXRzzPAVrUoK8ewhnBzdghgIPoCI6inAf1CUhcCX2xcnE/osO+CfvKuFnPYE WQ7UNLsdfka0a9kTK13r3GMs09z/VsDs0gD8UhPjoeO7LQhdU9tJ/qOaSP3s48pv sYzZurHUgKmVOaOE4t9DAdevSECEWCETRETA $Vbn %@@@%%rcdrctru65ryFaByb+ hTtGhDnoHwzt/cAGvLGV/RyWdGFAbu7Fb1rV94ceggE7nh1FqbdLH9siot6LlnQN MhEWp5PYgndOW49dDYUxoauCCkiA -----END CERTIFICATE----- You should keep a copy of the CA-signed digital certificate for future reference. The installed certificate 's CA and serial number for reference: CA: harvest2 Serial: 3FD1145F2976043012213d3009095534CCRDBD2 The certificate' s generated name for reference: harvest2 Finally, we need to enable SSL authentication with the following command (replace CLUSTER with the name of your cluster): security ssl modify -client-enabled true -vserver CLUSTER","title":"macOS"},{"location":"prepare-cdot-clusters/#reference","text":"https://github.com/jcbsmpsn/golang-https-example","title":"Reference"},{"location":"prepare-fsx-clusters/","text":"Prepare Amazon FSx for ONTAP \u00b6 To set up Harvest and FSx make sure you read through Monitoring FSx for ONTAP file systems using Harvest and Grafana Supported Harvest Dashboards \u00b6 Amazon FSx for ONTAP exposes a different set of metrics than ONTAP cDOT. That means a limited set of out-of-the-box dashboards are supported and some panels may be missing information. The dashboards that work with FSx are tagged with fsx and listed below: ONTAP: Volume ONTAP: SVM ONTAP: Security ONTAP: Data Protection Snapshots ONTAP: Compliance ONTAP: Headroom (Only works for Rest Collector)","title":"Amazon FSx for ONTAP"},{"location":"prepare-fsx-clusters/#prepare-amazon-fsx-for-ontap","text":"To set up Harvest and FSx make sure you read through Monitoring FSx for ONTAP file systems using Harvest and Grafana","title":"Prepare Amazon FSx for ONTAP"},{"location":"prepare-fsx-clusters/#supported-harvest-dashboards","text":"Amazon FSx for ONTAP exposes a different set of metrics than ONTAP cDOT. That means a limited set of out-of-the-box dashboards are supported and some panels may be missing information. The dashboards that work with FSx are tagged with fsx and listed below: ONTAP: Volume ONTAP: SVM ONTAP: Security ONTAP: Data Protection Snapshots ONTAP: Compliance ONTAP: Headroom (Only works for Rest Collector)","title":"Supported Harvest Dashboards"},{"location":"prepare-storagegrid-clusters/","text":"Prepare StorageGRID cluster \u00b6 NetApp Harvest requires login credentials to access StorageGRID hosts. Although, a generic admin account can be used, it is better to create a dedicated monitoring user with the fewest permissions. Here's a summary of what we're going to do Create a StorageGRID group with the necessary capabilities that Harvest will use to auth and collect data Create a user assigned to the group created in step #1. Create StorageGRID group permissions \u00b6 These steps are documented here . You will need a root or admin account to create a new group permission. Select CONFIGURATION > Access control > Admin groups Select Create group Select Local group Enter a display name for the group, which you can update later as required. For example, Harvest or monitoring . Enter a unique name for the group, which you cannot update later. Select Continue On the Manage group permissions screen, select the permissions you want. At a minimum, Harvest requires the Tenant accounts permission. Select Save changes Create a StorageGRID user \u00b6 These steps are documented here . You will need a root or admin account to create a new user. Select CONFIGURATION > Access control > Admin users Select Create user Enter the user\u2019s full name, a unique username, and a password. Select Continue . Assign the user to the previously created harvest group. Select Create user and select Finish. Reference \u00b6 See group permissions for more information on StorageGRID permissions.","title":"StorageGRID"},{"location":"prepare-storagegrid-clusters/#prepare-storagegrid-cluster","text":"NetApp Harvest requires login credentials to access StorageGRID hosts. Although, a generic admin account can be used, it is better to create a dedicated monitoring user with the fewest permissions. Here's a summary of what we're going to do Create a StorageGRID group with the necessary capabilities that Harvest will use to auth and collect data Create a user assigned to the group created in step #1.","title":"Prepare StorageGRID cluster"},{"location":"prepare-storagegrid-clusters/#create-storagegrid-group-permissions","text":"These steps are documented here . You will need a root or admin account to create a new group permission. Select CONFIGURATION > Access control > Admin groups Select Create group Select Local group Enter a display name for the group, which you can update later as required. For example, Harvest or monitoring . Enter a unique name for the group, which you cannot update later. Select Continue On the Manage group permissions screen, select the permissions you want. At a minimum, Harvest requires the Tenant accounts permission. Select Save changes","title":"Create StorageGRID group permissions"},{"location":"prepare-storagegrid-clusters/#create-a-storagegrid-user","text":"These steps are documented here . You will need a root or admin account to create a new user. Select CONFIGURATION > Access control > Admin users Select Create user Enter the user\u2019s full name, a unique username, and a password. Select Continue . Assign the user to the previously created harvest group. Select Create user and select Finish.","title":"Create a StorageGRID user"},{"location":"prepare-storagegrid-clusters/#reference","text":"See group permissions for more information on StorageGRID permissions.","title":"Reference"},{"location":"prometheus-exporter/","text":"Prometheus Exporter \u00b6 Prometheus Install The information below describes how to setup Harvest's Prometheus exporter. If you need help installing or setting up Prometheus, check out their documention . Overview \u00b6 The Prometheus exporter is responsible for: formatting metrics into the Prometheus line protocol creating a web-endpoint on http://<ADDR>:<PORT>/metrics for Prometheus to scrape A web end-point is required because Prometheus scrapes Harvest by polling that end-point. In addition to the /metrics end-point, the Prometheus exporter also serves an overview of all metrics and collectors available on its root address http://<ADDR>:<PORT>/ . Because Prometheus polls Harvest, don't forget to update your Prometheus configuration and tell Prometheus how to scrape each poller. There are two ways to configure the Prometheus exporter: using a port range or individual port s. The port range is more flexible and should be used when you want multiple pollers all exporting to the same instance of Prometheus. Both options are explained below. Parameters \u00b6 All parameters of the exporter are defined in the Exporters section of harvest.yml . An overview of all parameters: parameter type description default port_range int-int (range), overrides port if specified lower port to upper port (inclusive) of the HTTP end-point to create when a poller specifies this exporter. Starting at lower port, each free port will be tried sequentially up to the upper port. port int, required if port_range is not specified port of the HTTP end-point local_http_addr string, optional address of the HTTP server Harvest starts for Prometheus to scrape: use localhost to serve only on the local machine use 0.0.0.0 (default) if Prometheus is scrapping from another machine 0.0.0.0 global_prefix string, optional add a prefix to all metrics (e.g. netapp_ ) allow_addrs list of strings, optional allow access only if host matches any of the provided addresses allow_addrs_regex list of strings, optional allow access only if host address matches at least one of the regular expressions cache_max_keep string (Go duration format), optional maximum amount of time metrics are cached (in case Prometheus does not timely collect the metrics) 300s add_meta_tags bool, optional add HELP and TYPE metatags to metrics (currently no useful information, but required by some tools) false sort_labels bool, optional sort metric labels before exporting. Some open-metrics scrapers report stale metrics when labels are not sorted. false A few examples: port_range \u00b6 Exporters : prom-prod : exporter : Prometheus port_range : 2000-2030 Pollers : cluster-01 : exporters : - prom-prod cluster-02 : exporters : - prom-prod cluster-03 : exporters : - prom-prod # ... more cluster-16 : exporters : - prom-prod Sixteen pollers will collect metrics from 16 clusters and make those metrics available to a single instance of Prometheus named prom-prod . Sixteen web end-points will be created on the first 16 available free ports between 2000 and 2030 (inclusive). After staring the pollers in the example above, running bin/harvest status shows the following. Note that ports 2000 and 2003 were not available so the next free port in the range was selected. If no free port can be found an error will be logged. Datacenter Poller PID PromPort Status ++++++++++++ ++++++++++++ +++++++ +++++++++ ++++++++++++++++++++ DC-01 cluster-01 2339 2001 running DC-01 cluster-02 2343 2002 running DC-01 cluster-03 2351 2004 running ... DC-01 cluster-14 2405 2015 running DC-01 cluster-15 2502 2016 running DC-01 cluster-16 2514 2017 running allow_addrs \u00b6 Exporters : my_prom : allow_addrs : - 192.168.0.102 - 192.168.0.103 will only allow access from exactly these two addresses. allow_addrs_regex \u00b6 Exporters : my_prom : allow_addrs_regex : - ` ^192.168.0.\\d+$` will only allow access from the IP4 range 192.168.0.0 - 192.168.0.255 . Configure Prometheus to scrape Harvest pollers \u00b6 There are two ways to tell Prometheus how to scrape Harvest: using HTTP service discovery (SD) or listing each poller individually. HTTP service discovery is the more flexible of the two. It is also less error-prone, and easier to manage. Combined with the port_range configuration described above, SD is the least effort to configure Prometheus and the easiest way to keep both Harvest and Prometheus in sync. NOTE HTTP service discovery does not work with Docker yet. With Docker, you will need to list each poller individually or if possible, use the Docker Compose workflow that uses file service discovery to achieve a similar ease-of-use as HTTP service discovery. See the example below for how to use HTTP SD and port_range together. Prometheus HTTP Service Discovery \u00b6 HTTP service discovery was introduced in Prometheus version 2.28.0. Make sure you're using that version or later. The way service discovery works is: shortly after a poller starts up, it registers with the SD node (if one exists) the poller sends a heartbeat to the SD node, by default every 45s. if a poller fails to send a heartbeat, the SD node removes the poller from the list of active targets after a minute the SD end-point is reachable via SCHEMA:// /api/v1/sd To use HTTP service discovery you need to: tell Harvest to start the HTTP service discovery process tell Prometheus to use the HTTP service discovery endpoint Enable HTTP service discovery in Harvest \u00b6 Add the following to your harvest.yml Admin : httpsd : listen : :8887 This tells Harvest to create an HTTP service discovery end-point on interface 0.0.0.0:8887 . If you want to only listen on localhost, use 127.0.0.1:<port> instead. See net.Dial for details on the supported listen formats. Start the SD process by running bin/harvest admin start . Once it is started, you can curl the end-point for the list of running Harvest pollers. curl -s 'http://localhost:8887/api/v1/sd' | jq . [ { \"targets\": [ \"10.0.1.55:12990\", \"10.0.1.55:15037\", \"127.0.0.1:15511\", \"127.0.0.1:15008\", \"127.0.0.1:15191\", \"10.0.1.55:15343\" ] } ] Harvest HTTP Service Discovery options \u00b6 HTTP service discovery (SD) is configured in the Admin > httpsd section of your harvest.yml . parameter type description default listen required Interface and port to listen on, use localhost:PORT or :PORT for all interfaces auth_basic optional If present, enables basic authentication on /api/v1/sd end-point auth_basic username , password required child of auth_basic tls optional If present, enables TLS transport. If running in a container, see note tls cert_file , key_file required child of tls Relative or absolute path to TLS certificate and key file. TLS 1.3 certificates required. FIPS complaint P-256 TLS 1.3 certificates can be created with bin/harvest admin tls create server ssl_cert , ssl_key optional if auth_style is certificate_auth Absolute paths to SSL (client) certificate and key used to authenticate with the target system. If not provided, the poller will look for <hostname>.key and <hostname>.pem in $HARVEST_HOME/cert/ . To create certificates for ONTAP systems, see using certificate authentication heart_beat optional, Go Duration format How frequently each poller sends a heartbeat message to the SD node 45s expire_after optional, Go Duration format If a poller fails to send a heartbeat, the SD node removes the poller after this duration 1m Enable HTTP service discovery in Prometheus \u00b6 Edit your prometheus.yml and add the following section $ vim /etc/prometheus/prometheus.yml scrape_configs : - job_name : harvest http_sd_configs : - url : http://localhost:8887/api/v1/sd Harvest and Prometheus both support basic authentication for HTTP SD end-points. To enable basic auth, add the following to your Harvest config. Admin : httpsd : listen : :8887 # Basic auth protects GETs and publishes auth_basic : username : admin password : admin Don't forget to also update your Prometheus config with the matching basic_auth credentials. Prometheus HTTP Service Discovery and Port Range \u00b6 HTTP SD combined with Harvest's port_range feature leads to significantly less configuration in your harvest.yml . For example, if your clusters all export to the same Prometheus instance, you can refactor the per-poller exporter into a single exporter shared by all clusters in Defaults as shown below: Notice that none of the pollers specify an exporter. Instead, all the pollers share the single exporter named prometheus-r listed in Defaults . prometheus-r is the only exporter defined and as specified will manage up to 1,000 Harvest Prometheus exporters. If you add or remove more clusters in the Pollers section, you do not have to change Prometheus since it dynamically pulls the targets from the Harvest admin node. Admin : httpsd : listen : :8887 Exporters : prometheus-r : exporter : Prometheus port_range : 13000-13999 Defaults : collectors : - Zapi - ZapiPerf use_insecure_tls : false auth_style : password username : admin password : pass exporters : - prometheus-r Pollers : umeng_aff300 : datacenter : meg addr : 10.193.48.11 F2240-127-26 : datacenter : meg addr : 10.193.6.61 # ... add more clusters Static Scrape Targets \u00b6 If we define four prometheus exporters at ports: 12990, 12991, 14567, and 14568 you need to add four sections to your prometheus.yml . $ vim /etc/prometheus/prometheus.yml Scroll down to near the end of file and add the following lines: - job_name : 'harvest' scrape_interval : 60s static_configs : - targets : - 'localhost:12990' - 'localhost:12991' - 'localhost:14567' - 'localhost:14568' NOTE If Prometheus is not on the same machine as Harvest, then replace localhost with the IP address of your Harvest machine. Also note the scrape interval above is set to 60s. That matches the polling frequency of the default Harvest collectors. If you change the polling frequency of a Harvest collector to a lower value, you should also change the scrape interval. Prometheus Alerts \u00b6 Prometheus includes out-of-the-box support for simple alerting. Alert rules are configured in your prometheus.yml file. Setup and details can be found in the Prometheus guide on alerting . Harvest also includes ems alerts and sample alerts for reference. Refer EMS Collector for more details about EMS events. Alertmanager \u00b6 Prometheus's builtin alerts are good for simple workflows. They do a nice job telling you what's happening at the moment. If you need a richer solution that includes summarization, notification, advanced delivery, deduplication, etc. checkout Alertmanager . Reference \u00b6 Prometheus Alerting Alertmanager Alertmanager's notification metrics Prometheus Linter Collection of example Prometheus Alerts","title":"Prometheus"},{"location":"prometheus-exporter/#prometheus-exporter","text":"Prometheus Install The information below describes how to setup Harvest's Prometheus exporter. If you need help installing or setting up Prometheus, check out their documention .","title":"Prometheus Exporter"},{"location":"prometheus-exporter/#overview","text":"The Prometheus exporter is responsible for: formatting metrics into the Prometheus line protocol creating a web-endpoint on http://<ADDR>:<PORT>/metrics for Prometheus to scrape A web end-point is required because Prometheus scrapes Harvest by polling that end-point. In addition to the /metrics end-point, the Prometheus exporter also serves an overview of all metrics and collectors available on its root address http://<ADDR>:<PORT>/ . Because Prometheus polls Harvest, don't forget to update your Prometheus configuration and tell Prometheus how to scrape each poller. There are two ways to configure the Prometheus exporter: using a port range or individual port s. The port range is more flexible and should be used when you want multiple pollers all exporting to the same instance of Prometheus. Both options are explained below.","title":"Overview"},{"location":"prometheus-exporter/#parameters","text":"All parameters of the exporter are defined in the Exporters section of harvest.yml . An overview of all parameters: parameter type description default port_range int-int (range), overrides port if specified lower port to upper port (inclusive) of the HTTP end-point to create when a poller specifies this exporter. Starting at lower port, each free port will be tried sequentially up to the upper port. port int, required if port_range is not specified port of the HTTP end-point local_http_addr string, optional address of the HTTP server Harvest starts for Prometheus to scrape: use localhost to serve only on the local machine use 0.0.0.0 (default) if Prometheus is scrapping from another machine 0.0.0.0 global_prefix string, optional add a prefix to all metrics (e.g. netapp_ ) allow_addrs list of strings, optional allow access only if host matches any of the provided addresses allow_addrs_regex list of strings, optional allow access only if host address matches at least one of the regular expressions cache_max_keep string (Go duration format), optional maximum amount of time metrics are cached (in case Prometheus does not timely collect the metrics) 300s add_meta_tags bool, optional add HELP and TYPE metatags to metrics (currently no useful information, but required by some tools) false sort_labels bool, optional sort metric labels before exporting. Some open-metrics scrapers report stale metrics when labels are not sorted. false A few examples:","title":"Parameters"},{"location":"prometheus-exporter/#port_range","text":"Exporters : prom-prod : exporter : Prometheus port_range : 2000-2030 Pollers : cluster-01 : exporters : - prom-prod cluster-02 : exporters : - prom-prod cluster-03 : exporters : - prom-prod # ... more cluster-16 : exporters : - prom-prod Sixteen pollers will collect metrics from 16 clusters and make those metrics available to a single instance of Prometheus named prom-prod . Sixteen web end-points will be created on the first 16 available free ports between 2000 and 2030 (inclusive). After staring the pollers in the example above, running bin/harvest status shows the following. Note that ports 2000 and 2003 were not available so the next free port in the range was selected. If no free port can be found an error will be logged. Datacenter Poller PID PromPort Status ++++++++++++ ++++++++++++ +++++++ +++++++++ ++++++++++++++++++++ DC-01 cluster-01 2339 2001 running DC-01 cluster-02 2343 2002 running DC-01 cluster-03 2351 2004 running ... DC-01 cluster-14 2405 2015 running DC-01 cluster-15 2502 2016 running DC-01 cluster-16 2514 2017 running","title":"port_range"},{"location":"prometheus-exporter/#allow_addrs","text":"Exporters : my_prom : allow_addrs : - 192.168.0.102 - 192.168.0.103 will only allow access from exactly these two addresses.","title":"allow_addrs"},{"location":"prometheus-exporter/#allow_addrs_regex","text":"Exporters : my_prom : allow_addrs_regex : - ` ^192.168.0.\\d+$` will only allow access from the IP4 range 192.168.0.0 - 192.168.0.255 .","title":"allow_addrs_regex"},{"location":"prometheus-exporter/#configure-prometheus-to-scrape-harvest-pollers","text":"There are two ways to tell Prometheus how to scrape Harvest: using HTTP service discovery (SD) or listing each poller individually. HTTP service discovery is the more flexible of the two. It is also less error-prone, and easier to manage. Combined with the port_range configuration described above, SD is the least effort to configure Prometheus and the easiest way to keep both Harvest and Prometheus in sync. NOTE HTTP service discovery does not work with Docker yet. With Docker, you will need to list each poller individually or if possible, use the Docker Compose workflow that uses file service discovery to achieve a similar ease-of-use as HTTP service discovery. See the example below for how to use HTTP SD and port_range together.","title":"Configure Prometheus to scrape Harvest pollers"},{"location":"prometheus-exporter/#prometheus-http-service-discovery","text":"HTTP service discovery was introduced in Prometheus version 2.28.0. Make sure you're using that version or later. The way service discovery works is: shortly after a poller starts up, it registers with the SD node (if one exists) the poller sends a heartbeat to the SD node, by default every 45s. if a poller fails to send a heartbeat, the SD node removes the poller from the list of active targets after a minute the SD end-point is reachable via SCHEMA:// /api/v1/sd To use HTTP service discovery you need to: tell Harvest to start the HTTP service discovery process tell Prometheus to use the HTTP service discovery endpoint","title":"Prometheus HTTP Service Discovery"},{"location":"prometheus-exporter/#enable-http-service-discovery-in-harvest","text":"Add the following to your harvest.yml Admin : httpsd : listen : :8887 This tells Harvest to create an HTTP service discovery end-point on interface 0.0.0.0:8887 . If you want to only listen on localhost, use 127.0.0.1:<port> instead. See net.Dial for details on the supported listen formats. Start the SD process by running bin/harvest admin start . Once it is started, you can curl the end-point for the list of running Harvest pollers. curl -s 'http://localhost:8887/api/v1/sd' | jq . [ { \"targets\": [ \"10.0.1.55:12990\", \"10.0.1.55:15037\", \"127.0.0.1:15511\", \"127.0.0.1:15008\", \"127.0.0.1:15191\", \"10.0.1.55:15343\" ] } ]","title":"Enable HTTP service discovery in Harvest"},{"location":"prometheus-exporter/#harvest-http-service-discovery-options","text":"HTTP service discovery (SD) is configured in the Admin > httpsd section of your harvest.yml . parameter type description default listen required Interface and port to listen on, use localhost:PORT or :PORT for all interfaces auth_basic optional If present, enables basic authentication on /api/v1/sd end-point auth_basic username , password required child of auth_basic tls optional If present, enables TLS transport. If running in a container, see note tls cert_file , key_file required child of tls Relative or absolute path to TLS certificate and key file. TLS 1.3 certificates required. FIPS complaint P-256 TLS 1.3 certificates can be created with bin/harvest admin tls create server ssl_cert , ssl_key optional if auth_style is certificate_auth Absolute paths to SSL (client) certificate and key used to authenticate with the target system. If not provided, the poller will look for <hostname>.key and <hostname>.pem in $HARVEST_HOME/cert/ . To create certificates for ONTAP systems, see using certificate authentication heart_beat optional, Go Duration format How frequently each poller sends a heartbeat message to the SD node 45s expire_after optional, Go Duration format If a poller fails to send a heartbeat, the SD node removes the poller after this duration 1m","title":"Harvest HTTP Service Discovery options"},{"location":"prometheus-exporter/#enable-http-service-discovery-in-prometheus","text":"Edit your prometheus.yml and add the following section $ vim /etc/prometheus/prometheus.yml scrape_configs : - job_name : harvest http_sd_configs : - url : http://localhost:8887/api/v1/sd Harvest and Prometheus both support basic authentication for HTTP SD end-points. To enable basic auth, add the following to your Harvest config. Admin : httpsd : listen : :8887 # Basic auth protects GETs and publishes auth_basic : username : admin password : admin Don't forget to also update your Prometheus config with the matching basic_auth credentials.","title":"Enable HTTP service discovery in Prometheus"},{"location":"prometheus-exporter/#prometheus-http-service-discovery-and-port-range","text":"HTTP SD combined with Harvest's port_range feature leads to significantly less configuration in your harvest.yml . For example, if your clusters all export to the same Prometheus instance, you can refactor the per-poller exporter into a single exporter shared by all clusters in Defaults as shown below: Notice that none of the pollers specify an exporter. Instead, all the pollers share the single exporter named prometheus-r listed in Defaults . prometheus-r is the only exporter defined and as specified will manage up to 1,000 Harvest Prometheus exporters. If you add or remove more clusters in the Pollers section, you do not have to change Prometheus since it dynamically pulls the targets from the Harvest admin node. Admin : httpsd : listen : :8887 Exporters : prometheus-r : exporter : Prometheus port_range : 13000-13999 Defaults : collectors : - Zapi - ZapiPerf use_insecure_tls : false auth_style : password username : admin password : pass exporters : - prometheus-r Pollers : umeng_aff300 : datacenter : meg addr : 10.193.48.11 F2240-127-26 : datacenter : meg addr : 10.193.6.61 # ... add more clusters","title":"Prometheus HTTP Service Discovery and Port Range"},{"location":"prometheus-exporter/#static-scrape-targets","text":"If we define four prometheus exporters at ports: 12990, 12991, 14567, and 14568 you need to add four sections to your prometheus.yml . $ vim /etc/prometheus/prometheus.yml Scroll down to near the end of file and add the following lines: - job_name : 'harvest' scrape_interval : 60s static_configs : - targets : - 'localhost:12990' - 'localhost:12991' - 'localhost:14567' - 'localhost:14568' NOTE If Prometheus is not on the same machine as Harvest, then replace localhost with the IP address of your Harvest machine. Also note the scrape interval above is set to 60s. That matches the polling frequency of the default Harvest collectors. If you change the polling frequency of a Harvest collector to a lower value, you should also change the scrape interval.","title":"Static Scrape Targets"},{"location":"prometheus-exporter/#prometheus-alerts","text":"Prometheus includes out-of-the-box support for simple alerting. Alert rules are configured in your prometheus.yml file. Setup and details can be found in the Prometheus guide on alerting . Harvest also includes ems alerts and sample alerts for reference. Refer EMS Collector for more details about EMS events.","title":"Prometheus Alerts"},{"location":"prometheus-exporter/#alertmanager","text":"Prometheus's builtin alerts are good for simple workflows. They do a nice job telling you what's happening at the moment. If you need a richer solution that includes summarization, notification, advanced delivery, deduplication, etc. checkout Alertmanager .","title":"Alertmanager"},{"location":"prometheus-exporter/#reference","text":"Prometheus Alerting Alertmanager Alertmanager's notification metrics Prometheus Linter Collection of example Prometheus Alerts","title":"Reference"},{"location":"quickstart/","text":"1. Configuration file \u00b6 Harvest's configuration information is defined in harvest.yml . There are a few ways to tell Harvest how to load this file: If you don't use the --config flag, the harvest.yml file located in the current working directory will be used If you specify the --config flag like so harvest status --config /opt/harvest/harvest.yml , Harvest will use that file To start collecting metrics, you need to define at least one poller and one exporter in your configuration file. The default configuration comes with a pre-configured poller named unix which collects metrics from the local system. This is useful if you want to monitor resource usage by Harvest and serves as a good example. Feel free to delete it if you want. The next step is to add pollers for your ONTAP clusters in the Pollers section of the Harvest configuration file, harvest.yml . 2. Start Harvest \u00b6 Start all Harvest pollers as daemons: bin/harvest start Or start a specific poller(s). In this case, we're staring two pollers named jamaica and jamaica . bin/harvest start jamaica jamaica Replace jamaica and grenada with the poller names you defined in harvest.yml . The logs of each poller can be found in /var/log/harvest/ . 3. Import Grafana dashboards \u00b6 The Grafana dashboards are located in the $HARVEST_HOME/grafana directory. You can manually import the dashboards or use the bin/harvest grafana command ( more documentation ). Note: the current dashboards specify Prometheus as the datasource. If you use the InfluxDB exporter, you will need to create your own dashboards. 4. Verify the metrics \u00b6 If you use a Prometheus Exporter, open a browser and navigate to http://0.0.0.0:12990/ (replace 12990 with the port number of your poller). This is the Harvest created HTTP end-point for your Prometheus exporter. This page provides a real-time generated list of running collectors and names of exported metrics. The metric data that is exported for Prometheus to scrap is available at http://0.0.0.0:12990/metrics/ . More information on configuring the exporter can be found in the Prometheus exporter documentation. If you can't access the URL, check the logs of your pollers. These are located in /var/log/harvest/ . 5. (Optional) Setup Systemd service files \u00b6 If you're running Harvest on a system with Systemd, you may want to take advantage of systemd instantiated units to manage your pollers.","title":"Quickstart"},{"location":"quickstart/#1-configuration-file","text":"Harvest's configuration information is defined in harvest.yml . There are a few ways to tell Harvest how to load this file: If you don't use the --config flag, the harvest.yml file located in the current working directory will be used If you specify the --config flag like so harvest status --config /opt/harvest/harvest.yml , Harvest will use that file To start collecting metrics, you need to define at least one poller and one exporter in your configuration file. The default configuration comes with a pre-configured poller named unix which collects metrics from the local system. This is useful if you want to monitor resource usage by Harvest and serves as a good example. Feel free to delete it if you want. The next step is to add pollers for your ONTAP clusters in the Pollers section of the Harvest configuration file, harvest.yml .","title":"1. Configuration file"},{"location":"quickstart/#2-start-harvest","text":"Start all Harvest pollers as daemons: bin/harvest start Or start a specific poller(s). In this case, we're staring two pollers named jamaica and jamaica . bin/harvest start jamaica jamaica Replace jamaica and grenada with the poller names you defined in harvest.yml . The logs of each poller can be found in /var/log/harvest/ .","title":"2. Start Harvest"},{"location":"quickstart/#3-import-grafana-dashboards","text":"The Grafana dashboards are located in the $HARVEST_HOME/grafana directory. You can manually import the dashboards or use the bin/harvest grafana command ( more documentation ). Note: the current dashboards specify Prometheus as the datasource. If you use the InfluxDB exporter, you will need to create your own dashboards.","title":"3. Import Grafana dashboards"},{"location":"quickstart/#4-verify-the-metrics","text":"If you use a Prometheus Exporter, open a browser and navigate to http://0.0.0.0:12990/ (replace 12990 with the port number of your poller). This is the Harvest created HTTP end-point for your Prometheus exporter. This page provides a real-time generated list of running collectors and names of exported metrics. The metric data that is exported for Prometheus to scrap is available at http://0.0.0.0:12990/metrics/ . More information on configuring the exporter can be found in the Prometheus exporter documentation. If you can't access the URL, check the logs of your pollers. These are located in /var/log/harvest/ .","title":"4. Verify the metrics"},{"location":"quickstart/#5-optional-setup-systemd-service-files","text":"If you're running Harvest on a system with Systemd, you may want to take advantage of systemd instantiated units to manage your pollers.","title":"5. (Optional) Setup Systemd service files"},{"location":"release-notes/","text":"Changelog Releases","title":"Release Notes"},{"location":"system-requirements/","text":"Harvest is written in Go, which means it runs on recent Linux systems. It also runs on Macs for development. Hardware requirements depend on how many clusters you monitor and the number of metrics you chose to collect. With the default configuration, when monitoring 10 clusters, we recommend: CPU: 2 cores Memory: 1 GB Disk: 500 MB (mostly used by log files) Harvest is compatible with: Prometheus: 2.26 or higher InfluxDB: v2 Grafana: 8.1.X or higher Docker: 20.10.0 or higher and compatible Docker Compose","title":"System Requirements"},{"location":"troubleshooting/","text":"Troubleshooting Harvest FAQ NABox Troubleshooting","title":"Troubleshoot"},{"location":"upgrade/","text":"To upgrade Harvest Stop harvest cd <existing harvest directory> bin/harvest stop Verify that all pollers have stopped: bin/harvest status or pgrep --full '\\-\\-poller' # should return nothing if all pollers are stopped Follow the installation instructions to download and install Harvest and then copy your old harvest.yml into the new install directory like so: cp /path/to/old/harvest/harvest.yml /path/to/new/harvest.yml After upgrade, re-import all dashboards (either bin/harvest grafana import cli or via the Grafana UI) to get any new enhancements in dashboards.","title":"Upgrade"},{"location":"architecture/rest-collector/","text":"REST collector \u00b6 Status \u00b6 ~~Accepted~~ Superseded by REST strategy The exact version of ONTAP that has full ZAPI parity is subject to change. Everywhere you see version 9.12, may become 9.13 or later. Context \u00b6 We need to document and communicate to customers: - when they should switch from the ZAPI collectors to the REST ones - what versions of ONTAP are supported by Harvest's REST collectors - how to fill ONTAP gaps between the ZAPI and REST APIs The ONTAP version information is important because gaps are addressed in later versions of cDOT. Considered Options \u00b6 Only REST A clean cut-over, stop using ZAPI, and switch completely to REST. Both Support both ZAPI and REST collectors running at the same time, collecting the same objects. Flexible, but has the downside of last-write wins. Not recommended unless you selectively pick non-overlapping sets of objects. Template change that supports both Change the template to break ties, priority, etc. Rejected because additional complexity not worth the benefits. private-cli When there are REST gaps that have not been filled yet or will never be filled (WONTFIX), the Harvest REST collector will provide infrastructure and documentation on how to use private-cli pass-through to address gaps. Chosen Decision \u00b6 For clusters with ONTAP versions < 9.12, we recommend customers use the ZAPI collectors. (#2) (#4) Once ONTAP 9.12+ is released and customers have upgraded to it, they should make a clean cut-over to the REST collectors (#1). ONTAP 9.12 is the version of ONTAP that has the best parity with what Harvest collects in terms of config and performance counters. Harvest REST collectors, templates, and dashboards are validated against ONTAP 9.12+. Most of the REST config templates will work before 9.12, but unless you have specific needs, we recommend sticking with the ZAPI collectors until you upgrade to 9.12. There is little value in running both the ZAPI and REST collectors for an overlapping set of objects. It's unlikely you want to collect the same object via REST and ZAPI at the same time. Harvest doesn't support this use-case, but does nothing to detect or prevent it. If you want to collect a non-overlapping set of objects with REST and ZAPI, you can. If you do, we recommend you disable the ZAPI object collector. For example, if you enable the REST disk template, you should disable the ZAPI disk template. We do NOT recommend collecting an overlapping set of objects with both collectors since the last one to run will overwrite previously collected data. Harvest will document how to use the REST private cli pass-through to collect custom and non-public counters. The Harvest team recommends that customers open ONTAP issues for REST public API gaps that need filled. Consequences \u00b6 The Harvest REST collectors will work with limitations on earlier versions of ONTAP. ONTAP 9.12+ is the minimally validated version. We only validate the full set of templates, dashboards, counters, etc. on versions of ONTAP 9.12+ Harvest does not prevent you from collecting the same resource with ZAPI and REST.","title":"REST collector"},{"location":"architecture/rest-collector/#rest-collector","text":"","title":"REST collector"},{"location":"architecture/rest-collector/#status","text":"~~Accepted~~ Superseded by REST strategy The exact version of ONTAP that has full ZAPI parity is subject to change. Everywhere you see version 9.12, may become 9.13 or later.","title":"Status "},{"location":"architecture/rest-collector/#context","text":"We need to document and communicate to customers: - when they should switch from the ZAPI collectors to the REST ones - what versions of ONTAP are supported by Harvest's REST collectors - how to fill ONTAP gaps between the ZAPI and REST APIs The ONTAP version information is important because gaps are addressed in later versions of cDOT.","title":"Context"},{"location":"architecture/rest-collector/#considered-options","text":"Only REST A clean cut-over, stop using ZAPI, and switch completely to REST. Both Support both ZAPI and REST collectors running at the same time, collecting the same objects. Flexible, but has the downside of last-write wins. Not recommended unless you selectively pick non-overlapping sets of objects. Template change that supports both Change the template to break ties, priority, etc. Rejected because additional complexity not worth the benefits. private-cli When there are REST gaps that have not been filled yet or will never be filled (WONTFIX), the Harvest REST collector will provide infrastructure and documentation on how to use private-cli pass-through to address gaps.","title":"Considered Options"},{"location":"architecture/rest-collector/#chosen-decision","text":"For clusters with ONTAP versions < 9.12, we recommend customers use the ZAPI collectors. (#2) (#4) Once ONTAP 9.12+ is released and customers have upgraded to it, they should make a clean cut-over to the REST collectors (#1). ONTAP 9.12 is the version of ONTAP that has the best parity with what Harvest collects in terms of config and performance counters. Harvest REST collectors, templates, and dashboards are validated against ONTAP 9.12+. Most of the REST config templates will work before 9.12, but unless you have specific needs, we recommend sticking with the ZAPI collectors until you upgrade to 9.12. There is little value in running both the ZAPI and REST collectors for an overlapping set of objects. It's unlikely you want to collect the same object via REST and ZAPI at the same time. Harvest doesn't support this use-case, but does nothing to detect or prevent it. If you want to collect a non-overlapping set of objects with REST and ZAPI, you can. If you do, we recommend you disable the ZAPI object collector. For example, if you enable the REST disk template, you should disable the ZAPI disk template. We do NOT recommend collecting an overlapping set of objects with both collectors since the last one to run will overwrite previously collected data. Harvest will document how to use the REST private cli pass-through to collect custom and non-public counters. The Harvest team recommends that customers open ONTAP issues for REST public API gaps that need filled.","title":"Chosen Decision"},{"location":"architecture/rest-collector/#consequences","text":"The Harvest REST collectors will work with limitations on earlier versions of ONTAP. ONTAP 9.12+ is the minimally validated version. We only validate the full set of templates, dashboards, counters, etc. on versions of ONTAP 9.12+ Harvest does not prevent you from collecting the same resource with ZAPI and REST.","title":"Consequences"},{"location":"architecture/rest-strategy/","text":"REST Strategy \u00b6 Status \u00b6 Accepted Context \u00b6 ONTAP has published a customer product communiqu\u00e9 (CPC-00410) announcing that ZAPIs will reach end of availability (EOA) in ONTAP 9.13.1 released Q2 2023. This document describes how Harvest handles the ONTAP transition from ZAPI to REST. In most cases, no action is required on your part. Harvest API Transition \u00b6 By default, Harvest will use ZAPIs up until ONTAP version 9.12.1 . Beginning with ONTAP 9.12.1 and after, Harvest will default to REST. Harvest includes a full set of REST templates that export identical metrics as the included ZAPI templates. No changes to dashboards or downstream metric-consumers will be required. See below if you have added metrics to the Harvest out-of-the-box templates. Read on if you want to know how you can use REST sooner, or you want to take advantage of REST-only features in ONTAP. Frequently Asked Questions \u00b6 How does Harvest decide whether to use REST or ZAPI APIs? \u00b6 Harvest asks the cluster for its ONTAP version: If the version is earlier than 9.12.1 , Harvest will use the collector(s) defined in your harvest.yml . If the version is 9.12.1 , Harvest will use REST, unless you set the no-upgrade environment variable . If the version is 9.13.1 or later, Harvest will use REST, because ZAPI has been removed. graph TD A(Harvest asks the cluster<br>for its ONTAP version) --> B(Version before<br>9.12.X?) A --> C(9.12.X) A --> D(9.13.X) B --> AA{Does your harvest.yml<br>specify a REST collector?} AA -->|No| F(Use ZAPI) AA -->|Yes|G(Use REST) C --> CC{Is HARVEST_NO_COLLECTOR_UPGRADE<br>environment<br>variable set?} CC --> |No| G CC --> |Yes|CZ(Use ZAPI) D --> X(Use REST) Why would I switch to REST before 9.13.1 ? \u00b6 You have advanced use cases to validate before ONTAP removes ZAPIs in 9.13.1 You want to take advantage of new ONTAP features that are only available via REST (e.g. cloud features, event remediation's, name services, cluster peers, etc.) You want to collect a metric that is not available via ZAPI You want to collect a metric from the ONTAP CLI. The REST API includes a private CLI pass-through to access any ONTAP CLI command Can I start using REST before 9.13.1 ? \u00b6 Yes. Several customers already are. There are a few caveats to be aware of: Harvest collects config counters via REST by enabling the Rest collector in your harvest.yml , but ONTAP did not include performance counters via REST until 9.11.1 . That means Harvest's RestPerf collector won't work until 9.11.1 . ONTAP supports a subset of performance counters in 9.11.1 . The full set is available in 9.12.1 . It's preferable to publish a set of metrics once, instead of multiple times. Typically, you do not want to enable both the Zapi and Rest collector for an overlapping set of objects on the same cluster. It will work, but you'll put more load on the cluster and push duplicate metrics to Prometheus. See below for details on how to use both collectors at the same time. There may be performance metrics missing from versions of ONTAP earlier than 9.11.1 . A counter is missing from REST. What do I do? \u00b6 The Harvest team has ensured that all the out-of-the-box ZAPI templates have matching REST templates with the same metrics. Any additional counters you have added may be missing in REST. Join the Harvest discord channel and ask us about the counter. Sometimes we may know which release the missing counter is coming in, otherwise we can point you to the ONTAP process to request new counters . Can I use the REST and ZAPI collectors at the same time? \u00b6 Yes. It's best when using both collectors to ensure that you aren't collecting the same object(s) multiple times. For example, there is nothing to be gained by collecting disk from both collectors. Harvest won't do anything to prevent you from doing that, but our recommendation when using both collectors, is to use a non-overlapping set of objects. Typically, you will use ZAPI collectors with the out-of-the-box templates and add new REST templates for new objects. For example, if you want to collect controller RAM status you must use the REST collector, since there is no ZAPI that returns that metric. I've added counters to existing ZAPI templates. Will those counters work in REST? \u00b6 ZAPI config metrics often have a REST equivalent that can be found in ONTAP's ONTAPI to REST mapping document . ZAPI performance metrics may be missing in REST. If you have added new metrics or templates to the ZAPIPerf collector, those metrics likely aren't available via REST. You can check if the performance counter is available or ask the Harvest team on Discord . I'm using ONTAP version 9.12.X, but I want to continue using ZAPIs. How do I do that? \u00b6 Set the environment variable HARVEST_NO_COLLECTOR_UPGRADE=1 and Harvest will not upgrade your collector from ZAPI to REST. Reference \u00b6 Table of ONTAP versions, dates and API notes. ONTAP version Release Date ONTAP Notes 9.11.1 Q2 2022 First version with REST performance metrics 9.12.1 Q4 2022 ZAPIs still supported - REST performance metrics have parity with Harvest collected ZAPI performance metrics 9.13.1 Q2 2023 ZAPIs removed. REST only release - REST config and performance parity with ZAPIs","title":"REST Strategy"},{"location":"architecture/rest-strategy/#rest-strategy","text":"","title":"REST Strategy"},{"location":"architecture/rest-strategy/#status","text":"Accepted","title":"Status "},{"location":"architecture/rest-strategy/#context","text":"ONTAP has published a customer product communiqu\u00e9 (CPC-00410) announcing that ZAPIs will reach end of availability (EOA) in ONTAP 9.13.1 released Q2 2023. This document describes how Harvest handles the ONTAP transition from ZAPI to REST. In most cases, no action is required on your part.","title":"Context"},{"location":"architecture/rest-strategy/#harvest-api-transition","text":"By default, Harvest will use ZAPIs up until ONTAP version 9.12.1 . Beginning with ONTAP 9.12.1 and after, Harvest will default to REST. Harvest includes a full set of REST templates that export identical metrics as the included ZAPI templates. No changes to dashboards or downstream metric-consumers will be required. See below if you have added metrics to the Harvest out-of-the-box templates. Read on if you want to know how you can use REST sooner, or you want to take advantage of REST-only features in ONTAP.","title":"Harvest API Transition"},{"location":"architecture/rest-strategy/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"architecture/rest-strategy/#how-does-harvest-decide-whether-to-use-rest-or-zapi-apis","text":"Harvest asks the cluster for its ONTAP version: If the version is earlier than 9.12.1 , Harvest will use the collector(s) defined in your harvest.yml . If the version is 9.12.1 , Harvest will use REST, unless you set the no-upgrade environment variable . If the version is 9.13.1 or later, Harvest will use REST, because ZAPI has been removed. graph TD A(Harvest asks the cluster<br>for its ONTAP version) --> B(Version before<br>9.12.X?) A --> C(9.12.X) A --> D(9.13.X) B --> AA{Does your harvest.yml<br>specify a REST collector?} AA -->|No| F(Use ZAPI) AA -->|Yes|G(Use REST) C --> CC{Is HARVEST_NO_COLLECTOR_UPGRADE<br>environment<br>variable set?} CC --> |No| G CC --> |Yes|CZ(Use ZAPI) D --> X(Use REST)","title":"How does Harvest decide whether to use REST or ZAPI APIs?"},{"location":"architecture/rest-strategy/#why-would-i-switch-to-rest-before-9131","text":"You have advanced use cases to validate before ONTAP removes ZAPIs in 9.13.1 You want to take advantage of new ONTAP features that are only available via REST (e.g. cloud features, event remediation's, name services, cluster peers, etc.) You want to collect a metric that is not available via ZAPI You want to collect a metric from the ONTAP CLI. The REST API includes a private CLI pass-through to access any ONTAP CLI command","title":"Why would I switch to REST before 9.13.1?"},{"location":"architecture/rest-strategy/#can-i-start-using-rest-before-9131","text":"Yes. Several customers already are. There are a few caveats to be aware of: Harvest collects config counters via REST by enabling the Rest collector in your harvest.yml , but ONTAP did not include performance counters via REST until 9.11.1 . That means Harvest's RestPerf collector won't work until 9.11.1 . ONTAP supports a subset of performance counters in 9.11.1 . The full set is available in 9.12.1 . It's preferable to publish a set of metrics once, instead of multiple times. Typically, you do not want to enable both the Zapi and Rest collector for an overlapping set of objects on the same cluster. It will work, but you'll put more load on the cluster and push duplicate metrics to Prometheus. See below for details on how to use both collectors at the same time. There may be performance metrics missing from versions of ONTAP earlier than 9.11.1 .","title":"Can I start using REST before 9.13.1?"},{"location":"architecture/rest-strategy/#a-counter-is-missing-from-rest-what-do-i-do","text":"The Harvest team has ensured that all the out-of-the-box ZAPI templates have matching REST templates with the same metrics. Any additional counters you have added may be missing in REST. Join the Harvest discord channel and ask us about the counter. Sometimes we may know which release the missing counter is coming in, otherwise we can point you to the ONTAP process to request new counters .","title":"A counter is missing from REST. What do I do?"},{"location":"architecture/rest-strategy/#can-i-use-the-rest-and-zapi-collectors-at-the-same-time","text":"Yes. It's best when using both collectors to ensure that you aren't collecting the same object(s) multiple times. For example, there is nothing to be gained by collecting disk from both collectors. Harvest won't do anything to prevent you from doing that, but our recommendation when using both collectors, is to use a non-overlapping set of objects. Typically, you will use ZAPI collectors with the out-of-the-box templates and add new REST templates for new objects. For example, if you want to collect controller RAM status you must use the REST collector, since there is no ZAPI that returns that metric.","title":"Can I use the REST and ZAPI collectors at the same time?"},{"location":"architecture/rest-strategy/#ive-added-counters-to-existing-zapi-templates-will-those-counters-work-in-rest","text":"ZAPI config metrics often have a REST equivalent that can be found in ONTAP's ONTAPI to REST mapping document . ZAPI performance metrics may be missing in REST. If you have added new metrics or templates to the ZAPIPerf collector, those metrics likely aren't available via REST. You can check if the performance counter is available or ask the Harvest team on Discord .","title":"I've added counters to existing ZAPI templates. Will those counters work in REST?"},{"location":"architecture/rest-strategy/#im-using-ontap-version-912x-but-i-want-to-continue-using-zapis-how-do-i-do-that","text":"Set the environment variable HARVEST_NO_COLLECTOR_UPGRADE=1 and Harvest will not upgrade your collector from ZAPI to REST.","title":"I'm using ONTAP version 9.12.X, but I want to continue using ZAPIs. How do I do that?"},{"location":"architecture/rest-strategy/#reference","text":"Table of ONTAP versions, dates and API notes. ONTAP version Release Date ONTAP Notes 9.11.1 Q2 2022 First version with REST performance metrics 9.12.1 Q4 2022 ZAPIs still supported - REST performance metrics have parity with Harvest collected ZAPI performance metrics 9.13.1 Q2 2023 ZAPIs removed. REST only release - REST config and performance parity with ZAPIs","title":"Reference"},{"location":"install/containers/","text":"Overview \u00b6 Harvest is container-ready and supports several deployment options: Stand-up Prometheus, Grafana, and Harvest via Docker Compose . Choose this if you want to hit the ground running. Install, volume and network mounts automatically handled. Poller-per-container model that offers more flexibility in configuration. This deployment enables a broad range of orchestrators (Nomad, Mesosphere, Swarm, K8, etc.) since you pick-and-choose what gets built and how it's deployed, stronger familiarity with containers is recommended. If you prefer Ansible, David Blackwell created an Ansible script that stands up Harvest, Grafana, and Prometheus. Want to run Harvest on a Mac via containerd and Racher Desktop ? We got you covered. Local K8 Deployment via Kompose Docker Compose \u00b6 This is a quick way to install and get started with Harvest. Follow the four steps below to: Setup Harvest, Grafana, and Prometheus via Docker Compose Harvest dashboards are automatically imported and setup in Grafana with a Prometheus data source A separate poller container is created for each monitored cluster All pollers are automatically added as Prometheus scrape targets Download and untar \u00b6 Download the latest version of Harvest , untar, and cd into the harvest directory Setup harvest.yml \u00b6 Create a harvest.yml file with your cluster details, below is an example with annotated comments. Modify as needed for your scenario. This config is using the Prometheus exporter port_range feature, so you don't have to manage the Prometheus exporter port mappings for each poller. Exporters: prometheus1: exporter: Prometheus addr: 0.0.0.0 port_range: 2000-2030 # <====== adjust to be greater than equal to the number of monitored clusters Defaults: collectors: - Zapi - ZapiPerf use_insecure_tls: true # <====== adjust as needed to enable/disable TLS checks exporters: - prometheus1 Pollers: infinity: # <====== add your cluster(s) here, they use the exporter defined three lines above datacenter: DC-01 addr: 10.0.1.2 auth_style: basic_auth username: user password: 123#abc # next cluster .... Generate a Docker compose for your Pollers \u00b6 Generate a Docker compose file from your harvest.yml bin/harvest generate docker full --output harvest-compose.yml generate docker full does two things: Creates a Docker compose file with a container for each Harvest poller defined in your harvest.yml Creates a matching Prometheus service discovery file for each Harvest poller (located in docker/prometheus/harvest_targets.yml ). Prometheus uses this file to scrape the Harvest pollers. Start everything \u00b6 Bring everything up docker-compose -f prom-stack.yml -f harvest-compose.yml up -d --remove-orphans Prometheus and Grafana \u00b6 The prom-stack.yml compose file creates a frontend and backend network. Prometheus and Grafana publish their admin ports on the front-end network and are routable to the local machine. By default, the Harvest pollers are part of the backend network and do not expose their Prometheus web end-points. If you want their end-points exposed, pass the --port flag to the generate sub-command in the previous step , like so: bin/harvest generate docker full --port --output harvest-compose.yml Prometheus \u00b6 After bringing up the prom-stack.yml compose file, you can check Prometheus's list of targets at http://IP_OF_PROMETHEUS:9090/targets . Grafana \u00b6 After bringing up the prom-stack.yml compose file, you can access Grafana at http://IP_OF_GRAFANA:3000 . Default credentials - you'll be prompted to create a new password the first time you log in username: admin password: admin Manage pollers \u00b6 How do I add a new poller? \u00b6 Add poller to harvest.yml Regenerate compose file by running bin/harvest generate Run docker compose up , for example, docker-compose -f prom-stack.yml -f harvest-compose.yml up -d --remove-orphans Stop all containers \u00b6 docker-compose -f prom-stack.yml -f harvest-compose.yml down Upgrade Harvest \u00b6 Note: If you want to keep your historical Prometheus data, and you set up your Docker Compose workflow before Harvest 22.11 , please read how to migrate your Prometheus volume before continuing with the upgrade steps below. To upgrade Harvest: Download the latest tar.gz or packaged version and install it. This is needed since the new version may contain new templates, dashboards, or other files not included in the Docker image. Copy your existing harvest.yml into the new Harvest directory created in step #1. Regenerate your harvest-compose.yml file by running bin/harvest generate docker full --port --output harvest-compose.yml Pull new images and restart your containers like so: docker pull cr.netapp.io/harvest # or if using Docker Hub: docker pull rahulguptajss/harvest docker-compose -f prom-stack.yml -f harvest-compose.yml up -d --remove-orphans","title":"Containers"},{"location":"install/containers/#overview","text":"Harvest is container-ready and supports several deployment options: Stand-up Prometheus, Grafana, and Harvest via Docker Compose . Choose this if you want to hit the ground running. Install, volume and network mounts automatically handled. Poller-per-container model that offers more flexibility in configuration. This deployment enables a broad range of orchestrators (Nomad, Mesosphere, Swarm, K8, etc.) since you pick-and-choose what gets built and how it's deployed, stronger familiarity with containers is recommended. If you prefer Ansible, David Blackwell created an Ansible script that stands up Harvest, Grafana, and Prometheus. Want to run Harvest on a Mac via containerd and Racher Desktop ? We got you covered. Local K8 Deployment via Kompose","title":"Overview"},{"location":"install/containers/#docker-compose","text":"This is a quick way to install and get started with Harvest. Follow the four steps below to: Setup Harvest, Grafana, and Prometheus via Docker Compose Harvest dashboards are automatically imported and setup in Grafana with a Prometheus data source A separate poller container is created for each monitored cluster All pollers are automatically added as Prometheus scrape targets","title":"Docker Compose"},{"location":"install/containers/#download-and-untar","text":"Download the latest version of Harvest , untar, and cd into the harvest directory","title":"Download and untar"},{"location":"install/containers/#setup-harvestyml","text":"Create a harvest.yml file with your cluster details, below is an example with annotated comments. Modify as needed for your scenario. This config is using the Prometheus exporter port_range feature, so you don't have to manage the Prometheus exporter port mappings for each poller. Exporters: prometheus1: exporter: Prometheus addr: 0.0.0.0 port_range: 2000-2030 # <====== adjust to be greater than equal to the number of monitored clusters Defaults: collectors: - Zapi - ZapiPerf use_insecure_tls: true # <====== adjust as needed to enable/disable TLS checks exporters: - prometheus1 Pollers: infinity: # <====== add your cluster(s) here, they use the exporter defined three lines above datacenter: DC-01 addr: 10.0.1.2 auth_style: basic_auth username: user password: 123#abc # next cluster ....","title":"Setup harvest.yml"},{"location":"install/containers/#generate-a-docker-compose-for-your-pollers","text":"Generate a Docker compose file from your harvest.yml bin/harvest generate docker full --output harvest-compose.yml generate docker full does two things: Creates a Docker compose file with a container for each Harvest poller defined in your harvest.yml Creates a matching Prometheus service discovery file for each Harvest poller (located in docker/prometheus/harvest_targets.yml ). Prometheus uses this file to scrape the Harvest pollers.","title":"Generate a Docker compose for your Pollers"},{"location":"install/containers/#start-everything","text":"Bring everything up docker-compose -f prom-stack.yml -f harvest-compose.yml up -d --remove-orphans","title":"Start everything"},{"location":"install/containers/#prometheus-and-grafana","text":"The prom-stack.yml compose file creates a frontend and backend network. Prometheus and Grafana publish their admin ports on the front-end network and are routable to the local machine. By default, the Harvest pollers are part of the backend network and do not expose their Prometheus web end-points. If you want their end-points exposed, pass the --port flag to the generate sub-command in the previous step , like so: bin/harvest generate docker full --port --output harvest-compose.yml","title":"Prometheus and Grafana"},{"location":"install/containers/#prometheus","text":"After bringing up the prom-stack.yml compose file, you can check Prometheus's list of targets at http://IP_OF_PROMETHEUS:9090/targets .","title":"Prometheus"},{"location":"install/containers/#grafana","text":"After bringing up the prom-stack.yml compose file, you can access Grafana at http://IP_OF_GRAFANA:3000 . Default credentials - you'll be prompted to create a new password the first time you log in username: admin password: admin","title":"Grafana"},{"location":"install/containers/#manage-pollers","text":"","title":"Manage pollers"},{"location":"install/containers/#how-do-i-add-a-new-poller","text":"Add poller to harvest.yml Regenerate compose file by running bin/harvest generate Run docker compose up , for example, docker-compose -f prom-stack.yml -f harvest-compose.yml up -d --remove-orphans","title":"How do I add a new poller?"},{"location":"install/containers/#stop-all-containers","text":"docker-compose -f prom-stack.yml -f harvest-compose.yml down","title":"Stop all containers"},{"location":"install/containers/#upgrade-harvest","text":"Note: If you want to keep your historical Prometheus data, and you set up your Docker Compose workflow before Harvest 22.11 , please read how to migrate your Prometheus volume before continuing with the upgrade steps below. To upgrade Harvest: Download the latest tar.gz or packaged version and install it. This is needed since the new version may contain new templates, dashboards, or other files not included in the Docker image. Copy your existing harvest.yml into the new Harvest directory created in step #1. Regenerate your harvest-compose.yml file by running bin/harvest generate docker full --port --output harvest-compose.yml Pull new images and restart your containers like so: docker pull cr.netapp.io/harvest # or if using Docker Hub: docker pull rahulguptajss/harvest docker-compose -f prom-stack.yml -f harvest-compose.yml up -d --remove-orphans","title":"Upgrade Harvest"},{"location":"install/native/","text":"Native \u00b6 Visit the Releases page and copy the tar.gz link for the latest release. For example, to download the v22.08.0 release: wget https://github.com/NetApp/harvest/releases/download/v22.08.0/harvest-22.08.0-1_linux_amd64.tar.gz tar -xvf harvest-22.08.0-1_linux_amd64.tar.gz cd harvest-22.08.0-1_linux_amd64 # Run Harvest with the default unix localhost collector bin/harvest start With curl If you don't have wget installed, you can use curl like so: curl -L -O https://github.com/NetApp/harvest/releases/download/v22.08.0/harvest-22.08.0-1_linux_amd64.tar.gz It's best to run Harvest as a non-root user. Make sure the user running Harvest can write to /var/log/harvest/ or tell Harvest to write the logs somewhere else with the HARVEST_LOGS environment variable. If something goes wrong, examine the logs files in /var/log/harvest , check out the troubleshooting section on the wiki and jump onto Discord and ask for help.","title":"Native"},{"location":"install/native/#native","text":"Visit the Releases page and copy the tar.gz link for the latest release. For example, to download the v22.08.0 release: wget https://github.com/NetApp/harvest/releases/download/v22.08.0/harvest-22.08.0-1_linux_amd64.tar.gz tar -xvf harvest-22.08.0-1_linux_amd64.tar.gz cd harvest-22.08.0-1_linux_amd64 # Run Harvest with the default unix localhost collector bin/harvest start With curl If you don't have wget installed, you can use curl like so: curl -L -O https://github.com/NetApp/harvest/releases/download/v22.08.0/harvest-22.08.0-1_linux_amd64.tar.gz It's best to run Harvest as a non-root user. Make sure the user running Harvest can write to /var/log/harvest/ or tell Harvest to write the logs somewhere else with the HARVEST_LOGS environment variable. If something goes wrong, examine the logs files in /var/log/harvest , check out the troubleshooting section on the wiki and jump onto Discord and ask for help.","title":"Native"},{"location":"install/overview/","text":"Get up and running with Harvest on your preferred platform. We provide pre-compiled binaries for Linux, RPMs, Debs, as well as prebuilt container images for both nightly and stable releases . Binaries for Linux RPM and Debs Containers Nabox \u00b6 Instructions on how to install Harvest via NAbox . Source \u00b6 To build Harvest from source code, first make sure you have a working Go environment with version 1.19 or greater installed . Clone the repo and build everything. git clone https://github.com/NetApp/harvest.git cd harvest make build bin/harvest version If you're building on a Mac use GOOS=darwin make build Checkout the Makefile for other targets of interest.","title":"Overview"},{"location":"install/overview/#nabox","text":"Instructions on how to install Harvest via NAbox .","title":"Nabox"},{"location":"install/overview/#source","text":"To build Harvest from source code, first make sure you have a working Go environment with version 1.19 or greater installed . Clone the repo and build everything. git clone https://github.com/NetApp/harvest.git cd harvest make build bin/harvest version If you're building on a Mac use GOOS=darwin make build Checkout the Makefile for other targets of interest.","title":"Source"},{"location":"install/package-managers/","text":"Redhat \u00b6 Installation and upgrade of the Harvest package may require root or administrator privileges Download the latest rpm of Harvest from the releases tab and install or upgrade with yum. sudo yum install harvest.XXX.rpm Once the installation has finished, edit the harvest.yml configuration file located in /opt/harvest/harvest.yml After editing /opt/harvest/harvest.yml , manage Harvest with systemctl start|stop|restart harvest . After upgrade, re-import all dashboards (either bin/harvest grafana import cli or via the Grafana UI) to get any new enhancements in dashboards. To ensure that you don't run into permission issues , make sure you manage Harvest using systemctl instead of running the harvest binary directly. Changes install makes Directories /var/log/harvest/ and /var/log/run/ are created A harvest user and group are created and the installed files are chowned to harvest Systemd /etc/systemd/system/harvest.service file is created and enabled Debian \u00b6 Installation and upgrade of the Harvest package may require root or administrator privileges Download the latest deb of Harvest from the releases tab and install or upgrade with apt. sudo apt update sudo apt install|upgrade ./harvest-<RELEASE>.amd64.deb Once the installation has finished, edit the harvest.yml configuration file located in /opt/harvest/harvest.yml After editing /opt/harvest/harvest.yml , manage Harvest with systemctl start|stop|restart harvest . After upgrade, re-import all dashboards (either bin/harvest grafana import cli or via the Grafana UI) to get any new enhancements in dashboards. To ensure that you don't run into permission issues , make sure you manage Harvest using systemctl instead of running the harvest binary directly. Changes install makes Directories /var/log/harvest/ and /var/log/run/ are created A harvest user and group are created and the installed files are chowned to harvest Systemd /etc/systemd/system/harvest.service file is created and enabled","title":"Package Managers"},{"location":"install/package-managers/#redhat","text":"Installation and upgrade of the Harvest package may require root or administrator privileges Download the latest rpm of Harvest from the releases tab and install or upgrade with yum. sudo yum install harvest.XXX.rpm Once the installation has finished, edit the harvest.yml configuration file located in /opt/harvest/harvest.yml After editing /opt/harvest/harvest.yml , manage Harvest with systemctl start|stop|restart harvest . After upgrade, re-import all dashboards (either bin/harvest grafana import cli or via the Grafana UI) to get any new enhancements in dashboards. To ensure that you don't run into permission issues , make sure you manage Harvest using systemctl instead of running the harvest binary directly. Changes install makes Directories /var/log/harvest/ and /var/log/run/ are created A harvest user and group are created and the installed files are chowned to harvest Systemd /etc/systemd/system/harvest.service file is created and enabled","title":"Redhat"},{"location":"install/package-managers/#debian","text":"Installation and upgrade of the Harvest package may require root or administrator privileges Download the latest deb of Harvest from the releases tab and install or upgrade with apt. sudo apt update sudo apt install|upgrade ./harvest-<RELEASE>.amd64.deb Once the installation has finished, edit the harvest.yml configuration file located in /opt/harvest/harvest.yml After editing /opt/harvest/harvest.yml , manage Harvest with systemctl start|stop|restart harvest . After upgrade, re-import all dashboards (either bin/harvest grafana import cli or via the Grafana UI) to get any new enhancements in dashboards. To ensure that you don't run into permission issues , make sure you manage Harvest using systemctl instead of running the harvest binary directly. Changes install makes Directories /var/log/harvest/ and /var/log/run/ are created A harvest user and group are created and the installed files are chowned to harvest Systemd /etc/systemd/system/harvest.service file is created and enabled","title":"Debian"},{"location":"resources/matrix/","text":"Matrix \u00b6 The \u2133atri\u03c7 package provides the matrix.Matrix data-structure for storage, manipulation and transmission of both numeric and non-numeric (string) data. It is utilized by core components of Harvest, including collectors, plugins and exporters. It furthermore serves as an interface between these components, such that \"the left hand does not know what the right hand does\". Internally, the Matrix is a collection of metrics ( matrix.Metric ) and instances ( matrix.Instance ) in the form of a 2-dimensional array: Since we use hash tables for accessing the elements of the array, all metrics and instances added to the matrix must have a unique key. Metrics are typed and contain the numeric data (i.e. rows) of the Matrix. Instances only serve as pointers to the columents of the Matrix, but they also store non-numeric data as labels ( *dict.Dict ). This package is the architectural backbone of Harvest, therefore understanding it is key for an advanced user or contributor. Basic Usage \u00b6 Initialize \u00b6 func matrix . New ( name , object string , identifier string ) * Matrix // always returns successfully pointer to (empty) Matrix This section describes how to properly initialize a new Matrix instance. Note that if you write a collector, a Matrix instance is already properly initialized for you (as MyCollector.matrix ), and if you write a plugin or exporter, it is passed to you from the collector. That means most of the time you don't have to worry about initializing the Matrix. matrix.New() requires three arguments: * UUID is by convention the collector name (e.g. MyCollector ) if the Matrix comes from a collector, or the collector name and the plugin name concatenated with a . (e.g. MyCollector.MyPlugin ) if the Matrix comes from a plugin. * object is a description of the instances of the Matrix. For example, if we collect data about cars and our instances are cars, a good name would be car . * identifier is a unique key used to identify a matrix instance Note that identifier should uniquely identify a Matrix instance. This is not a strict requirement, but guarantees that your data is properly handled by exporters. Example \u00b6 Here is an example from the point of view of a collector: import \"github.com/netapp/harvest/v2/pkg/matrix\" var myMatrix * matrix . Matrix myMatrix = matrix . New ( \"CarCollector\" , \"car\" , \"car\" ) Next step is to add metrics and instances to our Matrix. Add instances and instance labels \u00b6 func ( x * Matrix ) NewInstance ( key string ) ( * Instance , error ) // returns pointer to a new Instance, or nil with error (if key is not unique) func ( i * Instance ) SetLabel ( key , value string ) // always successful, overwrites existing values func ( i * Instance ) GetLabel ( key ) string // always returns value, if label is not set, returns empty string Once we have initialized a Matrix, we can add instances and add labels to our instances. Example \u00b6 var ( instance * matrix . Instance err error ) if instance , err = myMatrix . NewInstance ( \"SomeCarMark\" ); err != nil { return err // or handle err, but beware that instance is nil } instance . SetLabel ( \"mark\" , \"SomeCarMark\" ) instance . SetLabel ( \"color\" , \"red\" ) instance . SetLabel ( \"style\" , \"coupe\" ) // add as many labels as you like instance . GetLabel ( \"color\" ) // return \"red\" instance . GetLabel ( \"owner\" ) // returns \"\" Add Metrics \u00b6 func ( x * Matrix ) NewMetricInt64 ( key string ) ( Metric , error ) // returns pointer to a new MetricInt64, or nil with error (if key is not unique) // note that Metric is an interface Metrics are typed and there are currently 8 types, all can be created with the same signature as above: * MetricUint8 * MetricUint32 * MetricUint64 * MetricInt * MetricInt32 * MetricInt64 * MetricFloat32 * MetricFloat64 * We are able to read from and write to a metric instance using different types (as displayed in the next section), however choosing a type wisely ensures that this is done efficiently and overflow does not occur. We can add labels to metrics just like instances. This is usually done when we deal with histograms: func ( m Metric ) SetLabel ( key , value string ) // always successful, overwrites existing values func ( m Metric ) GetLabel ( key ) string // always returns value, if label is not set, returns empty string Example \u00b6 Continuing our Matrix for collecting car-related data: var ( speed , length matrix . Metric err error ) if speed , err = myMatrix . NewMetricUint32 ( \"max_speed\" ); err != nil { return err } if length , err = myMatrix . NewMetricFloat32 ( \"length_in_mm\" ); err != nil { return err } Write numeric data \u00b6 func ( x * Matrix ) Reset () // flush numeric data from previous poll func ( m Metric ) SetValueInt64 ( i * Instance , v int64 ) error func ( m Metric ) SetValueUint8 ( i * Instance , v uint8 ) error func ( m Metric ) SetValueUint64 ( i * Instance , v uint64 ) error func ( m Metric ) SetValueFloat64 ( i * Instance , v float64 ) error func ( m Metric ) SetValueBytes ( i * Instance , v [] byte ) error func ( m Metric ) SetValueString ( i * Instance , v [] string ) error // sets the numeric value for the instance i to v // returns error if v is invalid (explained below) func ( m Metric ) AddValueInt64 ( i * Instance , v int64 ) error // increments the numeric value for the instance i by v // same signatures for all the types defined above When possible you should reuse a Matrix for each data poll, but to do that, you need to call Reset() to drop old data from the Matrix. It is safe to add new instances and metrics after calling this method. The SetValue*() and AddValue*() methods are typed same as the metrics. Even though you are not required to use the same type as the metric, it is the safest and most efficient way. Since most collectors get their data as bytes or strings, it is recommended to use the SetValueString() and SetValueBytes() methods. These methods return an error if value v can not be converted to the type of the metric. Error is always nil when the type of v matches the type of the metric. Example \u00b6 Continuing with the previous examples: if err = myMatrix . Reset (); err != nil { return } // write numbers to the matrix using the instance and the metrics we have created // let the metric do the conversion for us if err = speed . SetValueString ( instance , \"500\" ); err != nil { logger . Error ( me . Prefix , \"set speed value: \" , err ) } // here we ignore err since type is the metric type length . SetValueFloat64 ( instance , 10000.00 ) // safe to add new instances var instance2 matrix . Instance if instance2 , err = myMatrix . NewInstance ( \"SomeOtherCar\" ); err != nil { return err } // possible and safe even though speed has type Float32 } if err = length . SetValueInt64 ( instance2 , 13000 ); err != nil { logger . Error ( me . Prefix , \"set speed value:\" , err ) } // possible, but will overflow since speed is unsigned } if err = speed . SetValueInt64 ( instance2 , - 500 ); err != nil { logger . Error ( me . Prefix , \"set length value:\" , err ) } Read metrics and instances \u00b6 In this section we switch gears and look at the Matrix from the point of view of plugins and exporters. Both those components need to read from the Matrix and have no knowledge of its origin or contents. func ( x * Matrix ) GetMetrics () map [ string ] Metric // returns all metrics in the Matrix func ( x * Matrix ) GetInstances () map [ string ] * Instance // returns all instances in the Matrix Usually we will do a nested loop with these two methods to read all data in the Matrix. See examples below. Example: Iterate over instances \u00b6 In this example the method PrintKeys() will iterate over a Matrix and print all metric and instance keys. func PrintKeys ( x * matrix . Matrix ) { for instanceKey , _ := range x . GetInstances () { fmt . Println ( \"instance key=\" , instanceKey ) } } Example: Read instance labels \u00b6 Each instance has a set of labels. We can iterate over these labels with the GetLabel() and GetLabels() method. In this example, we write a function that prints all labels of an instance: func PrintLabels ( instance * matrix . Instance ) { for label , value , := range instance . GetLabels (). Map () { fmt . Printf ( \"%s=%s\\n\" , label , value ) } } Example: Read metric values labels \u00b6 Similar to the SetValue* and AddValue* methods, you can choose a type when reading from a metric. If you don't know the type of the metric, it is safe to read it as a string. In this example, we write a function that prints the value of a metric for all instances in a Matrix: func PrintMetricValues ( x * matrix . Matrix , m matrix . Metric ) { for key , instance := range x . GetInstances () { if value , has := m . GetValueString ( instance ) { fmt . Printf ( \"instance %s = %s\\n\" , key , value ) } else { fmt . Printf ( \"instance %s has no value\\n\" , key ) } } }","title":"Matrix"},{"location":"resources/matrix/#matrix","text":"The \u2133atri\u03c7 package provides the matrix.Matrix data-structure for storage, manipulation and transmission of both numeric and non-numeric (string) data. It is utilized by core components of Harvest, including collectors, plugins and exporters. It furthermore serves as an interface between these components, such that \"the left hand does not know what the right hand does\". Internally, the Matrix is a collection of metrics ( matrix.Metric ) and instances ( matrix.Instance ) in the form of a 2-dimensional array: Since we use hash tables for accessing the elements of the array, all metrics and instances added to the matrix must have a unique key. Metrics are typed and contain the numeric data (i.e. rows) of the Matrix. Instances only serve as pointers to the columents of the Matrix, but they also store non-numeric data as labels ( *dict.Dict ). This package is the architectural backbone of Harvest, therefore understanding it is key for an advanced user or contributor.","title":"Matrix"},{"location":"resources/matrix/#basic-usage","text":"","title":"Basic Usage"},{"location":"resources/matrix/#initialize","text":"func matrix . New ( name , object string , identifier string ) * Matrix // always returns successfully pointer to (empty) Matrix This section describes how to properly initialize a new Matrix instance. Note that if you write a collector, a Matrix instance is already properly initialized for you (as MyCollector.matrix ), and if you write a plugin or exporter, it is passed to you from the collector. That means most of the time you don't have to worry about initializing the Matrix. matrix.New() requires three arguments: * UUID is by convention the collector name (e.g. MyCollector ) if the Matrix comes from a collector, or the collector name and the plugin name concatenated with a . (e.g. MyCollector.MyPlugin ) if the Matrix comes from a plugin. * object is a description of the instances of the Matrix. For example, if we collect data about cars and our instances are cars, a good name would be car . * identifier is a unique key used to identify a matrix instance Note that identifier should uniquely identify a Matrix instance. This is not a strict requirement, but guarantees that your data is properly handled by exporters.","title":"Initialize"},{"location":"resources/matrix/#example","text":"Here is an example from the point of view of a collector: import \"github.com/netapp/harvest/v2/pkg/matrix\" var myMatrix * matrix . Matrix myMatrix = matrix . New ( \"CarCollector\" , \"car\" , \"car\" ) Next step is to add metrics and instances to our Matrix.","title":"Example"},{"location":"resources/matrix/#add-instances-and-instance-labels","text":"func ( x * Matrix ) NewInstance ( key string ) ( * Instance , error ) // returns pointer to a new Instance, or nil with error (if key is not unique) func ( i * Instance ) SetLabel ( key , value string ) // always successful, overwrites existing values func ( i * Instance ) GetLabel ( key ) string // always returns value, if label is not set, returns empty string Once we have initialized a Matrix, we can add instances and add labels to our instances.","title":"Add instances and instance labels"},{"location":"resources/matrix/#example_1","text":"var ( instance * matrix . Instance err error ) if instance , err = myMatrix . NewInstance ( \"SomeCarMark\" ); err != nil { return err // or handle err, but beware that instance is nil } instance . SetLabel ( \"mark\" , \"SomeCarMark\" ) instance . SetLabel ( \"color\" , \"red\" ) instance . SetLabel ( \"style\" , \"coupe\" ) // add as many labels as you like instance . GetLabel ( \"color\" ) // return \"red\" instance . GetLabel ( \"owner\" ) // returns \"\"","title":"Example"},{"location":"resources/matrix/#add-metrics","text":"func ( x * Matrix ) NewMetricInt64 ( key string ) ( Metric , error ) // returns pointer to a new MetricInt64, or nil with error (if key is not unique) // note that Metric is an interface Metrics are typed and there are currently 8 types, all can be created with the same signature as above: * MetricUint8 * MetricUint32 * MetricUint64 * MetricInt * MetricInt32 * MetricInt64 * MetricFloat32 * MetricFloat64 * We are able to read from and write to a metric instance using different types (as displayed in the next section), however choosing a type wisely ensures that this is done efficiently and overflow does not occur. We can add labels to metrics just like instances. This is usually done when we deal with histograms: func ( m Metric ) SetLabel ( key , value string ) // always successful, overwrites existing values func ( m Metric ) GetLabel ( key ) string // always returns value, if label is not set, returns empty string","title":"Add Metrics"},{"location":"resources/matrix/#example_2","text":"Continuing our Matrix for collecting car-related data: var ( speed , length matrix . Metric err error ) if speed , err = myMatrix . NewMetricUint32 ( \"max_speed\" ); err != nil { return err } if length , err = myMatrix . NewMetricFloat32 ( \"length_in_mm\" ); err != nil { return err }","title":"Example"},{"location":"resources/matrix/#write-numeric-data","text":"func ( x * Matrix ) Reset () // flush numeric data from previous poll func ( m Metric ) SetValueInt64 ( i * Instance , v int64 ) error func ( m Metric ) SetValueUint8 ( i * Instance , v uint8 ) error func ( m Metric ) SetValueUint64 ( i * Instance , v uint64 ) error func ( m Metric ) SetValueFloat64 ( i * Instance , v float64 ) error func ( m Metric ) SetValueBytes ( i * Instance , v [] byte ) error func ( m Metric ) SetValueString ( i * Instance , v [] string ) error // sets the numeric value for the instance i to v // returns error if v is invalid (explained below) func ( m Metric ) AddValueInt64 ( i * Instance , v int64 ) error // increments the numeric value for the instance i by v // same signatures for all the types defined above When possible you should reuse a Matrix for each data poll, but to do that, you need to call Reset() to drop old data from the Matrix. It is safe to add new instances and metrics after calling this method. The SetValue*() and AddValue*() methods are typed same as the metrics. Even though you are not required to use the same type as the metric, it is the safest and most efficient way. Since most collectors get their data as bytes or strings, it is recommended to use the SetValueString() and SetValueBytes() methods. These methods return an error if value v can not be converted to the type of the metric. Error is always nil when the type of v matches the type of the metric.","title":"Write numeric data"},{"location":"resources/matrix/#example_3","text":"Continuing with the previous examples: if err = myMatrix . Reset (); err != nil { return } // write numbers to the matrix using the instance and the metrics we have created // let the metric do the conversion for us if err = speed . SetValueString ( instance , \"500\" ); err != nil { logger . Error ( me . Prefix , \"set speed value: \" , err ) } // here we ignore err since type is the metric type length . SetValueFloat64 ( instance , 10000.00 ) // safe to add new instances var instance2 matrix . Instance if instance2 , err = myMatrix . NewInstance ( \"SomeOtherCar\" ); err != nil { return err } // possible and safe even though speed has type Float32 } if err = length . SetValueInt64 ( instance2 , 13000 ); err != nil { logger . Error ( me . Prefix , \"set speed value:\" , err ) } // possible, but will overflow since speed is unsigned } if err = speed . SetValueInt64 ( instance2 , - 500 ); err != nil { logger . Error ( me . Prefix , \"set length value:\" , err ) }","title":"Example"},{"location":"resources/matrix/#read-metrics-and-instances","text":"In this section we switch gears and look at the Matrix from the point of view of plugins and exporters. Both those components need to read from the Matrix and have no knowledge of its origin or contents. func ( x * Matrix ) GetMetrics () map [ string ] Metric // returns all metrics in the Matrix func ( x * Matrix ) GetInstances () map [ string ] * Instance // returns all instances in the Matrix Usually we will do a nested loop with these two methods to read all data in the Matrix. See examples below.","title":"Read metrics and instances"},{"location":"resources/matrix/#example-iterate-over-instances","text":"In this example the method PrintKeys() will iterate over a Matrix and print all metric and instance keys. func PrintKeys ( x * matrix . Matrix ) { for instanceKey , _ := range x . GetInstances () { fmt . Println ( \"instance key=\" , instanceKey ) } }","title":"Example: Iterate over instances"},{"location":"resources/matrix/#example-read-instance-labels","text":"Each instance has a set of labels. We can iterate over these labels with the GetLabel() and GetLabels() method. In this example, we write a function that prints all labels of an instance: func PrintLabels ( instance * matrix . Instance ) { for label , value , := range instance . GetLabels (). Map () { fmt . Printf ( \"%s=%s\\n\" , label , value ) } }","title":"Example: Read instance labels"},{"location":"resources/matrix/#example-read-metric-values-labels","text":"Similar to the SetValue* and AddValue* methods, you can choose a type when reading from a metric. If you don't know the type of the metric, it is safe to read it as a string. In this example, we write a function that prints the value of a metric for all instances in a Matrix: func PrintMetricValues ( x * matrix . Matrix , m matrix . Metric ) { for key , instance := range x . GetInstances () { if value , has := m . GetValueString ( instance ) { fmt . Printf ( \"instance %s = %s\\n\" , key , value ) } else { fmt . Printf ( \"instance %s has no value\\n\" , key ) } } }","title":"Example: Read metric values labels"},{"location":"resources/templates-and-metrics/","text":"Harvest Templates and Metrics \u00b6 Harvest collects ONTAP counter information, augments it, and stores it in a time-series DB. Refer ONTAP Metrics for details about ONTAP metrics exposed by Harvest. flowchart RL Harvest[Harvest<br>Get & Augment] -- REST<br>ZAPI --> ONTAP id1[(Prometheus<br>Store)] -- Scrape --> Harvest Three concepts work in unison to collect ONTAP metrics data, prepare it and make it available to Prometheus. ZAPI/REST Harvest templates Exporters We're going to walk through an example from a running system, focusing on the disk object. At a high-level, Harvest templates describe what ZAPIs to send to ONTAP and how to interpret the responses. ONTAP defines twos ZAPIs to collect disk info Config information is collected via storage-disk-get-iter Performance counters are collected via disk:constituent These ZAPIs are found in their corresponding object template file conf/zapi/cdot/9.8.0/disk.yaml and conf/zapiperf/cdot/9.8.0/disk.yaml . These files also describe how to map the ZAPI responses into a time-series-friendly format Prometheus uniquely identifies a time series by its metric name and optional key-value pairs called labels. Handy Tools \u00b6 dasel is useful to convert between XML, YAML, JSON, etc. We'll use it to make displaying some of the data easier. ONTAP ZAPI disk example \u00b6 We'll use the bin/zapi tool to interrogate the cluster and gather information about the counters. This is one way you can send ZAPIs to ONTAP and explore the return types and values. bin/zapi -p u2 show attrs --api storage-disk-get-iter Output edited for brevity and line numbers added on left The hierarchy and return type of each counter is shown below. We'll use this hierarchy to build a matching Harvest template. For example, line 3 is the bytes-per-sector counter, which has an integer value, and is the child of storage-disk-info > disk-inventory-info . To capture that counter's value as a metric in a Harvest, the ZAPI template must use the same hierarchical path. The matching path can be seen below . building tree for attribute [attributes-list] => [storage-disk-info] 1 [storage-disk-info] - * 2 [disk-inventory-info] - 3 [bytes-per-sector] - integer 4 [capacity-sectors] - integer 5 [disk-type] - string 6 [is-shared] - boolean 7 [model] - string 8 [serial-number] - string 9 [shelf] - string 10 [shelf-bay] - string 11 [disk-name] - string 12 [disk-ownership-info] - 13 [home-node-name] - string 14 [is-failed] - boolean 15 [owner-node-name] - string 16 [disk-raid-info] - 17 [container-type] - string 18 [disk-outage-info] - 19 [is-in-fdr] - boolean 20 [reason] - string 21 [disk-stats-info] - 22 [average-latency] - integer 23 [disk-io-kbps] - integer 24 [power-on-time-interval] - integer 25 [sectors-read] - integer 26 [sectors-written] - integer 27 [disk-uid] - string 28 [node-name] - string 29 [storage-disk-state] - integer 30 [storage-disk-state-flags] - integer Harvest Templates \u00b6 To understand templates, there are a few concepts to cover: There are three kinds of information included in templates that define what Harvest collects and exports: Configuration information is exported into the _labels metric (e.g. disk_labels see below) Metrics data is exported as disk_\"metric name\" e.g. disk_bytes_per_sector , disk_sectors , etc. Metrics are leaf nodes that are not prefixed with a ^ or ^^. Metrics must be one of the number types: float or int. Plugins may add additional metrics, increasing the number of metrics exported in #2 A resource will typically have multiple instances. Using disk as an example, that means there will be one disk_labels and a metric row per instance. If we have 24 disks and the disk template lists seven metrics to capture, Harvest will export a total of 192 rows of Prometheus data. 24 instances * (7 metrics per instance + 1 label per instance) = 192 rows Sum of disk metrics that Harvest exports curl -s 'http://localhost:14002/metrics' | grep ^disk | cut -d'{' -f1 | sort | uniq -c 24 disk_bytes_per_sector 24 disk_labels 24 disk_sectors 24 disk_stats_average_latency 24 disk_stats_io_kbps 24 disk_stats_sectors_read 24 disk_stats_sectors_written 24 disk_uptime # 192 rows Read on to see how we control which labels from #1 and which metrics from #2 are included in the exported data. Instance Keys and Labels \u00b6 Instance key - An instance key defines the set of attributes Harvest uses to construct a key that uniquely identifies an object. For example, the disk template uses the node + disk attributes to determine uniqueness. Using node or disk alone wouldn't be sufficient since disks on separate nodes can have the same name. If a single label does not uniquely identify an instance, combine multiple keys for uniqueness. Instance keys must refer to attributes that are of type string . Because instance keys define uniqueness, these keys are also added to each metric as a key-value pair. ( see Control What Labels and Metrics are Exported for examples) Instance label - Labels are key-value pairs used to gather configuration information about each instance. All of the key-value pairs are combined into a single metric named disk_labels . There will be one disk_labels for each monitored instance. Here's an example reformatted so it's easier to read: disk_labels{ datacenter=\"dc-1\", cluster=\"umeng-aff300-05-06\", node=\"umeng-aff300-06\", disk=\"1.1.23\", type=\"SSD\", model=\"X371_S1643960ATE\", outage=\"\", owner_node=\"umeng-aff300-06\", shared=\"true\", shelf=\"1\", shelf_bay=\"23\", serial_number=\"S3SENE0K500532\", failed=\"false\", container_type=\"shared\" } Harvest Object Template \u00b6 Continuing with the disk example, below is the conf/zapi/cdot/9.8.0/disk.yaml that tells Harvest which ZAPI to send to ONTAP ( storage-disk-get-iter ) and describes how to interpret and export the response. Line 1 defines the name of this resource and is an exact match to the object defined in your default.yaml or custom.yaml file. Eg. # default.yaml objects: Disk: disk.yaml Line 2 is the name of the ZAPI that Harvest will send to collect disk resources Line 3 is the prefix used to export metrics associated with this object. i.e. all metrics will be of the form disk_* Line 5 the counter section is where we define the metrics, labels, and what constitutes instance uniqueness Line 7 the double hat prefix ^^ means this attribute is an instance key used to determine uniqueness. Instance keys are also included as labels. Uuids are good choices for uniqueness Line 13 the single hat prefix ^ means this attribute should be stored as a label. That means we can include it in the export_options section as one of the key-value pairs in disk_labels Rows 10, 11, 23, 24, 25, 26, 27 - these are the metrics rows - metrics are leaf nodes that are not prefixed with a ^ or ^^. If you refer back to the ONTAP ZAPI disk example above, you'll notice each of these attributes are integer types. Line 43 defines the set of labels to use when constructing the disk_labels metrics. As mentioned above , these labels capture config-related attributes per instance. Output edited for brevity and line numbers added for reference. 1 name: Disk 2 query: storage-disk-get-iter 3 object: disk 4 5 counters: 6 storage-disk-info: 7 - ^^disk-uid 8 - ^^disk-name => disk 9 - disk-inventory-info: 10 - bytes-per-sector => bytes_per_sector # notice this has the same hierarchical path we saw from bin/zapi 11 - capacity-sectors => sectors 12 - ^disk-type => type 13 - ^is-shared => shared 14 - ^model => model 15 - ^serial-number => serial_number 16 - ^shelf => shelf 17 - ^shelf-bay => shelf_bay 18 - disk-ownership-info: 19 - ^home-node-name => node 20 - ^owner-node-name => owner_node 21 - ^is-failed => failed 22 - disk-stats-info: 23 - average-latency 24 - disk-io-kbps 25 - power-on-time-interval => uptime 26 - sectors-read 27 - sectors-written 28 - disk-raid-info: 29 - ^container-type => container_type 30 - disk-outage-info: 31 - ^reason => outage 32 33 plugins: 34 - LabelAgent: 35 # metric label zapi_value rest_value `default_value` 36 value_to_num: 37 - new_status outage - - `0` #ok_value is empty value, '-' would be converted to blank while processing. 38 39 export_options: 40 instance_keys: 41 - node 42 - disk 43 instance_labels: 44 - type 45 - model 46 - outage 47 - owner_node 48 - shared 49 - shelf 50 - shelf_bay 51 - serial_number 52 - failed 53 - container_type Control What Labels and Metrics are Exported \u00b6 Let's continue with disk and look at a few examples. We'll use curl to examine the Prometheus wire format that Harvest uses to export the metrics from conf/zapi/cdot/9.8.0/disk.yaml . The curl below shows all exported disk metrics. There are 24 disks on this cluster, Harvest is collecting seven metrics + one disk_labels + one plugin-created metric, disk_new_status for a total of 216 rows. curl -s 'http://localhost:14002/metrics' | grep ^disk | cut -d'{' -f1 | sort | uniq -c 24 disk_bytes_per_sector # metric 24 disk_labels # labels 24 disk_new_status # plugin created metric 24 disk_sectors # metric 24 disk_stats_average_latency # metric 24 disk_stats_io_kbps # metric 24 disk_stats_sectors_read # metric 24 disk_stats_sectors_written # metric 24 disk_uptime # metric # sum = ((7 + 1 + 1) * 24 = 216 rows) Here's a disk_labels for one instance, reformated to make it easier to read. curl -s 'http://localhost:14002/metrics' | grep ^disk_labels | head -1 disk_labels{ datacenter = \"dc-1\", # always included - value taken from datacenter in harvest.yml cluster = \"umeng-aff300-05-06\", # always included node = \"umeng-aff300-06\", # node is in the list of export_options instance_keys disk = \"1.1.13\", # disk is in the list of export_options instance_keys type = \"SSD\", # remainder are included because they are listed in the template's instance_labels model = \"X371_S1643960ATE\", outage = \"\", owner_node = \"umeng-aff300-06\", shared = \"true\", shelf = \"1\", shelf_bay = \"13\", serial_number = \"S3SENE0K500572\", failed = \"false\", container_type = \"\", } 1.0 Here's the disk_sectors metric for a single instance. curl -s 'http://localhost:14002/metrics' | grep ^disk_sectors | head -1 disk_sectors{ # prefix of disk_ + metric name (line 11 in template) datacenter = \"dc-1\", # always included - value taken from datacenter in harvest.yml cluster = \"umeng-aff300-05-06\", # always included node = \"umeng-aff300-06\", # node is in the list of export_options instance_keys disk = \"1.1.17\", # disk is in the list of export_options instance_keys } 1875385008 # metric value - number of sectors for this disk instance Number of rows for each template = number of instances * (number of metrics + 1 (for <name>_labels row) + plugin additions) Number of metrics = number of counters which are not labels or keys, those without a ^ or ^^ Common Errors and Troubleshooting \u00b6 1. Failed to parse any metrics \u00b6 You add a new template to Harvest, restart your poller, and get an error message: WRN ./poller.go:649 > init collector-object (Zapi:NetPort): no metrics => failed to parse any This means the collector, Zapi NetPort , was unable to find any metrics. Recall metrics are lines without prefixes. In cases where you don't have any metrics, but still want to collect labels, add the collect_only_labels: true key-value to your template. This flag tells Harvest to ignore that you don't have metrics and continue. Example . 2. Missing Data \u00b6 What happens if an attribute is listed in the list of instance_labels (line 43 above), but that label is missing from the list of counters captured at line 5? The label will still be written into disk_labels , but the value will be empty since it's missing. e.g if line 29 was deleted container_type would still be present in disk_labels{container_type=\"\"} . Prometheus Wire Format \u00b6 https://prometheus.io/docs/instrumenting/exposition_formats/ Keep in mind that Prometheus does not permit dashes ( - ) in labels. That's why Harvest templates use name replacement to convert dashed-names to underscored-names with => . e.g. bytes-per-sector => bytes_per_sector converts bytes-per-sector into the Prometheus accepted bytes_per_sector . Every time series is uniquely identified by its metric name and optional key-value pairs called labels. Labels enable Prometheus's dimensional data model: any combination of labels for the same metric name identifies a particular dimensional instantiation of that metric (for example: all HTTP requests that used the method POST to the /api/tracks handler). The query language allows filtering and aggregation based on these dimensions. Changing any label value, including adding or removing a label, will create a new time series. <metric_name>{<label_name>=<label_value>, ...} value [ timestamp ] metric_name and label_name carry the usual Prometheus expression language restrictions label_value can be any sequence of UTF-8 characters, but the backslash (), double-quote (\"), and line feed (\\n) characters have to be escaped as \\, \\\", and \\n, respectively. value is a float represented as required by Go's ParseFloat() function. In addition to standard numerical values, NaN, +Inf, and -Inf are valid values representing not a number, positive infinity, and negative infinity, respectively. timestamp is an int64 (milliseconds since epoch, i.e. 1970-01-01 00:00:00 UTC, excluding leap seconds), represented as required by Go's ParseInt() function Exposition formats","title":"Templates And Metrics"},{"location":"resources/templates-and-metrics/#harvest-templates-and-metrics","text":"Harvest collects ONTAP counter information, augments it, and stores it in a time-series DB. Refer ONTAP Metrics for details about ONTAP metrics exposed by Harvest. flowchart RL Harvest[Harvest<br>Get & Augment] -- REST<br>ZAPI --> ONTAP id1[(Prometheus<br>Store)] -- Scrape --> Harvest Three concepts work in unison to collect ONTAP metrics data, prepare it and make it available to Prometheus. ZAPI/REST Harvest templates Exporters We're going to walk through an example from a running system, focusing on the disk object. At a high-level, Harvest templates describe what ZAPIs to send to ONTAP and how to interpret the responses. ONTAP defines twos ZAPIs to collect disk info Config information is collected via storage-disk-get-iter Performance counters are collected via disk:constituent These ZAPIs are found in their corresponding object template file conf/zapi/cdot/9.8.0/disk.yaml and conf/zapiperf/cdot/9.8.0/disk.yaml . These files also describe how to map the ZAPI responses into a time-series-friendly format Prometheus uniquely identifies a time series by its metric name and optional key-value pairs called labels.","title":"Harvest Templates and Metrics"},{"location":"resources/templates-and-metrics/#handy-tools","text":"dasel is useful to convert between XML, YAML, JSON, etc. We'll use it to make displaying some of the data easier.","title":"Handy Tools"},{"location":"resources/templates-and-metrics/#ontap-zapi-disk-example","text":"We'll use the bin/zapi tool to interrogate the cluster and gather information about the counters. This is one way you can send ZAPIs to ONTAP and explore the return types and values. bin/zapi -p u2 show attrs --api storage-disk-get-iter Output edited for brevity and line numbers added on left The hierarchy and return type of each counter is shown below. We'll use this hierarchy to build a matching Harvest template. For example, line 3 is the bytes-per-sector counter, which has an integer value, and is the child of storage-disk-info > disk-inventory-info . To capture that counter's value as a metric in a Harvest, the ZAPI template must use the same hierarchical path. The matching path can be seen below . building tree for attribute [attributes-list] => [storage-disk-info] 1 [storage-disk-info] - * 2 [disk-inventory-info] - 3 [bytes-per-sector] - integer 4 [capacity-sectors] - integer 5 [disk-type] - string 6 [is-shared] - boolean 7 [model] - string 8 [serial-number] - string 9 [shelf] - string 10 [shelf-bay] - string 11 [disk-name] - string 12 [disk-ownership-info] - 13 [home-node-name] - string 14 [is-failed] - boolean 15 [owner-node-name] - string 16 [disk-raid-info] - 17 [container-type] - string 18 [disk-outage-info] - 19 [is-in-fdr] - boolean 20 [reason] - string 21 [disk-stats-info] - 22 [average-latency] - integer 23 [disk-io-kbps] - integer 24 [power-on-time-interval] - integer 25 [sectors-read] - integer 26 [sectors-written] - integer 27 [disk-uid] - string 28 [node-name] - string 29 [storage-disk-state] - integer 30 [storage-disk-state-flags] - integer","title":"ONTAP ZAPI disk example"},{"location":"resources/templates-and-metrics/#harvest-templates","text":"To understand templates, there are a few concepts to cover: There are three kinds of information included in templates that define what Harvest collects and exports: Configuration information is exported into the _labels metric (e.g. disk_labels see below) Metrics data is exported as disk_\"metric name\" e.g. disk_bytes_per_sector , disk_sectors , etc. Metrics are leaf nodes that are not prefixed with a ^ or ^^. Metrics must be one of the number types: float or int. Plugins may add additional metrics, increasing the number of metrics exported in #2 A resource will typically have multiple instances. Using disk as an example, that means there will be one disk_labels and a metric row per instance. If we have 24 disks and the disk template lists seven metrics to capture, Harvest will export a total of 192 rows of Prometheus data. 24 instances * (7 metrics per instance + 1 label per instance) = 192 rows Sum of disk metrics that Harvest exports curl -s 'http://localhost:14002/metrics' | grep ^disk | cut -d'{' -f1 | sort | uniq -c 24 disk_bytes_per_sector 24 disk_labels 24 disk_sectors 24 disk_stats_average_latency 24 disk_stats_io_kbps 24 disk_stats_sectors_read 24 disk_stats_sectors_written 24 disk_uptime # 192 rows Read on to see how we control which labels from #1 and which metrics from #2 are included in the exported data.","title":"Harvest Templates"},{"location":"resources/templates-and-metrics/#instance-keys-and-labels","text":"Instance key - An instance key defines the set of attributes Harvest uses to construct a key that uniquely identifies an object. For example, the disk template uses the node + disk attributes to determine uniqueness. Using node or disk alone wouldn't be sufficient since disks on separate nodes can have the same name. If a single label does not uniquely identify an instance, combine multiple keys for uniqueness. Instance keys must refer to attributes that are of type string . Because instance keys define uniqueness, these keys are also added to each metric as a key-value pair. ( see Control What Labels and Metrics are Exported for examples) Instance label - Labels are key-value pairs used to gather configuration information about each instance. All of the key-value pairs are combined into a single metric named disk_labels . There will be one disk_labels for each monitored instance. Here's an example reformatted so it's easier to read: disk_labels{ datacenter=\"dc-1\", cluster=\"umeng-aff300-05-06\", node=\"umeng-aff300-06\", disk=\"1.1.23\", type=\"SSD\", model=\"X371_S1643960ATE\", outage=\"\", owner_node=\"umeng-aff300-06\", shared=\"true\", shelf=\"1\", shelf_bay=\"23\", serial_number=\"S3SENE0K500532\", failed=\"false\", container_type=\"shared\" }","title":"Instance Keys and Labels"},{"location":"resources/templates-and-metrics/#harvest-object-template","text":"Continuing with the disk example, below is the conf/zapi/cdot/9.8.0/disk.yaml that tells Harvest which ZAPI to send to ONTAP ( storage-disk-get-iter ) and describes how to interpret and export the response. Line 1 defines the name of this resource and is an exact match to the object defined in your default.yaml or custom.yaml file. Eg. # default.yaml objects: Disk: disk.yaml Line 2 is the name of the ZAPI that Harvest will send to collect disk resources Line 3 is the prefix used to export metrics associated with this object. i.e. all metrics will be of the form disk_* Line 5 the counter section is where we define the metrics, labels, and what constitutes instance uniqueness Line 7 the double hat prefix ^^ means this attribute is an instance key used to determine uniqueness. Instance keys are also included as labels. Uuids are good choices for uniqueness Line 13 the single hat prefix ^ means this attribute should be stored as a label. That means we can include it in the export_options section as one of the key-value pairs in disk_labels Rows 10, 11, 23, 24, 25, 26, 27 - these are the metrics rows - metrics are leaf nodes that are not prefixed with a ^ or ^^. If you refer back to the ONTAP ZAPI disk example above, you'll notice each of these attributes are integer types. Line 43 defines the set of labels to use when constructing the disk_labels metrics. As mentioned above , these labels capture config-related attributes per instance. Output edited for brevity and line numbers added for reference. 1 name: Disk 2 query: storage-disk-get-iter 3 object: disk 4 5 counters: 6 storage-disk-info: 7 - ^^disk-uid 8 - ^^disk-name => disk 9 - disk-inventory-info: 10 - bytes-per-sector => bytes_per_sector # notice this has the same hierarchical path we saw from bin/zapi 11 - capacity-sectors => sectors 12 - ^disk-type => type 13 - ^is-shared => shared 14 - ^model => model 15 - ^serial-number => serial_number 16 - ^shelf => shelf 17 - ^shelf-bay => shelf_bay 18 - disk-ownership-info: 19 - ^home-node-name => node 20 - ^owner-node-name => owner_node 21 - ^is-failed => failed 22 - disk-stats-info: 23 - average-latency 24 - disk-io-kbps 25 - power-on-time-interval => uptime 26 - sectors-read 27 - sectors-written 28 - disk-raid-info: 29 - ^container-type => container_type 30 - disk-outage-info: 31 - ^reason => outage 32 33 plugins: 34 - LabelAgent: 35 # metric label zapi_value rest_value `default_value` 36 value_to_num: 37 - new_status outage - - `0` #ok_value is empty value, '-' would be converted to blank while processing. 38 39 export_options: 40 instance_keys: 41 - node 42 - disk 43 instance_labels: 44 - type 45 - model 46 - outage 47 - owner_node 48 - shared 49 - shelf 50 - shelf_bay 51 - serial_number 52 - failed 53 - container_type","title":"Harvest Object Template"},{"location":"resources/templates-and-metrics/#control-what-labels-and-metrics-are-exported","text":"Let's continue with disk and look at a few examples. We'll use curl to examine the Prometheus wire format that Harvest uses to export the metrics from conf/zapi/cdot/9.8.0/disk.yaml . The curl below shows all exported disk metrics. There are 24 disks on this cluster, Harvest is collecting seven metrics + one disk_labels + one plugin-created metric, disk_new_status for a total of 216 rows. curl -s 'http://localhost:14002/metrics' | grep ^disk | cut -d'{' -f1 | sort | uniq -c 24 disk_bytes_per_sector # metric 24 disk_labels # labels 24 disk_new_status # plugin created metric 24 disk_sectors # metric 24 disk_stats_average_latency # metric 24 disk_stats_io_kbps # metric 24 disk_stats_sectors_read # metric 24 disk_stats_sectors_written # metric 24 disk_uptime # metric # sum = ((7 + 1 + 1) * 24 = 216 rows) Here's a disk_labels for one instance, reformated to make it easier to read. curl -s 'http://localhost:14002/metrics' | grep ^disk_labels | head -1 disk_labels{ datacenter = \"dc-1\", # always included - value taken from datacenter in harvest.yml cluster = \"umeng-aff300-05-06\", # always included node = \"umeng-aff300-06\", # node is in the list of export_options instance_keys disk = \"1.1.13\", # disk is in the list of export_options instance_keys type = \"SSD\", # remainder are included because they are listed in the template's instance_labels model = \"X371_S1643960ATE\", outage = \"\", owner_node = \"umeng-aff300-06\", shared = \"true\", shelf = \"1\", shelf_bay = \"13\", serial_number = \"S3SENE0K500572\", failed = \"false\", container_type = \"\", } 1.0 Here's the disk_sectors metric for a single instance. curl -s 'http://localhost:14002/metrics' | grep ^disk_sectors | head -1 disk_sectors{ # prefix of disk_ + metric name (line 11 in template) datacenter = \"dc-1\", # always included - value taken from datacenter in harvest.yml cluster = \"umeng-aff300-05-06\", # always included node = \"umeng-aff300-06\", # node is in the list of export_options instance_keys disk = \"1.1.17\", # disk is in the list of export_options instance_keys } 1875385008 # metric value - number of sectors for this disk instance Number of rows for each template = number of instances * (number of metrics + 1 (for <name>_labels row) + plugin additions) Number of metrics = number of counters which are not labels or keys, those without a ^ or ^^","title":"Control What Labels and Metrics are Exported"},{"location":"resources/templates-and-metrics/#common-errors-and-troubleshooting","text":"","title":"Common Errors and Troubleshooting"},{"location":"resources/templates-and-metrics/#1-failed-to-parse-any-metrics","text":"You add a new template to Harvest, restart your poller, and get an error message: WRN ./poller.go:649 > init collector-object (Zapi:NetPort): no metrics => failed to parse any This means the collector, Zapi NetPort , was unable to find any metrics. Recall metrics are lines without prefixes. In cases where you don't have any metrics, but still want to collect labels, add the collect_only_labels: true key-value to your template. This flag tells Harvest to ignore that you don't have metrics and continue. Example .","title":"1. Failed to parse any metrics"},{"location":"resources/templates-and-metrics/#2-missing-data","text":"What happens if an attribute is listed in the list of instance_labels (line 43 above), but that label is missing from the list of counters captured at line 5? The label will still be written into disk_labels , but the value will be empty since it's missing. e.g if line 29 was deleted container_type would still be present in disk_labels{container_type=\"\"} .","title":"2. Missing Data"},{"location":"resources/templates-and-metrics/#prometheus-wire-format","text":"https://prometheus.io/docs/instrumenting/exposition_formats/ Keep in mind that Prometheus does not permit dashes ( - ) in labels. That's why Harvest templates use name replacement to convert dashed-names to underscored-names with => . e.g. bytes-per-sector => bytes_per_sector converts bytes-per-sector into the Prometheus accepted bytes_per_sector . Every time series is uniquely identified by its metric name and optional key-value pairs called labels. Labels enable Prometheus's dimensional data model: any combination of labels for the same metric name identifies a particular dimensional instantiation of that metric (for example: all HTTP requests that used the method POST to the /api/tracks handler). The query language allows filtering and aggregation based on these dimensions. Changing any label value, including adding or removing a label, will create a new time series. <metric_name>{<label_name>=<label_value>, ...} value [ timestamp ] metric_name and label_name carry the usual Prometheus expression language restrictions label_value can be any sequence of UTF-8 characters, but the backslash (), double-quote (\"), and line feed (\\n) characters have to be escaped as \\, \\\", and \\n, respectively. value is a float represented as required by Go's ParseFloat() function. In addition to standard numerical values, NaN, +Inf, and -Inf are valid values representing not a number, positive infinity, and negative infinity, respectively. timestamp is an int64 (milliseconds since epoch, i.e. 1970-01-01 00:00:00 UTC, excluding leap seconds), represented as required by Go's ParseInt() function Exposition formats","title":"Prometheus Wire Format"}]}